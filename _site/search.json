[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a data scientist with an interest in public policy, statistics, and coding. This blog is mostly to help me organize my thoughts, but hopefully at least a few people will find it worth reading."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Policy, Politics, and Data",
    "section": "",
    "text": "The Perpetual Myth of Eliminating Waste, Fraud, and Abuse\n\n\n\n\n\n\nPolicy\n\n\nPolitics\n\n\n\nWaste, Fraud, and Abuse is a slogan used to hide the basic tradeoff that cutting spending requires cutting services.\n\n\n\n\n\nApr 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEnd Exclusionary Zoning\n\n\n\n\n\n\nLouisville\n\n\nPoverty\n\n\nHousing\n\n\nPolicy\n\n\n\nOver half of Louisville’s land is reserved for large single family dwellings. Only 6 percent is zoned for multi-family use.\n\n\n\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSimpson’s Paradox\n\n\n\n\n\n\nPython\n\n\nCausal Inference\n\n\nStatistics\n\n\n\nSimpson’s Paradox illustrated with Penguins\n\n\n\n\n\nApr 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Census Data with Python\n\n\n\n\n\n\nPython\n\n\nCensus\n\n\nGeoPandas\n\n\nKentucky\n\n\nLouisville\n\n\nInternet\n\n\n\nA tutorial on how to make maps by pulling Census data into Python\n\n\n\n\n\nMar 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPartitioned Regression with Palmer Penguins and Scikit-Learn\n\n\n\n\n\n\nPython\n\n\nPenguins\n\n\nStatistics\n\n\n\nUsing partitioned regression to gain a better understanding of how linear regression works.\n\n\n\n\n\nMar 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThink before adding more variables to that analysis\n\n\n\n\n\n\nPython\n\n\nCausal Inference\n\n\nPolicy\n\n\nStatistics\n\n\n\nAn introduction to thinking about causal models for data analysis. The purpose is to demonstrate that the popular approach of simply gathering as much data as you can and controlling for it via regression or other methods is not a good one, and is actively misleading in many cases. We should instead carefully think about plausible causal models using tools like diagrams (directed acyclic graphs, or DAGs) and then do data analysis in accordance with those models.\n\n\n\n\n\nMar 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRacing Louisville in their first NWSL season\n\n\n\n\n\n\nPython\n\n\nSports\n\n\nData Viz\n\n\n\nA quick look back at how Racing Louisville played in their first season in the NWSL\n\n\n\n\n\nMar 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Use Census Microdata to Analyze High Speed Internet in Kentucky\n\n\n\n\n\n\nR\n\n\nCensus\n\n\nInternet\n\n\nPolicy\n\n\n\nThis post is a start to finish descriptive analysis of high speed internet access in Kentucky, including tables, graphs, and maps. All of the detail of cleaning the data and iterating while exploring the data is included. This makes for a rather lengthy post, but it also makes it relatively unique in including all of those steps. We go through five attempts at making a table of high speed internet before finally getting it right! There’s quite a bit of cleaning work and then also a detour into calculating standard errors via bootstrap so we can correctly display uncertainty in our visuals.\n\n\n\n\n\nSep 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nMeritocracy is Unjust\n\n\n\n\n\n\nPhilosophy\n\n\nEconomics\n\n\n\nMeritocracy tends to confuse a very practical sense of merit with a more abstract and moral one. An individual may deserve a high-paying job or admission to a selective college because they are productive or qualified. However, in a moral sense, individuals do not merit the skills and abilities they are born with, nor do they merit the environments they were born into that allowed them to develop those skills.\n\n\n\n\n\nOct 25, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Market Values and Policy Analysis\n\n\n\n\n\n\nPhilosophy\n\n\nEconomics\n\n\nPolicy\n\n\nKentucky\n\n\n\nThere are a lot of things in life that don’t have market value. It’s important to find a way to value those things in policy analysis.\n\n\n\n\n\nApr 15, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nRational Fools: Amartya Sen’s Critique of Economic Theory\n\n\n\n\n\n\nPhilosophy\n\n\nEconomics\n\n\nPolicy\n\n\n\nSome history of economic thought around self-interest looking at Francis Edgeworth, Adam Smith, and Amartya Sen\n\n\n\n\n\nFeb 3, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nA Theory of Moral Sentiments\n\n\n\n\n\n\nEconomics\n\n\nPhilosophy\n\n\n\nReflections on Adam Smith’s Theory of Moral Sentiments and economics education today\n\n\n\n\n\nSep 3, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I Believe\n\n\n\n\n\n\nReligion\n\n\nPhilosophy\n\n\n\nSorting through my own belief system\n\n\n\n\n\nAug 20, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Education for?\n\n\n\n\n\n\nEconomics\n\n\nPhilosophy\n\n\nPolicy\n\n\nEducation\n\n\n\nSTEM often (at the undergraduate level) teaches a certain type of thinking, which is a very effective and practical way to solve problems. STEM fields seek answers, while the humanities focus first on training students to ask the correct questions, and to take an extremely broad view of any problem. A lot of damage has been done by narrow, practical solutions. The technology we have is an engineering marvel, and the economic abundance we possess is a tribute to the efficiency of solving practical problems. And yet for all our abundance we still have massive poverty and environmental degradation, as well as a society that is becoming increasingly polarized, distrustful, and distant.\n\n\n\n\n\nAug 12, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we know about race and racism in the United States\n\n\n\n\n\n\nPolicy\n\n\n\nAn attempt to add historical and statistical context to recent events\n\n\n\n\n\nJul 21, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nMarket Norms are Crowding out Social Norms\n\n\n\n\n\n\nPhilosophy\n\n\nEconomics\n\n\n\nReflecting on how market norms are squeezing out social norms to the detriment of society\n\n\n\n\n\nJul 13, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bible and Public Policy\n\n\n\n\n\n\nReligion\n\n\n\nAn overview of what the Bible has to say about public policy\n\n\n\n\n\nDec 20, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nThe Moral Limits of Markets\n\n\n\n\n\n\nPhilosophy\n\n\nEconomics\n\n\n\nA review of Michael Sandel’s book ‘What Money Can’t Buy: The Moral Limits of Markets’\n\n\n\n\n\nMay 26, 2012\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Nate Kratzer",
    "section": "",
    "text": "MIT License\nCopyright (c) 2023 Nate Kratzer\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html",
    "href": "posts/bible_and_public_policy/index.html",
    "title": "The Bible and Public Policy",
    "section": "",
    "text": "I am frequently asked what Jesus has to do with public policy. It is a difficult question to answer, because Jesus didn’t live in a participatory democracy in which he could clearly let us know his stand on the political controversies of the day. He lived in an occupied land where ultimate political power rested with the Roman empire and the proximate political power was the Sanhedrin (the Jewish court) that ruled over Israel. Since the Sanhedrin was a religious body, this meant that Jesus, like all Jews before him lived in a theocracy.\nNeedless to say, it is a little difficult to figure out how to apply the actions of an oppressed powerless Jew living two thousand years ago in a conquered theocracy to today’s politics. But there are a few things we can do to bridge that gap.\nFirst, we need to remember that politics is simply the way in which we choose to order our cities. In a democracy we tend to think of partisan politics, but the actual word comes from the Greek ‘polis’ which simply meant the city-state. Politics is how we organize as a community, whether it is a democracy, theocracy, autocracy, or kingdom it is still a political order.\nSecond, we need to remember that Jesus’ life and death did not happen in isolation. We have the entire rest of the Bible to give context to Jesus’s life and ministry. And since this is a broad and important question, I’m going to start at the very beginning."
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html#from-genesis-through-the-kings",
    "href": "posts/bible_and_public_policy/index.html#from-genesis-through-the-kings",
    "title": "The Bible and Public Policy",
    "section": "From Genesis through the Kings",
    "text": "From Genesis through the Kings\nIn the beginning, God created the world out of love and God saw that the world was good and the world was God’s. (Gen 1:31) Then came the fall. Human beings gave into temptation to be God-like and were banished from paradise (Gen 3). This dual nature of human beings as being both good and made in the image of God (Gen 1:27) as well as fallen is a basis for the a theological anthropology that holds that each human being is at once both saint and sinner.\nThe stories of the patriarchs confirm both the saint and the sinner in humanity and end with the Hebrews being enslaved in Egypt as a result of Joseph’s use of the famine to put all of the land under the control of Pharaoh. (Gen 48:20-27)\nBy bringing the Hebrew people out of Egypt God is revealed as a God who liberates God’s people from oppression and leads them out of slavery into the promised land (Exodus 20:2, Deuteronomy 7:8 and others). Once there God gives them laws to live by and we get our first glimpse of what a society ordered by God looks like. There is no King, because God is their king (1 Samuel 12:12). The laws cover religion, politics, and economics. The economics laws in particular reveal a remarkable egalitarianism:\n\nDuring the harvest, anything that falls to the ground is to be left for the poor to come and gather (Deuteronomy 24:19-22).\nEvery seven years debts are to be forgiven(Deuteronomy 15:1).\nSlaves are to be freed every seven years (Exodus 21:2-4, Deuteronomy 15:12).\nEvery fifty years land is to be restored to its original owners (Leviticus 25: 8-28).\nThere is to be no oppression of strangers living among them (Exodus 22:21, Exodus 23:9)\nDo not oppress the widow or the orphan (Exodus 22: 22-24).\nThe terms of lending are not to include interest payments or take as collateral the borrowers means of production and shelter (Exodus 22:25-27, Deuteronomy 24:6).\n\nEventually the people grow restless and desire a king. God tries to talk them out of it (1 Samuel 8:10-18, 1 Samuel 12:11-25, Deuteronomy 17:14-20) but relents and anoints Saul to be their king (1 Samuel 15:1) and then regrets appointing Saul a mere ten verses later (1 Samuel 15:10-11). Skipping over a wonderful succession narrative we then wind up with King David and the united Kingdom of Israel. King David reigned about 1,000 years before Jesus’ birth and from a political and economic perspective it was clearly the golden age of Israel. However, it all fell apart under King Solomon due to excessive taxation and the fact that the kingdom was only tentatively united to begin with. Israel split into a Northern Kingdom (Israel) and a Southern Kingdom (Judah). In 722 BCE the Northern Kingdom was conquered by the Assyrians and ceased to exist."
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html#the-prophets",
    "href": "posts/bible_and_public_policy/index.html#the-prophets",
    "title": "The Bible and Public Policy",
    "section": "The Prophets",
    "text": "The Prophets\nIt is during the 8th Century BCE that the tradition of prophetic criticism of unjust government begins to emerge (Nathan had criticized King David for personal immorality regarding Bethsheba, but this is the beginning of a more general critique of justice).\nAmos and Hosea prophesy in the Northern Kingdom and Isaiah and Micah in the south. Isaiah 1:13-17 sums it up well:\n\n…bringing offerings is futile; incense is an abomination to me….I cannot endure solemn assemblies with iniquity…When you stretch out your hands, I will hide my eyes from you; even though you make many prayers I will not listen; your hands are full of blood…cease to do evil, learn to do good, seek justice, rescue the oppressed, defend the orphan, plead for the widow.\n\nBut Isaiah 1:13-17 is hardly alone. It is impossible to do justice to all the 8th century prophets had to say about justice, but here is a (by no means complete) collection of verses broken down by subject*:\n\nConcentrated wealth (Isaiah 5:7-8, Amos 6:4-8, Micah 6:10-16)\nOppression/Exploitation of the poor (Isaiah 3:13-15, Isaiah 32:1-8, Hosea 12:7-8, Amos 2:6-8, Amos 4:1, Amos 5:11-12, 15, Amos 8:4-6, Micah 2:1-9, Micah 3:1-8, Micah 7:3-4)\nPeace (Isaiah 2:4, Hosea 10:13, Amos 3:10, Micah 4:3)\nLiberation (Isaiah 14:3-5, Isaiah 31:8-9, Isaiah 35:5-7, Amos 9:7)\nJustice (Isaiah 11:1-9, Isaiah 16: 3-5, Amos 5:23-24, Micah 6:8)\n\nAround or slightly after the time at which this critical prophetic tradition was developing there was also a new line of theological thought that is given its fullest expression in the books of Job and Ecclesiastes. The wisdom sages realized that sometimes bad things happen to good people and started asking questions we still ask today. This realization that being either poor or wealthy is not a direct manifestation of God’s will put the plight of the poor in a new light.\nIn the 6th century BCE (600 years before Christ) the Southern Kingdom was defeated by Babylon and sent into exile. This lead to a new series of prophets that were concerned with the theological implications of their defeat and removal from their sacred land. (Jeremiah’s idea of God writing a covenant on the heart of each Israelite probably owes something to the exile from sacred land (Jeremiah 31:31-34)). However, these prophets and those who oversaw the return from exile in the later half of the century were also concerned with just governance.\nThe most notable new development might be Jeremiah saying that to know the Lord is to do justice to the poor and needy (Jeremiah 22:16). Again, this is a rich period that deserves more attention than I can give it in this brief overview, but here are some notable verses again broken down by subject:\n\nOppression/Exploitation of the poor (Ezekiel 35:17-20, Isaiah 65:19-25)\nPeace (Jeremiah 29:7)\nLiberation (Isaiah 65:19-25, Isaiah 61:1-2, 8, Ezekiel 37:14, Isaiah 41:17)\nJustice (Jeremiah 7:5-6, Jeremiah 22:13-16, Isaiah chapters 58 and 59)\n\nAfter the Persians defeated the Babylonians the exile was ended, but Israel was now under Persian rule. When Ezra and Nehemiah rebuilt after the Exile they followed not only the religious laws, but also the economic ones (Nehemiah 5). They failed to include the resident alien and forbid intermarriage, but were rebuked by the storytelling of books like Jonah and Ruth, stories that cast aliens as heroes and even as the ancestor of King David.\nIn the 4th Century BCE Israel was conquered by the Macedonian empire of Alexander the Great, but as the Macedonian Empire declined Israel managed to seize about a century of freedom (the revolt of Judas Maccabeus and the Hasmonean dynasty) before being conquered by the Romans in the 1st century BCE."
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html#jesus",
    "href": "posts/bible_and_public_policy/index.html#jesus",
    "title": "The Bible and Public Policy",
    "section": "Jesus",
    "text": "Jesus\nIt is against all of this background that we can know begin to explore the political implications of Jesus’ ministry. The most common (though not the only) messianic notion of the time period was that the Messiah would be sent by God to restore the Davidic Kingdom, the golden age of Israel. Jesus, however, was clearly a nonviolent messiah (Matthew 26:52, Matthew 5:38-42, Luke 9:54) who did not call forth God’s wrath on Rome in order to free Israel from oppression. Instead, Jesus proclaimed the coming of the Kingdom of God.\nJesus is consistently engaged in debate with the pharisees (religious leaders) of his day over questions of the law and the prophets and their meaning for the present day. To speak in broad terms, within these debates Jesus tended to argue that compassion is more important than ritual purity. The summation of this thesis is found in Matthew 23:23 (cf. Luke 11:42) “For you tithe mint, dill, and cumin, but have neglected the weightier matters of the law: justice and mercy and faith.” Jesus sums up the law and prophets saying that they all hang on love of God and love of neighbor (Matthew 22:37-40, Luke 10:25-28). Jesus parable of the good Samaritan (Luke 10:29-37) shows that loving ones neighbor is even more important than preserving one’s ritual purity (the priest and the levite passed by in part because contact with a dead body would make them unclean). Some of Jesus’ other relevant teachings include***:\n\nTelling a young man that to be perfect he must sell all he has, give the money to the poor, and follow Jesus (Matthew 19:21, Mark 20-22, Luke 18:18-25)\nLove your enemies (Matthew 5:38-48, Luke 6:27-36)\nForgive debts (Matthew 6:12, 18:21-35, Luke 16:1-13)\nBear good fruit (Matthew 3:8-10, 7:16-20, 12:33-35, 21:43, Luke 6:43-45)\nBlessed are the poor, hungry, mourning, meek, merciful, pure in heart, peacemakers, and persecuted (Matthew 5:1-12, Luke 6:21-26)\nTreat others as you would like to be treated (Matthew 7:12)\nCare for sinners and spend time among them (Matthew 9:10-13, Luke 7:40-50, 19:5-10)\nThe signs of God’s kingdom are that the lame walk, the hungry are feed, and the blind see. (Matthew 11:4-6, Luke 4:18-19, 7:22-23)\nThe Sabbath is made for human beings; compassion and human need are more important. (Matthew 12:1-8, Mark 2:23-28, 3:1-5, Luke 13:12-15, 14:3-6)\nIt is what comes out of your mouth, not what goes in, that matters (Matthew 15:16-20, Mark 7:14-23)\nSupporting your elderly parents monetarily is more important than temple offerings (Matthew 15:3-9, Mark 7:6-12)\nForbidding divorce in a culture where divorce left women economically desolate (Matthew 5:31, Matthew 19:9, Luke 16:18)\nTax collectors and prostitutes are more righteous than religious leaders (Matthew 21:31)\nI came as a servant, not a master (Mark 10:45)\nThe powerful are brought down and the hungry are fed (Luke 1:52-53)\nShare your coats and food with those who have none (Luke 3:11)\nAbundant possessions will not help you (Luke 12:13-21)\n“None of you can become my disciple if you do not give up all your possessions” (Luke 14:33)\nZacchaeus is praised for giving half of his possessions to the poor (Luke 19:8-9)\nCondemning the scribes because “they devour widows’ houses” (Luke 20:47)\n\nJesus’ life also witnessed to compassion with numerous healings and several instances of feeding the multitudes. It is the driving out of the money changersfrom the temple that leads to Jesus’ arrest (Matthew 21:12-13, Mark 11:17-18, Luke 19:45-48). Jesus has now threatened the monetary base of the religious and political authority and is handed over to the Romans to be put to death as a political prisoner. The theology of Christ’s death is a source of dispute but all theories I am aware of stress that it is an act of compassion and a means of reconciling God’s people back to God (Colossians 1:20, 2 Corinthians 5: 17-19)."
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html#the-early-christian-community-and-the-kingdom-of-god",
    "href": "posts/bible_and_public_policy/index.html#the-early-christian-community-and-the-kingdom-of-god",
    "title": "The Bible and Public Policy",
    "section": "The early Christian Community and the Kingdom of God",
    "text": "The early Christian Community and the Kingdom of God\nFinally, we must consider the practices of the early Christian community as we attempt to form a coherent vision of the Kingdom of God as preached by Jesus. The early followers of Christ experienced Pentecost and then lived together holding all of their possessions in common, selling whatever they had and distributing the proceeds to anyone who had need (Acts 2:43-47). None of them had any private property (Acts 4:32) and when Ananias tries to join but holds back property from the group he falls down and dies (Acts 5:1-6). These early followers of Christ were not yet known as Christians, but were called followers of the way (Acts 9:2). Though in later years much of the emphasis of Christians would shift from following Jesus’ teachings by living in a certain way to a system more focused on beliefs about Jesus, in the early years Jesus’ followers sought to live in a certain way, the way that Jesus taught.\nIt is in exploring the way of life that Jesus taught that we finally find what that way of life has to do with our current public policy. The Kingdom of God as proclaimed by Jesus is one that is:\n\nalready present (Luke 7:22).\ncollaborative (Luke 17:20-21).\nnonviolent (Matthew 26:52).\n\nThe Kingdom of God is already present inasmuch as individuals are willing to collaborate with it, to enter into it and to live it out. It is also nonviolent, it does not overthrow power with violence but instead seeks to subvert that power (Matthew 5:38-41). The way of Jesus is to enter into this Kingdom that is typified by the teachings of Jesus, and the prophets, and the law. A kingdom is, of course, a form of political order. The Kingdom of God is one in which God is king, and clearly God is a very different kind of king than King David or any of the other kings of earth with whom we are familiar. Based on the overview given above there are a few characteristics that stand out to me:\n\nAn overwhelming concern with and compassion for the poor and vulnerable.\nA rejection of physical wealth and power as status symbols or a way to be secure.\nA desire for proper relationships with God and among human beings.\nPeace that comes through justice, not violence.\n\nThe kingdom of God is collaborative, which means that we can’t do it without God. But it also means that God is waiting for us to accept and enter into the Kingdom of God by living it out in our churches and communities. No political order should ever be identified with the Kingdom, because all human beings are both saints and sinners. However, all political orders should be judged against the Kingdom as a standard, an ‘impossible but relevant ideal’ for which Christians are called to strive. This, then, is what Jesus has to do with public policy. Jesus invites us to enter the Kingdom, and by striving towards the Kingdom we are irrevocably committed to striving towards an impossible ideal in our community life and our social and political order."
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html#policy-in-a-nation-with-multiple-religious-views",
    "href": "posts/bible_and_public_policy/index.html#policy-in-a-nation-with-multiple-religious-views",
    "title": "The Bible and Public Policy",
    "section": "Policy in a nation with multiple religious views",
    "text": "Policy in a nation with multiple religious views\nOne of the frequent objections made to using the Bible as a source of public policy is that the United States is not a Christian nation and should not take its policy guidance from sacred scripture. This is an entirely correct reply to an argument that I have never made. The Bible is supposed to guide Christian views of public policy, but not to guide the U.S. government’s view of public policy.\nReligious pluralism does not require Christians (or any other religious groups) to ignore the public policy implications of their theology and scripture, it simply requires that if they wish to be persuasive in public debate they use arguments that will resonate with their audience. If I were speaking to a large group about immigration reform I would talk about the economic benefits to immigration as well as a fairness argument arising from the lottery of birthplace. If I were speaking to a Christian congregation about immigration, I would start with theology (and also probably mention economics).\nThe Bible really does have a lot to say about public policy, but no one who doesn’t consider the Bible to be their sacred scripture needs to listen.\n\nFootnotes\n*In reading the prophets it is important to know that ‘the gate’ was the cite of legal proceedings. When the prophets call for justice in the gate or condemn injustice at the gate they are taking about the judicial system. Also, there is naturally quite a bit of overlap among the categories.\n**The book of Isaiah is generally accepted by scholars as the work of three different prophets within the school of thought of Isaiah, the 8th century prophet. The other two are unnamed and prophesied during the exile and immediately after the exile (early and late 6th century BCE).\n*** I have included teachings about Jesus that were said by Mary and John the Baptist"
  },
  {
    "objectID": "posts/census_internet_microdata/index.html",
    "href": "posts/census_internet_microdata/index.html",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "",
    "text": "This post is a start to finish descriptive analysis of high speed internet access in Kentucky, including tables, graphs, and maps. All of the detail of cleaning the data and iterating while exploring the data is included. This makes for a rather lengthy post, but it also makes it relatively unique in including all of those steps. We go through five attempts at making a table of high speed internet before finally getting it right! There’s quite a bit of cleaning work and then also a detour into calculating standard errors via bootstrap so we can correctly display uncertainty in our visuals.\nCensus microdata is individual level data provided by the census. Most references to census data refer to tables the census bureau has already made out of their surveys, but the mostly raw survey data is available at the individual level, and that’s what we’ll use for this analysis. While the focus is on the census data, I do show the code used for analysis. I did decide to hide the code used to make all the tables (it gets rather lengthy), but you can see that code on Github if interested."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#getting-wrong-answers-by-not-knowing-the-data",
    "href": "posts/census_internet_microdata/index.html#getting-wrong-answers-by-not-knowing-the-data",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Getting wrong answers by not knowing the data",
    "text": "Getting wrong answers by not knowing the data\nNow that we have a high speed internet category we can group the data and count up how many responses are in each group. I’ll also pivot the dataframe to make it easy to calculate percent with high speed internet.\n\n# Count numbers with and without high speed internet\ndf_group &lt;- df %&gt;%\n  group_by(hspd_int, YEAR) %&gt;%\n  summarize(count = n(), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide &lt;- df_group  %&gt;%\n  pivot_wider(id_cols = YEAR,\n              names_from = hspd_int,\n              values_from = count) %&gt;%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  ) \n\n\n\n\n\n\n  \n    \n      Table 1: Quick Analysis\n    \n    \n      These results are wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n87%\n13%\n28%\n28.3K\n4.27K\n12.4K\n    2014\n86%\n14%\n27%\n28.1K\n4.70K\n12.1K\n    2015\n86%\n14%\n26%\n28.7K\n4.52K\n11.5K\n    2016\n82%\n18%\n20%\n29.2K\n6.49K\n9.02K\n    2017\n81%\n19%\n19%\n29.4K\n7.08K\n8.77K\n    2018\n80%\n20%\n17%\n30.3K\n7.35K\n7.86K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's incorrect analysis of IPUMS data. Used as an example of a mistake to avoid.\n    \n  \n  \n\n\n\n\nThis table is actually wrong for multiple reasons, but the first one we’ll take care of is the failure to use weights. Survey data is often weighted to make it representative of the population. The census bureau provides a PERWT variable that should be used as the weight for each person in the file. There’s also a HHWT variable for household level analysis. We’ll stick with weighting the data by the number of people. One of the really nice features of the PERWT variable is that it sums to the population. That means our tables can show both an overall number of people and the percentage of people.\n\n# Count numbers with and without high speed internet\ndf_group &lt;- df %&gt;%\n  group_by(hspd_int, YEAR) %&gt;%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide &lt;- df_group  %&gt;%\n  pivot_wider(id_cols = YEAR,\n              names_from = hspd_int,\n              values_from = count) %&gt;%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  )\n\n\n\n\n\n\n  \n    \n      Table 2: Quick Analysis with Weights\n    \n    \n      These results are still wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n86%\n14%\n27%\n2.76M\n454K\n1.18M\n    2014\n84%\n16%\n26%\n2.74M\n509K\n1.17M\n    2015\n86%\n14%\n25%\n2.85M\n479K\n1.10M\n    2016\n80%\n20%\n19%\n2.87M\n708K\n855K\n    2017\n80%\n20%\n18%\n2.90M\n735K\n817K\n    2018\n79%\n21%\n16%\n2.97M\n790K\n709K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's incorrect analysis of IPUMS data. Used as an example of a mistake to avoid.\n    \n  \n  \n\n\n\n\nThis is better. The second problem is harder to spot. There are 3 hints in the data:\n\nThere is a very high percentage of NA responses. There are more NA answers than there are people who say they don’t have high speed access.\nPercent of of people with high speed access is going down over time, which isn’t what I’d expect to see. That doesn’t mean it’s wrong - sometimes the data shows high level trends we don’t expect. However, it’s always worth a second look when you get a counterintuitive result.\nThese numbers look very high for Kentucky.\n\nA sensible guess is that people who say they don’t have internet access at all aren’t then asked about high speed internet and show up as an NA value when we want to code them as not having high speed interent.\nSo let’s get to know the data a bit better by adding in internet access. We’ll do the same analysis, but I’ll add internet as another id variable just like year. We can see right away that the answers we have above are only including cases where individuals have internet.\n\ndf &lt;- df %&gt;%\n  mutate(\n    int = case_when(\n      CINETHH == 0 ~ NA_character_,\n      CINETHH == 1 | CINETHH == 2 ~ \"Yes\",\n      CINETHH == 3 ~ \"No\",\n      TRUE ~ NA_character_\n    )\n  )\n\ndf_group &lt;- df %&gt;%\n  group_by(hspd_int, int, YEAR) %&gt;%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide &lt;- df_group  %&gt;%\n  pivot_wider(\n    id_cols = c(YEAR, int),\n    names_from = hspd_int,\n    values_from = count\n  ) %&gt;%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  )\n\n\n\n\n\n\n  \n    \n      Table 3: Exploring Internet Data\n    \n    \n      These results are exploratory. But not wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    \n      Has Internet Access\n    \n    2013\n86%\n14%\n5%\n2.76M\n454K\n185K\n    2014\n84%\n16%\n6%\n2.74M\n509K\n193K\n    2015\n86%\n14%\n6%\n2.85M\n479K\n217K\n    2016\n80%\n20%\n3%\n2.87M\n708K\n129K\n    2017\n80%\n20%\n3%\n2.90M\n735K\n124K\n    2018\n79%\n21%\n3%\n2.97M\n790K\n125K\n    \n      No Internet Access\n    \n    2013\nNA\nNA\nNA\nNA\nNA\n866K\n    2014\nNA\nNA\nNA\nNA\nNA\n842K\n    2015\nNA\nNA\nNA\nNA\nNA\n753K\n    2016\nNA\nNA\nNA\nNA\nNA\n596K\n    2017\nNA\nNA\nNA\nNA\nNA\n560K\n    2018\nNA\nNA\nNA\nNA\nNA\n452K\n    \n      Internet Access is NA\n    \n    2013\nNA\nNA\nNA\nNA\nNA\n126K\n    2014\nNA\nNA\nNA\nNA\nNA\n130K\n    2015\nNA\nNA\nNA\nNA\nNA\n129K\n    2016\nNA\nNA\nNA\nNA\nNA\n131K\n    2017\nNA\nNA\nNA\nNA\nNA\n132K\n    2018\nNA\nNA\nNA\nNA\nNA\n132K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nWhen we split it out this way we can see that the groups without any internet access are all NA values. So what we were looking at was the percentage of people with internet who have high speed internet. What we want is the percentage of all people who have high speed internet. We can fix the way we create our categories by saying that anyone who has no internet also has no high speed internet.\n\ndf &lt;- df %&gt;%\n  mutate(\n    #create a categorical variable for having any internet\n    int = case_when(\n      CINETHH == 0 ~ NA_character_,\n      CINETHH == 1 | CINETHH == 2 ~ \"Yes\",\n      CINETHH == 3 ~ \"No\",\n      TRUE ~ NA_character_\n    ),\n    # change how high speed internet is defined so that houses w/o any interent are counted as 'No' instead of NA\n    hspd_int = case_when(\n      CIHISPEED == 00 & int != \"No\" ~ NA_character_,\n      CIHISPEED == 20 | int == \"No\" ~ \"No\",\n      CIHISPEED &gt;= 10 & CIHISPEED &lt; 20 ~ \"Yes\",\n      TRUE ~ NA_character_\n    )\n  )\n\n# Count numbers with and without high speed internet\ndf_group &lt;- df %&gt;%\n  group_by(hspd_int, YEAR) %&gt;%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide &lt;- df_group  %&gt;%\n  pivot_wider(\n    id_cols = c(YEAR),\n    names_from = hspd_int,\n    values_from = count\n  ) %&gt;%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  )\n\n\n\n\n\n\n  \n    \n      Table 4: Almost right!\n    \n    \n      These results are still a little wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n68%\n32%\n7%\n2.76M\n1.32M\n311K\n    2014\n67%\n33%\n7%\n2.74M\n1.35M\n324K\n    2015\n70%\n30%\n8%\n2.85M\n1.23M\n346K\n    2016\n69%\n31%\n6%\n2.87M\n1.30M\n260K\n    2017\n69%\n31%\n6%\n2.90M\n1.29M\n256K\n    2018\n71%\n29%\n6%\n2.97M\n1.24M\n257K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's incorrect analysis of IPUMS data. Used as an example of a mistake to avoid.\n    \n  \n  \n\n\n\n\nThese results look much better, although there’s still one more thing to do about those NA results."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#group-quarters-in-the-census",
    "href": "posts/census_internet_microdata/index.html#group-quarters-in-the-census",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Group Quarters in the Census",
    "text": "Group Quarters in the Census\nThe census data includes individuals living in group quarters (mostly prisons, senior living centers, and dorms, but includes any sort of communal living arrangement). However, all census questions about appliances and utilities (the category that internet access falls under) are NA for group quarters. So we’ll add one more line to filter out individuals living in group quarters (a common practice when working with Census microdata). The code below adds a filter for Group Quarters. Since this table is showing correct results I’ll also add a little additional formatting to make it stand out from the others.\nI’ll also note that the way the Census Bureau constructs weights is very convenient for getting totals. While I’m focusing on the percent of people who have internet access, the Yes and No columns are accurate estimates of the population with and without access.\n\n# Count numbers with and without high speed internet\ndf_group &lt;- df %&gt;%\n  filter(GQ == 1 | GQ ==2 | GQ == 5) %&gt;%\n  group_by(hspd_int, YEAR) %&gt;%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide &lt;- df_group  %&gt;%\n  pivot_wider(id_cols = c(YEAR), names_from = hspd_int, values_from = count) %&gt;%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd,\n         percent_NA = (`NA` / (Yes + No + `NA`)))\n\n\n\n\n\n\n  \n    \n      Table 5: High Speed Internet in Kentucky\n    \n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n68%\n32%\n4%\n2.76M\n1.32M\n185K\n    2014\n67%\n33%\n5%\n2.74M\n1.35M\n193K\n    2015\n70%\n30%\n5%\n2.85M\n1.23M\n217K\n    2016\n69%\n31%\n3%\n2.87M\n1.30M\n129K\n    2017\n69%\n31%\n3%\n2.90M\n1.29M\n124K\n    2018\n71%\n29%\n3%\n2.97M\n1.24M\n125K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nThat removed about half of our NA values. It might be nice to know a bit more about the missing data, but at around 3 percent of observations it’s unlikely to change our substantive conclusions. I suspect these are cases where there wasn’t an answer for that question. We’ll keep an eye on NA values as we do the analysis, because as we get into questions like how internet access varies by race, income, age, and education we’ll want to know if NA answers are more or less likely in any of those categories."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#checking-against-data.census.gov",
    "href": "posts/census_internet_microdata/index.html#checking-against-data.census.gov",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Checking against data.census.gov",
    "text": "Checking against data.census.gov\nTo do a quick check against the way the census bureau itself analyzes the data I looked at data.census.gov for 2018 in Kentucky. An important note is that their data is for households, and so their numeric counts look quite different because I’m counting number of people. They also have a breakdown where cellular is included in broadband, which I do not want, as a cell phone is not really an adequate work or study device. So to get to what I have we need to add “Broadband such as cable, fiber optic or DSL” and “Satellite Internet service”, which gets us to 70.8% compared to the 70.5% in this analysis. The difference is small and most likely the result of their analysis being weighted to the household level rather than the person level. (Internet is measured at the household level and the same for every person in the household, but by choosing to weight it at the person level I am a) letting us talk in terms of people, b) giving more weight to larger households, c) making it possible to break down internet access by categories that do vary within households, like age)."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#standard-errors",
    "href": "posts/census_internet_microdata/index.html#standard-errors",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Standard Errors",
    "text": "Standard Errors\nKnow that we know the data we’d also like to know how uncertain our sample is so that we know if movements over time are real or just a result of noisy data. There are a few ways to do this. The survey package does an excellent job with complex survey designs, but does require learning a new syntax to use. The alternative I’ll use here is a method known as bootstrap. IPUMS suggests using bootstrap might be the best way to get standard errors on census microdata. The basic idea of the bootstrap is to resample the existing data and use the sampling error from that as an estimate for sampling error in the overall population. Let’s do an example with high speed internet in 2018 to see how it works. The output here will be the mean and standard deviation for Kentucky. (We’ll use the standard error to calculate confidence intervals once we start displaying actual results.)\n\n#set seed\nset.seed(42)\n\n# Filter to just 2018\n# Exclude NA values\n# Recode as numeric vector of 1 and 0\n# The numeric 1 and 0 form will make it much easier to get means without pivoting, which matters a lot when doing this 1000 times\ndf2018 &lt;- df %&gt;%\n  filter(YEAR == 2018 & !is.na(hspd_int)) %&gt;%\n  mutate(hspd_num = if_else(hspd_int == \"Yes\", 1, 0)) %&gt;%\n  select(hspd_num, PERWT)\n\n# Write a function so I can map over it.\n# In this case, we need the function to do the same thing X number of times and assign an ID that we can use as a grouping variable\ncreate_samples &lt;- function(sample_id){\n  df_out &lt;- df2018[sample(nrow(df2018), nrow(df2018), replace = TRUE) , ] %&gt;%\n    as_tibble()\n  df_out$sample_id &lt;- sample_id\n  return(df_out)\n}\n\nnlist &lt;- as.list(seq(1, 1000, by = 1))\nsamples &lt;- purrr::map_df(nlist, create_samples)\n\nsample_summary &lt;- samples %&gt;%\n  group_by(sample_id) %&gt;%\n  mutate(ind_weight = PERWT / sum(PERWT),\n         hspd_weight = hspd_num * ind_weight) %&gt;% # PERWT is population and doesn't sum to 1. Rescale it to sum to one\n  summarize(group_mean = sum(hspd_weight),\n            weight_check = sum(ind_weight), .groups = \"drop\") # Check that my weights add up to one\n\ndisplay_tbl &lt;- tibble(\n  mean = mean(sample_summary$group_mean),\n  sd = sd(sample_summary$group_mean)\n) \n\n\n\n\n\n\n\n\n\nmean\nsd\n\n\n\n\n0.7053407\n0.002866898\n\n\n\n\n\n\n\nWe can also take a look at our bootstrap graphically. We want to check that the distribution of the sample is roughly normal. If it’s not, that means we didn’t do enough bootstrap samples for the Central Limit Theorem to kick in.\n\n#Check that the distribution is normal and than the middle of the distribution is close to the 70.5% we estimated had internet access above\nggplot(sample_summary, aes(group_mean)) +\n  geom_density() + theme_bw() +\n  labs(title = \"Bootstrapped means of High Speed Internet Access\",\n       x = \"Mean\", \n       y = \"Kernel Density\")"
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#checking-our-results-against-the-survey-package",
    "href": "posts/census_internet_microdata/index.html#checking-our-results-against-the-survey-package",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Checking our results against the survey package",
    "text": "Checking our results against the survey package\nAbove we found a mean of 0.705 for 2018 and and standard error of 0.0029 based on our bootstrap analysis. It’s worth checking that this is the same result we’d get using an analytic approach (instead of bootstrap).\n\n# Here we're assuming a simple design. \n# Survey requires the creation of a design object and then has functions that work with that object.\n# You can get more complicated, which is when the survey package would be most useful.\nsvy_df &lt;- svydesign(ids = ~ 1, weights = ~PERWT, data = df2018)\n\n# Taking the mean and standard error from our design object\nhint_tbl &lt;- svymean(~hspd_num, design = svy_df)\n\nhint_tbl &lt;- as_tibble(hint_tbl)\nnames(hint_tbl) &lt;- c(\"mean\", \"sd\") #The names weren't coerced correctly when transforming into a tibble. \n\n\n\n\n\n\n\n\n\nmean\nsd\n\n\n\n\n0.7051774\n0.00293509\n\n\n\n\n\n\n\nThese results are very similar. Following the IPUMS recommendation we’ll continue on with the bootstrap, but it’s good to know the results are the same for practical purposes. So now instead of just doing 2018, we’ll need to do every year. We already know the mean values for every year, and they’re still saved in the df_wide variable right now. So let’s write a function for bootstrap that will let us find standard errors for every year or for any other grouping we choose."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#writing-a-bootstrap-function",
    "href": "posts/census_internet_microdata/index.html#writing-a-bootstrap-function",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Writing a bootstrap function",
    "text": "Writing a bootstrap function\n\n# Create a helper function\n# It needs to have a way to recieve the dataframe from the function that calls it, so we've added a second argument\ncreate_samples &lt;- function(sample_id, df){\n  \n  df_out &lt;- df[sample(nrow(df), nrow(df), replace = TRUE) , ] %&gt;%\n    as_tibble()\n  \n  df_out$sample_id &lt;- sample_id\n  \n  return(df_out)\n}\n\n# Need to be able to take in grouping variables so that the summaries can be specific to the groups\n# Using the embrace {{}} notation allows use to pass in unquoted variables to the function. \n\nbootstrap_pums &lt;- function(df, num_samples, group_vars) {\n  \n  nlist &lt;- as.list(seq(1, num_samples, by = 1))\n  samples &lt;- purrr::map_df(nlist, create_samples, df)\n  \n  sample_summary &lt;- samples %&gt;%\n    group_by( sample_id, across( {{group_vars}} )) %&gt;%\n    mutate(ind_weight = PERWT / sum(PERWT),\n           hspd_weight = hspd_n * ind_weight) %&gt;% # PERWT sums to population instead of to 1. Rescale it to sum to 1.\n    summarize(group_mean = sum(hspd_weight), .groups = \"drop\") # Not dropping .groups here results in problems in the next group_by call.\n  \n  sample_sd &lt;- sample_summary %&gt;%\n    group_by( across( {{ group_vars }} )) %&gt;%\n    summarize(sd = sd(group_mean), .groups = \"drop\")\n}\n\n# We do need to prep the data a little so that we're not carrying through the whole dataframe.\ndf_in &lt;- df %&gt;%\n   filter(!is.na(hspd_int)) %&gt;%\n   mutate(hspd_n = if_else(hspd_int == \"Yes\", 1, 0)) %&gt;%\n   select(hspd_n, PERWT, YEAR)\n\n# And finally we can call the function\nboot_results &lt;- bootstrap_pums(df = df_in, num_samples = 500, group_vars = YEAR)\n\nNow that we have our bootstrap standard errors we can combine them with the data and plot them. We’ll use 95% confidence intervals, which we get by multiplying the standard error by 1.96 (the number of standard deviations that corresponds to a 95% confidence interval).\n\ndf_plt &lt;- df_wide %&gt;%\n  full_join(boot_results, by = \"YEAR\") %&gt;%\n  transmute(Year = YEAR,\n            Percent = 100 * percent_hspd,\n            me = 100 * 1.96 * sd)\n  \nplt_int &lt;- ggplot(df_plt, aes(x = Year, y = Percent)) +\n  geom_errorbar(aes(ymin = Percent - me, ymax = Percent + me), width = .1) +\n  geom_line() +\n  geom_point() +\n  theme_bw() +\n  labs(title = \"High Speed Internet Access\") +\n  theme(legend.position = \"bottom\")\n\nplt_int"
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#race-poverty-age-and-geography",
    "href": "posts/census_internet_microdata/index.html#race-poverty-age-and-geography",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Race, Poverty, Age, and Geography",
    "text": "Race, Poverty, Age, and Geography\nGoing a little deeper we can use microdata to get results by custom groupings. I’ll show examples for race, poverty, age, and geography, but for any group you can construct with available Census data you can produce an estimate for their internet acess.\n\nRace\nNext we’ll build a table by race and year.\n\n# Let's build a table first and then we'll do the standard errors\n\n# Coding a race variable using case_when\ndf &lt;- df %&gt;%\n  mutate(race = case_when(\n            RACE == 1 ~ \"White\",\n            RACE == 2 ~ \"Black\",\n            RACE &gt; 3 & RACE &lt; 7 ~ \"Asian\",\n            HISPAN &gt; 0 & HISPAN &lt; 5 ~ \"Hispanic\",\n            TRUE ~ \"All Others\"\n          ))\n\ndf_group &lt;- df %&gt;%\n  group_by(hspd_int, race, YEAR) %&gt;%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide &lt;- df_group  %&gt;%\n  pivot_wider(id_cols = c(race, YEAR), names_from = hspd_int, values_from = count) %&gt;%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd,\n         percent_NA = (`NA` / (Yes + No + `NA`)))\n\n\n\n\n\n\n  \n    \n      Table 6: High Speed Internet Access\n    \n    \n      By Race and Ethnicity\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    \n      Asian\n    \n    2013\n84%\n16%\n4%\n42.2K\n8.03K\n2.06K\n    2014\n76%\n24%\n2%\n41.6K\n12.9K\n1.19K\n    2015\n83%\n17%\n3%\n48.8K\n10.3K\n1.76K\n    2016\n77%\n23%\n10%\n43.8K\n13.0K\n6.55K\n    2017\n80%\n20%\n1%\n52.5K\n12.9K\n703\n    2018\n82%\n18%\n3%\n52.8K\n11.7K\n1.66K\n    \n      Black\n    \n    2013\n64%\n36%\n9%\n192K\n106K\n28.7K\n    2014\n58%\n42%\n9%\n171K\n122K\n30.3K\n    2015\n62%\n38%\n12%\n183K\n113K\n40.5K\n    2016\n62%\n38%\n5%\n204K\n127K\n15.6K\n    2017\n64%\n36%\n2%\n216K\n122K\n8.04K\n    2018\n62%\n38%\n3%\n200K\n122K\n9.90K\n    \n      Hispanic\n    \n    2013\n48%\n52%\n4%\n21.6K\n23.8K\n1.90K\n    2014\n38%\n62%\n12%\n15.7K\n25.2K\n5.66K\n    2015\n47%\n53%\n10%\n17.2K\n19.5K\n4.01K\n    2016\n59%\n41%\n4%\n23.0K\n15.8K\n1.59K\n    2017\n57%\n43%\n1%\n24.7K\n18.5K\n421\n    2018\n59%\n41%\n1%\n30.9K\n21.1K\n616\n    \n      White\n    \n    2013\n68%\n32%\n4%\n2.45M\n1.16M\n148K\n    2014\n68%\n32%\n4%\n2.44M\n1.16M\n152K\n    2015\n70%\n30%\n4%\n2.54M\n1.07M\n164K\n    2016\n69%\n31%\n3%\n2.54M\n1.13M\n102K\n    2017\n70%\n30%\n3%\n2.54M\n1.11M\n113K\n    2018\n71%\n29%\n3%\n2.61M\n1.06M\n108K\n    \n      All Other Races\n    \n    2013\n72%\n28%\n6%\n58.3K\n22.3K\n4.90K\n    2014\n70%\n30%\n4%\n65.9K\n27.7K\n3.91K\n    2015\n71%\n29%\n7%\n59.6K\n24.3K\n6.19K\n    2016\n75%\n25%\n4%\n63.2K\n21.3K\n3.12K\n    2017\n71%\n29%\n2%\n68.9K\n28.2K\n2.23K\n    2018\n72%\n28%\n5%\n73.9K\n29.4K\n5.28K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nWhile we do see high NA values in some years, overall they’re at reasonable levels and seem to be lowest in 2018. This table is quite long though. While that works for exploring the data, for an actual display we’d probably want to focus on just 2018 and show the history in a graph. Below is the table filtered to just 2018 and slightly reformatted.\n\n\n\n\n\n  \n    \n      Table 7: High Speed Internet Access in 2018\n    \n    \n      By Race and Ethnicity\n    \n  \n  \n    \n      Race/Ethnicity\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    All Others\n72%\n28%\n5%\n73.9K\n29.4K\n5.28K\n    Asian\n82%\n18%\n3%\n52.8K\n11.7K\n1.66K\n    Black\n62%\n38%\n3%\n200K\n122K\n9.90K\n    Hispanic\n59%\n41%\n1%\n30.9K\n21.1K\n616\n    White\n71%\n29%\n3%\n2.61M\n1.06M\n108K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nNow let’s add standard errors and graph the data. I’ll write the code for graphing the data as a function since we will use it again.\n\n# We do need to prep the data a little so that we're not carrying through the whole dataframe.\ndf_in &lt;- df %&gt;%\n  filter(!is.na(hspd_int)) %&gt;%\n  mutate(hspd_n = if_else(hspd_int == \"Yes\", 1, 0)) %&gt;%\n  select(hspd_n, PERWT, YEAR, race)\n\n# And we can call the bootstrap function\nboot_results &lt;- bootstrap_pums(df = df_in, num_samples = 500, group_vars = c(YEAR, race))\n\ndf_plt &lt;- df_wide %&gt;%\n  full_join(boot_results, by = c(\"race\", \"YEAR\")) %&gt;%\n  transmute(Year = YEAR,\n            Race = race,\n            Percent = 100 * percent_hspd,\n            me = 100 * 1.96 * sd) %&gt;%\n  filter(Race != \"All Others\") # When plotting All Others overlaps White and having five lines makes it quite hard to read. \n\n# At this point I'll introduce a function to plot multiple groups over time, since we'll use this again \nplt_by &lt;- function(df, group_var, title_text = \"\") {\n  \n  plt &lt;- ggplot(data = df, aes(x = Year, y = Percent, group = {{group_var}}, colour = {{group_var}})) +\n    geom_errorbar(aes(ymin = Percent - me, ymax = Percent + me), width = .1) +\n    geom_point() +\n    geom_line() +\n    theme_bw() +\n    labs(title = title_text, x = \"Year\", y = \"Percent\") +\n    theme(legend.position = \"bottom\")\n\n  plt\n}\n\nplt_race &lt;- plt_by(df_plt, Race, title_text = \"High Speed Internet Access by Race and Ethnicity\")\n\nplt_race\n\n\n\n\n\n\n\n\n\n\nPoverty Status\nI’m going to skip showing the code for Poverty, Age, and Geography because it’s extremely similar to the code used for Race. The poverty variable in the IPUMS data is measured in income as a percent of the poverty line. So for this analysis I code under 100 as being in poverty, between 100 and 200 percent of the poverty line as near poverty, and above 200 percent as not being in poverty.\n\n\n\n\n\n  \n    \n      Table 8: High Speed Internet Access in 2018\n    \n    \n      By Poverty Status\n    \n  \n  \n    \n      Poverty Status\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    In Poverty\n53%\n47%\n4%\n376K\n335K\n30.6K\n    Near Poverty\n62%\n38%\n4%\n499K\n311K\n29.7K\n    Not in Poverty\n78%\n22%\n2%\n2.09M\n595K\n64.8K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\nThe age varible in IPUMS is the most straightforward, it’s just numeric age. It is worth pointing out that this is based on individual ages, while high speed internet is a household level variable. If we really wanted to do a deep dive into just age, we’d want to look at the age composition of the whole household. The really nice thing about microdata is you can slice and dice it any way you think is appropriate. So if you want to look at households that have individuals under 18 and over 65, you can. If you want to look at households with no one under 65, you can. I’ve just taken a high level cut of the data here though.\n\n\n\n\n\n  \n    \n      Table 9: High Speed Internet Access in 2018\n    \n    \n      By Age Group\n    \n  \n  \n    \n      Age Group\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    18 and under\n74%\n26%\n2%\n760K\n263K\n26.0K\n    19 to 64\n72%\n28%\n3%\n1.81M\n691K\n76.7K\n    65+\n58%\n42%\n3%\n398K\n288K\n22.4K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeography\nIPUMs provides categorizations of where people live as either being in a prinicipal city, in the metro but not the principal city, or outside of a metro area. I have chosen to present this as the more familiar (and shorter) urban, suburban, and rural. IPUMS also has a direct measure of density that would be useful in an analysis - but for tables and graphs the categorial variable is better.\n\n\n\n\n\n  \n    \n      Table 10: High Speed Internet Access in 2018\n    \n    \n      By Metropolitan Status\n    \n  \n  \n    \n      Metropolitan Status\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    City\n85%\n15%\n3%\n257K\n46.2K\n8.16K\n    Mixed/Unknown\n68%\n32%\n3%\n1.60M\n752K\n74.7K\n    Rural\n67%\n33%\n3%\n694K\n341K\n35.4K\n    Suburbs\n81%\n19%\n1%\n420K\n102K\n6.77K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#school-age-children",
    "href": "posts/census_internet_microdata/index.html#school-age-children",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "School Age Children",
    "text": "School Age Children\nAs I’ve mentioned, a nice feature of using microdata is that you can look at the data in ways that aren’t available in the premade Census tabulations. With a lot of school districts using online learning, a look at the percent of school age children (ages 5-18) without high speed internet access at home could be useful for policymakers. Here we can see that there are parts of Western Kentucky where up to 60% do not have high speed access at home.\n\n#need to recreate the sf object for the joins to work correctly\nky_sf &lt;- st_as_sf(ky_shp)\n\n# Calculate internet access at the PUMA level\ndf_group &lt;- df %&gt;%\n  filter(YEAR == 2018 & AGE &gt;= 5 & AGE &lt;= 18) %&gt;%\n  group_by(hspd_int, PUMA) %&gt;%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide &lt;- df_group  %&gt;%\n  pivot_wider(id_cols = PUMA, names_from = hspd_int, values_from = count) %&gt;%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd)\n\nint_puma &lt;- tibble(\n  PUMA = df_wide$PUMA,\n  int_per = df_wide$percent_no * 100,\n  int_num = df_wide$No\n  )\n\nky_sf &lt;- full_join(ky_sf, int_puma, by = \"PUMA\")\n\nggplot(ky_sf) + \n  geom_sf(aes(fill=int_per)) +\n  scale_fill_gradient(low = \"blue\", high = \"purple\", name = \"Percent\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        panel.border = element_blank()) +\n  labs(title = \"Children ages 5-18 in Households without High Speed Internet Access\",\n       caption = \"Map is shaded by percent of children without high speed access in each Public Use Microdata Area\")\n\n\n\n\n\n\n\n\n\nLouisville (Jefferson County) with Labels\n\n#need to recreate the sf object for the joins to work correctly\nky_sf &lt;- st_as_sf(ky_shp) %&gt;%\n  filter(PUMA %in% c(\"1701\", \"1702\", \"1703\", \"1704\", \"1705\", \"1706\"))\n\n# Calculate internet access at the PUMA level\n# Filter to only the PUMAs in Jefferson County\ndf_group &lt;- df %&gt;%\n  filter(YEAR == 2018 & AGE &gt;= 5 & AGE &lt;= 18 & PUMA %in% c(\"1701\", \"1702\", \"1703\", \"1704\", \"1705\", \"1706\")) %&gt;%\n  group_by(hspd_int, PUMA) %&gt;%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide &lt;- df_group  %&gt;%\n  pivot_wider(id_cols = PUMA, names_from = hspd_int, values_from = count) %&gt;%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd)\n\nint_puma &lt;- tibble(\n  PUMA = df_wide$PUMA,\n  int_per = df_wide$percent_no * 100,\n  int_num = formattable::comma(round(df_wide$No, -2), digits = 0)\n  )\n\nky_sf &lt;- full_join(ky_sf, int_puma, by = \"PUMA\")\n\nggplot(ky_sf) + \n  geom_sf(aes(fill=int_per)) +\n  geom_sf_label(aes(label = int_num, fontface = \"bold\")) +\n  scale_fill_gradient(low = \"blue\", high = \"purple\", name = \"Percent\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        panel.border = element_blank()) +\n  labs(title = \"Children ages 5-18 in Households without High Speed Internet Access in Jefferson County\",\n       caption = \"Map is shaded by percent of children without access in each Public Use Microdata Area and \n       the number of children without access is given by the label rounded to the nearest 100\")\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data"
  },
  {
    "objectID": "posts/census_map/index.html",
    "href": "posts/census_map/index.html",
    "title": "Mapping Census Data with Python",
    "section": "",
    "text": "This post covers two useful skils, making maps and accessing Census data. Most of the python map making tutorials I found online showed how to make maps using data that already came with the library, and didn’t have many notes on how the data is formatted or how to bring in new data.\nIn addition to the standard analysis libraries numpy and pandas, we’re going to use GeoPandas for mapping data and the requests library for getting Census data\n\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport requests\n\n\nThe Initial Map Layer\nThe census has shapefiles here and I’ve downloaded a zip file of all counties in the United States. I’ll filter it down to just Kentucky for this example.\nGeoPandas works well with census shapefiles out of the box. Don’t unzip the downloaded file, it can be read straight into a GeoPandas dataframe, which resembles a pandas dataframe, but with an additional column of geographic attributes.\n\ncounty_shp = gpd.read_file('cb_2021_us_county_500k.zip')\nky_counties = county_shp[county_shp.STATE_NAME == 'Kentucky']\n\nGeopandas also provides built-in integrations with both matplotlib (for static maps) and folium (for interactive maps). Folium is a python wrapper for leaflet, which produces interactive maps. Producing the initial map layer is straightforward using the built in explore method.\n\nky_counties.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nGetting Census Data\nI’m going to use internet access as our example data. The American Community Survey collects information about internet access in table S2801. However, downloading data from data.census.gov is a formatting nightmare, because there’s no clear geography column to match on and the data seems to only be available in an oddly nested structure. Instead, I’m going to use the Census API.\nI used data.census.gov to figure out what tables contained information on internet access. Then I found the variables table for ACS 5 year data and searched for table S2801. It wasn’t there, but B28003 was and contains the same information in raw counts without the summary percentages. It’ll be easy enough to recompute the percentages.\nI want to know what percent of households have internet access, not including cell phone only internet access or dial-up. Variable B28002_007E tells me how many households have an internet subscription that isn’t dial up or cellular. Variable B28002_001E gives me the total population.\nThere is a census python package, but it doesn’t make things that much simpler than it is to just use the API directly using the more popular and generic python library requests. You need your own census API key, which is available here. I saved mine in a text file to avoid putting it into the blog directly.\nPutting together the request to the api involves constructing a long url string out of multiple pieces. An advantage of pasting together the pieces from separate variables is that it makes it easier to modify this example to suit your use case.\n\n#load API key\nwith open('census_api_key.txt') as key:\n    api_key=key.read().strip()\n\n#specify the data source by year and survey\nyear = '2021'\ndsource = 'acs' #American Community Survey\ndname = 'acs5' #5 year average from American Community Survey\nbase_url = f'https://api.census.gov/data/{year}/{dsource}/{dname}'\n\n#unique to this specific data request\ncols = 'NAME,B28002_001E,B28002_007E' #NAME of geography as well as the variables I want to pull\ngeo = 'county' #county geography level\nstate = 'state:21' #21 is the FIPS code for Kentucky\n\n#add unique request features to the base_url\ndata_url = f'{base_url}?get={cols}&for={geo}&in={state}&key={api_key}'\n\n#go get the data\nresponse = requests.get(data_url)\n\n#take the data in json and format it into a dataframe\ndata = response.json()\ndf = (pd.DataFrame(data = data[1:], columns = data[0]) #first row is column names, everything else is data.\n        .rename(columns = {'NAME' : 'county_name',\n                           'B28002_001E' : 'population',\n                           'B28002_007E': 'pop_int_access',\n                           'state' : 'state_fips',\n                           'county' : 'county_fips'}))\n\n#the fips for a county is a concatenation of state and county fips\ndf['fips'] = df['state_fips'] + df['county_fips'] #make sure these are strings so it concatenates and doesn't add. \n\n#changing the data to be numeric, since everything starts as string\ndf[['population', 'pop_int_access']] = df[['population', 'pop_int_access']].apply(pd.to_numeric)\n\n#calculating percent with internet access\ndf['percent_int'] = np.round(100 * (df['pop_int_access']/df['population']), 1)\n\ndf.head()\n\n\n\n\n\n\n\n\ncounty_name\npopulation\npop_int_access\nstate_fips\ncounty_fips\nfips\npercent_int\n\n\n\n\n0\nAdair County, Kentucky\n6890\n3989\n21\n001\n21001\n57.9\n\n\n1\nAllen County, Kentucky\n7629\n4559\n21\n003\n21003\n59.8\n\n\n2\nAnderson County, Kentucky\n9063\n5032\n21\n005\n21005\n55.5\n\n\n3\nBallard County, Kentucky\n2945\n1610\n21\n007\n21007\n54.7\n\n\n4\nBarren County, Kentucky\n17307\n12309\n21\n009\n21009\n71.1\n\n\n\n\n\n\n\n\n\nMapping the Data\nNow that we have our data, we need to combine it back into our map dataset. A nice thing about GeoPandas is that we can work with it the same way we work with a pandas dataframe.\nWe’ll still use the explore method from GeoPandas to map, and to create a chloropleth map we set the column argument to the name of the column we want to use to shade the map. We can specify what information goes in the tooltip by passing a list of column names. I’ve also set a few other options to change the map background to something a bit more muted and bring out the lines between the counties a bit more cleanly.\n\n#construct the fips code to match with the geography data\n#matching on fips is generally preferable to matching on county name because of potential formatting differences in names. \nky_counties['fips'] = ky_counties['STATEFP'] + ky_counties['COUNTYFP']\n\n#merge in the new data\n#clean up the names to be a bit more presentable\ndf_map = (ky_counties.merge(df, how = 'left', on = 'fips')\n                     .rename(columns = {'NAME' : 'County Name',\n                                       'percent_int' : 'Home Internet Access (%)',\n                                       'population' : 'County Population'}))\n\ndf_map.explore(\n    column = 'Home Internet Access (%)',\n    tooltip = ['County Name', 'Home Internet Access (%)', 'County Population'],\n    tiles = 'CartoDB positron', #fades into the background better than the default\n    style_kwds=dict(color=\"black\") #outlines stay black for a crisper feel\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nTract level analysis\nTo show the flexibility of this approach and out of personal curiosity, let’s switch from county level data to tract level data and zoom in on Louisville, Kentucky (Jefferson County). The Census Bureau’s example API queries are extremely useful once you’ve mastered the basics. Here we have to change the geography request from county to tract, and then add a filter to restrict it to Jefferson County, Kentucky.\nThe tract numbers aren’t easily interpretable, which is one place where the interactive background map can be very helpful. You can zoom in and see the default neighborhood names in the background map, as well as major roads and interstates that allow people familiar with Louisville to understand where each census tract is located.\nThe code here is a little repetitive with the code for Kentucky counties, and if we were doing this frequently we’d want to factor out the common parts into functions (i.e. a function that did the routine data processing post-API call and another function that set all the default styles for the map).\n\n## First we need a tract level map, downloaded from the same census collection\n\n# if downloading from census, read in as a zip file: https://geopandas.org/en/stable/docs/user_guide/io.html\nky_shp = gpd.read_file('cb_2021_21_tract_500k.zip')\nlou_shp = ky_shp[ky_shp['COUNTYFP'] == '111']\n\n## Now pulling Census data\n\n#we can use the same base_url, but the data_url needs to be updated\n#the variables we want to pull stay the same\ncols = 'NAME,B28002_001E,B28002_007E' #NAME of geography as well as the variables I want to pull\ngeo = 'tract' #county geography level\nstate = 'state:21' #21 is the FIPS code for Kentucky\ncounty = 'county:111' #111 is the FIPs code for Jefferson County, KY\n\n#add unique request features to the base_url\ndata_url = f'{base_url}?get={cols}&for={geo}&in={state}&in={county}&key={api_key}'\n\n#go get the data\nresponse = requests.get(data_url)\n\n#clean up the data into a dataframe\ndata = response.json()\n\ndf = (pd.DataFrame(data = data[1:], columns = data[0]) #first row is column names, everything else is data.\n        .rename(columns = {'NAME' : 'tract_name',\n                           'B28002_001E' : 'population',\n                           'B28002_007E': 'pop_int_access',\n                           'state' : 'state_fips',\n                           'county' : 'county_fips',\n                           'tract' : 'tract_fips'}))\n\n#the fips for a tract is a concatenation of state, county, and tract fips\ndf['fips'] = df['state_fips'] + df['county_fips'] + df['tract_fips'] #make sure these are strings so it concatenates and doesn't add. \n\n#changing the data to be numeric, since everything starts as string\ndf[['population', 'pop_int_access']] = df[['population', 'pop_int_access']].apply(pd.to_numeric)\n\n#calculating percent with internet access\ndf['percent_int'] = np.round(100 * (df['pop_int_access']/df['population']), 1)\n\n#tract fips is in our shape file already as GEOID\nlou_map = (lou_shp.merge(df, how = 'left', left_on = 'GEOID', right_on = 'fips')\n                    .rename(columns = {'tract_name' : 'Tract Number',\n                                       'percent_int' : 'Home Internet Access (%)',\n                                       'population' : 'Tract Population'}))\n\n#tracts are pretty small in the map, so I've opted against the black boundary outlines\nlou_map.explore(\n    column = 'Home Internet Access (%)',\n    tooltip = ['Tract Number', 'Home Internet Access (%)', 'Tract Population'],\n    tiles = 'CartoDB positron' #fades into the background better than the default\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nMore on Internet Access\nWhen schools were moving to remote learning during Covid, I made a much longer post on how to use Census microdata to analyze at an individual level how internet access relates not only to geography, but also to race, poverty, and age. Internet access remains an important consideration. There seems to me to be an assumption that internet access is widespread in the United States. While people may have some access through libraries and cell phone data, there is a large gap in terms of who has reliable home internet access that can be used for work and homework."
  },
  {
    "objectID": "posts/collider_bias/index.html",
    "href": "posts/collider_bias/index.html",
    "title": "Think before adding more variables to that analysis",
    "section": "",
    "text": "Introduction\n\n\nOverview\nHuman beings think in terms of stories and in terms of how the actions they take impact the things around them. It’s our natural default way of thinking, and generally it’s pretty useful.\nDoing data analysis doesn’t stop us from thinking in terms of stories and causation, but it should make us careful. With the increase in data and in the computing power to process it all, there have been claims that all we need in order to understand and act in the world is to listen to the data. But data does not speak for itself! It is interpretted by humans who will think about it through the lens of causality.\nThis is an introduction to thinking about causal models for data analysis. The purpose is to demonstrate that the popular approach of simply gathering as much data as you can and controlling for it via regression or other methods is not a good one, and is actively misleading in many cases. We should instead carefully think about plausible causal models using tools like diagrams (directed acyclic graphs, or DAGs) and then do data analysis in accordance with those models.\n\n\nA Simple Example of Confounding\nLet’s start with an example where using regression does make sense. I have noticed that the sports teams I like are more likely to lose when I am watching them on TV. This is true, but the idea that my watching them causes them to lose is not plausible. So either I’m mistaken in my data collection, very unlucky in my fanship (I am a fan of Cleveland sports teams, so this does seem likely), or there’s something else that explains the connection between my watching and my team losing. We can draw a simple diagram of what we’ve observed so far.\n(I’m using Mermaid and will put the code for each diagram above them so that it’s easy to recreate and edit later).\ngraph LR; A[Watch Game]–&gt;B[Lose Game]\n\nThe problem is I’m not equally likely to watch every game. I’m more likely watch major rivalry games or games against good opponents. I don’t usually watch UK basketball play their beginning of season games against teams I’ve never heard of. I watch them play against UNC and Louisville, or in the March Madness tournament. We can shorthand this as being ‘opponent quality.’ I’m more likely to watch games where UK plays strong opponents and UK is more likely to lose those games - not because I’m watching, but because they’re playing against better than average teams. We can diagram this below, showing that it’s opponent quality that has a causal connection both to my team losing and to me watching them lose. There’s no causal connection at all between my watching and their losing, something else (opponent quality) causes both things creating a correlation that we can observe, but should not mistake for causation.\n\nOnce we know what the proper causal model looks like, we can see that the conclusion that my watching games caused my teams to lose was based on an incomplete view - or more technically it suffered from omitted variable bias. The analysis left out an important variable that impacted things. Once we control for opponent quality, the relationship between my watching and my team losing should go back to zero.\n\n\nA Much More Important Example: Women’s Wages\nThe idea of drawing out the diagram before doing the analysis can be applied to more important cases, like the ongoing dispute around the wage gap between men and women. Here, I’m taking an example from the excellent book Causal Inference: The Mixtape by Scott Cunningham.\nWhen companies are accused of paying women less one of their first lines of defense is to argue that if you account for the occupational differences within the company between men and women the wage gap vanishes or at least shrinks dramatically. Cunningham (and I) think this is a poor causal model and an inadequate defense. This is important, so we’re going to consider several causal models and look directly at what they tell us using some simulated data under different specifications. Using simulated data gives us the advantage of knowing the truth of the data - so to speak - we’ll create it to have certain causal relationships and then we’ll see how the different models capture (and fail to capture) those relationships.\nI’ll start with the causal diagram that we’re going to use to simulate our data. It’s a bit complicated, but we’ll take it piece by piece as we move through the data simulation and modeling.\ngraph LR; D[Discrimination] –&gt; E[Earnings] D –&gt; O[Occupation] O –&gt; E A[Ability] -.-&gt; O A -.-&gt; E\n\n\nimport numpy as np # for generating arrays with random numbers\nimport pandas as pd # dataframes\nimport statsmodels.api as sm # to run the actual ols model\n\nnp.random.seed(42) # to make it reproducible\n\nWe’re going to first generate a labor force where half of it is discriminated against (e.g. women being paid less, the well known gender gap in wages) and has ability randomly distributed. In the causal model sketched above both Discrimination and Ability are root causes - they’re not caused by anything else in the diagram. (Both obviously have causes outside of the system we’re currently considering). So that’s the place we’ll start.\n\ngenerated_data = {\n    'discrimination'  : np.random.randint(low = 0, high = 2, size = 10000, dtype = int), #the high argument is not inclusive, so this is randomly generating 0s and 1s. \n    'ability' : np.random.normal(size = 10000),\n}\n\ndf = pd.DataFrame(data = generated_data)\n\nNow we need to generate some other variables of interest. We’re looking at the impact of discrimination, so let’s set that to be experienced by half of the labor force. We’re going to assume that discrimination affects both wages and choice of occupation. Here we’re worried about occupations in terms of higher and lower pay scales, so let’s set occupations to be positively associated with ability and negatively associated with discrimination.\nFinally, wages are negatively associated with discrimination and positively associated with both occupation and ability.\n\ndf['occupation'] = 1 + 2 * df['ability'] - 2 * df['discrimination'] + np.random.normal(size = 10000)\ndf['wage'] = 1 - 1 * df['discrimination'] + 1 * df['occupation'] + 2 * df['ability'] + np.random.normal(size = 10000)\n\ndf.describe()\n\n\n\n\n\n\n\n\ndiscrimination\nability\noccupation\nwage\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n0.498700\n-0.008041\n-0.009388\n0.471065\n\n\nstd\n0.500023\n1.004178\n2.449597\n4.545405\n\n\nmin\n0.000000\n-3.922400\n-10.018905\n-18.328506\n\n\n25%\n0.000000\n-0.674327\n-1.640437\n-2.517222\n\n\n50%\n0.000000\n-0.007682\n-0.022777\n0.482132\n\n\n75%\n1.000000\n0.668901\n1.633467\n3.501387\n\n\nmax\n1.000000\n3.529055\n9.500154\n16.731628\n\n\n\n\n\n\n\nNow that we have our simulated data with specified causal relationships, let’s look at a few different regression models. We’ll first look at a model that only includes discrimination as a cause of wages.\n\n# Set up matrices for regression\nY = df['wage']\nX1 = df['discrimination']\n\n# add constant for the intercept of the model\nX1 = sm.add_constant(X1)\n\n# specify the model\nmodel1 = sm.OLS(Y, X1)\n\n# fit the model\nresults1 = model1.fit()\n\n# Look at results\n# results1.summary()\nresults1.params\n\nconst             1.952182\ndiscrimination   -2.969956\ndtype: float64\n\n\nWhat we’re mainly interested in is the coefficient on discrimination. Here we see that being discriminated against has a strong negative impact on wages earned. (Don’t worry about the const (constant) term, it’s not important in this example).\nThis isn’t a surprise based on how we set up the data. It also correctly reflects that in the real world if you just divide wages by gender you will find a large gender gap.\nThe dispute comes in when we talk about controlling for occupation, or a model that looks like this:\ngraph LR; D[Discrimination] –&gt; E[Earnings] D –&gt; O[Occupation] O –&gt; E\n\nIn this model it looks like being discriminated against might raise wages slightly. We know that’s not right since we know we set up the data to have discrimination decrease earnungs. The problem is that when we added occupation to the model we opened up a brand new causal pathway from discrimination to earnings. It’s the one that runs from Discrimination–&gt;Occupation–&gt;Ability–&gt;Earnings in our original causal model.\nWhen we controlled for occupation we did two things:\n\nIgnored the fact that occupational choice is also a result of discrimination and as a defense of pay discrimination it would then be the mechanism by which discrimination happens, not a defense that discrimination isn’t happening.\nOpened up a causal pathway that made our estimates worse.\n\n\n\nX3 = df[['discrimination', 'occupation', 'ability']]\nX3 = sm.add_constant(X3)\nmodel3 = sm.OLS(Y, X3)\nresults3 = model3.fit()\nresults3.params\n\nconst             0.988717\ndiscrimination   -0.986841\noccupation        1.025762\nability           1.975298\ndtype: float64\n\n\n\n\nWhat have we learned?\nA major problem is that in the real world we can’t observe ability directly and put it in a regression model. Another issue is that this causal model is still very incomplete. Nonetheless, the way the sign flips back and forth depending on the model is hopefully an illustration of why it’s so important to have a theoretical model and not just throw in as much data as possible.\nData is a powerful way to tell stories, but data by itself never tells us everything we need to know. We have to interpret it carefully and think hard about the underlying models of the world we’re bringing to the data when we interpret it.\nTwo things to remember from this post:\n\nThink about the causal model before doing statistics or machine learning\nDon’t believe companies that say the gender gap goes away if you control for other things. That’s only true if you believe the causal model underlying their analysis - and you probably shouldn’t."
  },
  {
    "objectID": "posts/louisville_zoning/index.html",
    "href": "posts/louisville_zoning/index.html",
    "title": "End Exclusionary Zoning",
    "section": "",
    "text": "It is illegal to build affordable housing in most of Louisville. While there are a lot of contributing factors to the shortage of affordable housing in the U.S., one major reason it exists is because of policies we have intentionally put in place to reduce the supply of housing out of fear of a possible decline in property values and having to share ‘our neighborhoods’ with people of different classes and races. It is illegal to redline a map and explicitly exclude people from buying in certain neighborhoods on the basis of race, but it is a legal and widespread practice to use zoning ordinances to restrict the supply of affordable housing. In practice, zoning restrictions ensure high levels of racial and economic segregation.\n\n\nMulti-family housing includes a lot more than just large apartment complexes. There’s a wide range of types of housing that can include multiple families. A review of Louisville’s current housing situation refers to this as the ‘missing middle housing’. Louisville currently has limited areas zoned for large apartment complexes and huge swathes of land dedicate exclusively to single family housing.\n\n\n\nAn Illustration of the types of Housing made illegal by Louisville’s current zoning\n\n\nSingle family housing starts out less affordable because it requires more land per person, but additional zoning restrictions around minimum lot sizes and required parking spaces also drive up the cost. If you want to learn a little more about zoning and housing prices in general, this is a good 10 minute video explaining the key ways restricted zoning makes housing less affordable."
  },
  {
    "objectID": "posts/louisville_zoning/index.html#zoning-in-louisville",
    "href": "posts/louisville_zoning/index.html#zoning-in-louisville",
    "title": "End Exclusionary Zoning",
    "section": "Zoning in Louisville",
    "text": "Zoning in Louisville\nLouisville’s zoning data is available online and is regularly updated. I used the most recent data available for download (it’s a little unclear if it’s from 2020 or 2022 in the documentation, but it’s recent enough for this analysis) and divided the official zoning designations into five broad categories:\n\nMulti-Family residential: affordable housing is legal in these areas\nSingle Family Residential: restricting housing supply by mandating single family homes\nSingle Family Large Lot: highly restrictive zoning that requires a 9,000 sq. ft. lot size for single family dwellings.\nCommercial/Industrial: includes enterprise zones and anything else clearly commercial\nOther: several very small categories that are most likely related to commercial use, although this does include the ~1 percent of land that is zoned for both commercial and residential uses\n\nAn overview of zoning types is provided by Louisville Government, and there is also an official map with additional details.\n\nLouisville is not zoned for affordable housing\n\nThe overwhelming majority (73%) of Louisville’s land is reserved for single family dwellings.\nOnly 6% of Louisville’s land allows multi-family structures to be built, even if we restrict the analysis to residential land it’s only 7% of residential land that allows multi-family dwellings. In other words, 93 percent of residential land in Louisville is reserved for more expensive single family units\nThe most common single-family zoning code (R4 zoning) requires a 9,000 sq ft lot size, a requirement first adopted by the city in 1963. This type of zoning makes up a over half of Louisville’s land and makes even single family homes more expensive than they would be if they could be built on smaller lots.\n\n\n\n\nZoning Map\nWe can also look at where multi-family dwellings are allowed. The small green patches represent multi-family housing. Typing an address into the search bar will place a marker on the map locating that address, making it easy to find the zoning for your residence.\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/louisville_zoning/index.html#ending-exclusionary-zoning",
    "href": "posts/louisville_zoning/index.html#ending-exclusionary-zoning",
    "title": "End Exclusionary Zoning",
    "section": "Ending Exclusionary Zoning",
    "text": "Ending Exclusionary Zoning\nI was inspired to look at Louisville’s zoning regulations after reading Matthew Desmond’s new book Poverty, By America. It’s a fantastic book, and one thing he emphasizes throughout is the many different ways in which we construct barriers that are intended to keep the poor in poverty. Modern zoning acts as a wall to keep people out.\n\nYou can learn a lot about a town from its walls. Our first walls were primitive things: sharpened tree trunks, mud and stone. We learned to dig trenches and build parapets. Someone in the American West invented barbed wire. Today, we fashion our walls out of something much more durable and dispiriting: money and laws. Zoning laws govern what kind of property can be built in a community, and because different kinds of properties generally house different kinds of people, those laws also govern who gets in and who does not. Like all walls, they determine so much; and like all walls, they are boring. There may be no phrase more soulless in the English language than “municipal zoning ordinance.” Yet there is perhaps no better way to grasp the soul of a community than this. (p. 113 - 114)\n\nThere is good social science research on the incredibly beneficial effect of moving to lower-poverty neighborhoods - even if the income of the individual moving is not changed. These moves are beneficial for adults, and they are especially beneficial for children. As Desmond points out, it almost feels silly to need to cite research to show that the environment children are in matters to them. Of course it does - that’s why those of us who have children and are lucky enough to have choices spend so much time and money on shaping our children’s environment, their schools and their neighborhood.\nDeconcentrating poverty and ending neighborhood segregation is one of the first things we must do to reverse the impact of decades of intentional policies promoting segregation by race and income. Desmond writes:\n\nHow can we, at last, end our embrace of segregation? The most important thing we can do is to replace exclusionary zoning policies with inclusionary ordinances, tearing down our walls and using the rubble to build bridges. There are two parts to this. The first is to get rid of all the devious legal minutiae we’ve developed to keep low-income families out of high-opportunity neighborhoods, rules that make it illegal to build multifamily apartment complexes or smaller, more affordable homes. We cannot in good faith claim that our communities are antiracist or antipoverty if they continue to uphold exclusionary zoning - our politer, quieter means of promoting segregation. (p. 165-166)\n\nThere is already at least one example of this in the United States. New Jersey’s supreme court prohibited exclusionary zoning and required municipalities to provide their ‘fair share’ or affordable housing. As a result, they’ve seen a huge increase in affordable multi-family units being built across the state (Desmond, p. 169)\nIt’s long past time to tell our elected officials that we want to change our zoning codes and build more affordable housing. We should join in saying:\n\nThis Community’s long-standing tradition of segregation stops with me. I refuse to deny other children opportunities my children enjoy by living here. Build it. (p. 170)"
  },
  {
    "objectID": "posts/louisville_zoning/index.html#how-to-get-involved",
    "href": "posts/louisville_zoning/index.html#how-to-get-involved",
    "title": "End Exclusionary Zoning",
    "section": "How to Get Involved",
    "text": "How to Get Involved\nLouisville is currently having meetings about zoning reform. Unfortunately, these meetings tend to be dominated by current homeowners who tend to oppose changes that would increase affordability. It’s important that people who want to end exclusionary zoning make their voices heard either at the meetings or through phone calls to elected representatives."
  },
  {
    "objectID": "posts/louisville_zoning/index.html#recommended-resources",
    "href": "posts/louisville_zoning/index.html#recommended-resources",
    "title": "End Exclusionary Zoning",
    "section": "Recommended Resources",
    "text": "Recommended Resources\n\nLocal Organizations\n\nCoalition for the Homeless\nMetropolitan Housing Coalition\nLook up Contact Info for your Metro Council Member\nLand Development Code Reform: This is a current effort to reform Louisville’s zoning codes\n\n\n\nLocal Analysis\n\nCode for this analysis\nConfronting Racism in City Planning and Zoning by Louisville Metro Planning and Design Services\nState of Metropolitan Housing Report, 2022 by The Metropolitan Housing Coalition\n\n\n\nBooks about Race, Poverty, and Housing in the United States\n\nThe Sum of Us by Heather McGhee\nPoverty, By America by Matthew Desmond\nThe Color of Law by Richard Rothstein"
  },
  {
    "objectID": "posts/market_norms_social_norms/index.html",
    "href": "posts/market_norms_social_norms/index.html",
    "title": "Market Norms are Crowding out Social Norms",
    "section": "",
    "text": "Imagine you are going over to a friend’s house for a nice dinner. You want to show appreciation for the lovely meal they’ve prepared for you, and you have a few options. You can offer to have them over for dinner in the future. You can bring a bottle of wine or a six pack of beer. You can simply say thank you and tell them what a lovely evening you have had. The one thing you would not want to do is offer to pay them the fair market value of the food and ambiance they have prepared for you. This would be a major social faux pas, the replacing of a social norm with a market interaction.\nThere are some situations that are governed by social norms and other situations that are governed by market norms. To an economist, however, if those norms ever do overlap, then they could only strengthen each other. After all, if someone is willing to do something based only on the strength of a social norm, shouldn’t they be more willing to do it for both the social norm and the additional monetary reward of the market? To be more concrete about it, if lawyers are willing to volunteer their time to help the elderly with legal analysis, it would be completely irrational for them to not be at least as willing to sell their time at a discount.\nAs it turns out, in many situations market norms don’t complement social norms, they crowd them out. The AARP really did ask some lawyers if they would offer services to retirees at a discount. The lawyers said no. Then the AARP asked if they would do it for free and they said yes. Now the economically rational thing to do would be for them to think of themselves as volunteers who get paid a small stipend, but once market norms were part of the consideration, social norms departed.\nIn Predictably Irrational, Dan Ariely writes about an experiment he set up to test market norms and social norms. Participants were asked to complete a mind-numbingly dull computer task, dragging circles into a square for 5 minutes. They split their participants into three groups, one group received 5 dollars, one group 50 cents, and one group was simply asked to complete the task as a favor to the experimenters. Those making 5 dollars outperformed those making only 50 cents by a wide margin, 159 to 101. The volunteers, however, dragged 168 circles. To be clear, that’s not significantly more than the 5 dollar group. If you pay people a reasonable wage to do a task they really will work hard at it. The point, however, is that the social norm was every bit as strong as the fair wage norm, and much, much stronger than the market norm created by offering substandard compensation.\nOne other concrete example of this crowding out of social norms comes from an Israeli daycare. The daycare decided to implement a fine on parents who were late to pick up their children. The fine backfired in epic fashion, and the number of parents who were late actually increased. Before, the parents felt bound by a social norm. Having worked in after-school care I can tell you that parents really do feel bad about inconveniencing the staff by being late. Once money enters the equation, however, being late is simply an extra service that can be paid for and falls under the realm of market norms.\nThe most disturbing part of the day care story is what happened next. The day care, realizing that the fine wasn’t working, decided to eliminate it. The parents behavior, however, did not revert to prior levels of lateness, in fact, it rose a little bit. Now that the social norm was gone it did not return, and having eliminated the market penalty as well it makes sense that tardiness increased. Once market norms have crowded out social norms it may not be easy to get them back.\nIt turns out that being exposed to market cues and signals has a negative effect on our social behavior. Psychologist use a technique called priming to see how certain unconscious cues affect our behavior. Priming works by exposing its subjects to something that will later impact their thinking. In this case, subjects were exposed to something money-related, i.e. a stack of monopoly money on a table or a computer screen saver with dollar bills floating in the water. Money primed people become more self-reliant, but also more selfish and less willing to help others. They were less social and showed an increased desire to be left alone.\nMichael Sandel writes about the moral implications of our transition to a market driven society in What Money Can’t Buy:\n\nwhen we decide that certain goods may be bought and sold, we decide, at least implicitly, that it is appropriate to treat them as commodities, as instruments of profit and use. But not all goods are properly valued in this way.\n\nThe problem is that over time, and without ever really discussing it,\n\n…we drifted from having a market economy to being a market society. The difference is this: A market economy is a tool – for organizing productive activity. A market society is a way of life in which market values seep into every aspect of human endeavor. It’s a place where social relations are made over in the image of the market.\n\nNow, a market economy is a good way to regulate the sales of books, clothing, shoes, technology, and a great many other things. But it’s not a good way to live a full and flourishing life as a human being. My problem is not with markets, but rather with a series of assumptions that associates human well-being with market well-being, as though the two were the same thing. For now let me conclude with some wise words from Robert F. Kennedy, who recognized this problem back in 1968:\n\nOur Gross National Product is now over 800 billion dollars a year. But that Gross National Product counts air pollution and cigarette advertising, and ambulances to clear our highways of carnage. It counts special locks for our doors and the jails for the people who break them. It counts the destruction of the redwood and the loss of our natural wonder in chaotic sprawl. It counts napalm and counts nuclear warheads and armored cars for the police to fight the riots in our cities. It counts…the television programs which glorify violence in order to sell toys to our children. Yet the Gross National Product does not allow for the health of our children, the quality of their education or the joy of their play. It does not include the beauty of our poetry or the strength of our marriages, the intelligence of our public debate or the integrity of our public officials. It measures neither our wit nor our courage, neither our wisdom nor our learning, neither our compassion nor our devotion to our country. It measures everything, in short, except that which makes life worthwhile."
  },
  {
    "objectID": "posts/meritocracy_unjust/index.html",
    "href": "posts/meritocracy_unjust/index.html",
    "title": "Meritocracy is Unjust",
    "section": "",
    "text": "Equality of opportunity is one of the few goals that is still shared by policymakers in both parties (who disagree on how to achieve it, but pay it considerable lip service in speeches). It’s also shared by academics who spend considerable time and effort measuring just how likely it is that a child born to poor parents will end up middle-class or wealthy. Equality of opportunity and the meritocratic society it envisions (one where everyone has an equal opportunity to get what they deserve) assumes that the important thing is that those who deserve it have the opportunity to attain high economic outcomes.\nTo think that a more meritocratic society is a just one that is worth striving for is to misunderstand the nature of what it means to deserve something. Meritocracy tends to confuse a very practical sense of merit with a more abstract and moral one. An individual may deserve a high-paying job or admission to a selective college because they are productive or qualified. However, in a moral sense, individuals do not merit the skills and abilities they are born with, nor do they merit the environments they were born into that allowed them to develop those skills.\nShould society really be structured such that smart kids who are born into poverty are able to escape poverty while less intelligent children are not?\nThis is undoubtedly better than a world in which no one escapes poverty, but is genetic predisposition for problem solving really the criteria by which we should determine who is condemned to the suffering that poverty entails? For that matter, the ability to focus and the development of a strong work ethic are also determined by factors outside of a child’s control. Some of those factors are genetic, and some are environmental but none of them are actually controlled by the child. Thus from the standpoint of the lottery of birth, a meritocratic society is not just, it’s arbitrary.\nNone of this is meant to deny the good work that is done in the name of equality of opportunity. It really is good to have better schools in poor communities. But it’s not because it means some percent of the children who are actually deserving will escape poverty (while the rest will be said to have had an opportunity and squandered it), it’s because all children deserve good schools.\nPeople don’t choose where they’re born, to whom, with what genetic predispositions, or in what sort of environment they grow up. They don’t choose their intelligence, creativity, or work ethic. Unfortunately, equality of opportunity all too often turns into a way to sort people (particularly poor people of color) into deserving and undeserving. It’s time to stop sorting, and focus on making people’s lives better (or at least a bit less harsh). None of us deserve the life we’re born into. Shouldn’t those of us who were lucky enough to be born with the ability to help others use that ability to help them without stopping to inquire whether or not they deserve it?"
  },
  {
    "objectID": "posts/moral_limits_of_markets/index.html",
    "href": "posts/moral_limits_of_markets/index.html",
    "title": "The Moral Limits of Markets",
    "section": "",
    "text": "Michael Sandel’s new book What Money Can’t Buy: The Moral Limits of Markets is a well-timed critique not of capitalist economics, but of the spread of economic thinking well beyond the boundaries of traditional economic issues like trading, inflation, prices, wages, etc. I just started a microeconomics course in preparation for graduate school in the fall and the textbook simply defined economics as “the study of choice.”\nSandel’s thesis is relatively simple: “…we drifted from having a market economy to being a market society. The difference is this: A market economy is a tool - a valuable and effective tool - for organizing productive activity. A market society is a way of life in which market values seep into every aspect of human endeavor. It’s a place where social relations are made over in the image of the market.”\nSandel makes two key arguments for not extending economic thinking to other aspects of life. First, he argues applying prices to certain goods degrades the good itself. Second, he argues that pricing certain goods is unfair. He refers to these two objections as the corruption objection and the fairness objection. The fairness objection is relatively familiar, summed up well by Sandel, who writes “the willingness to pay for a good does not show who values it most highly. This is because market prices reflect the ability as well as the willingness to pay.” To illustrate his point Sandel talks about the growing market of paying people to stand in line for things as varied as congressional hearings and free theatre performances of Shakespeare in Central Park.\nSandel argues that there are both goods that cannot be bought and sold, such as friendships and honorifics (Oscars, Nobel Prize, etc.) and goods that are degraded when they are sold. Some of the more obvious examples include websites that allow you to buy wedding toasts and services that will send people to apologize for you. The rise of giving money as a gift is another example. As Miss Manners writes that money and gift cards have “taken the heart and soul out of the holiday. You’re basically paying somebody - paying them to go away.” Gift giving is economically inefficient, and yet attempts to increase its efficiency through gift cards and direct money transfers destroy both the meaning and the joy behind giving.\nAs Sandel writes, “when we decide that certain goods may be bought and sold, we decide, at least implicitly, that it is appropriate to treat them as commodities, as instruments of profit and use. But not all goods are properly valued in this way.” Sandel continues,“Economists often assume that markets do not touch or taint the goods they regulate. But this is untrue. Markets leave their mark on social norms. Often, market incentives crowd out or erode nonmarket incentives.”\nOne of the best documented cases of crowded out comes from a two-part survey in Switzerland, in which a small community was asked about storing nuclear waste in a nearby mountain. There were initially simply asked to store the waste out of civic duty, and then in round two they were asked and offered an annual payment. As a result, the acceptance rate went down, from 51 percent who would accept the nuclear waste simply out of civic duty, to a mere 25 percent who would accept when offered an annual monetary payment. Economists then increased the monetary offer, but even that did not help. Sandel sums up, “If the community was found to be the safest storage site they were willing to bear the burden. Against the background of this civic commitment, the offer of cash to residents felt like a bribe, an effort to buy their vote. In fact, 83 percent of those who rejected the monetary proposal explained their opposition by saying they could not be bribed.”\nChild care centers that begin charging late fees have also found that the number of late parents actually increases, because instead of viewing it as an inconvenience to the child care providers, it is a paid service. (In a sense, they are even one of the providers best customers).\nSandel is at his very best in a short section entitled ‘Two Tenets of Market Faith’: The first assumption is that “commercializing an activity doesn’t change it,” an assumption already critiqued by his arguments about corruption and the subsequent crowding out of non-market incentives. The second tenet is “that ethical behavior is a commodity that needs to be economized.” He cites several economists, including Lawrence Summers, who replied to an objection that markets rely on selfishness and greed by arguing, “We all have only so much altruism in us. Economists like me think of altruism as a valuable and rare good that needs conserving.”\nSandel’s take-down is blunt and immediate, “Altruism, generosity, solidarity, and civic spirit are not like commodities that are depleted with use. They are more like muscles that develop and grow stronger with exercise. One of the defects of a market-driven society is that it lets these virtues languish. To renew our public life we need to exercise them more strenuously.”\n\nWhy Inequality Matters\nSandel’s key insight is his ability to insist that the market is simultaneously an invaluable tool for the distribution of commodities, and a terrible danger to society when goods that should never be treated as commodities are placed on the market to be bought and sold like the latest computer gadgets or fashions. With that in mind, Sandel offers a better argument for why inequality matters than any I have heard or read before:\n\nIf the only advantage of affluence were the ability to buy yachts, sports cars, and fancy vacations, inequalities of income and wealth would not matter very much. But as money comes to buy more and more - political influence, good medical care, a home in a safe neighborhood rather than a crime-ridden one, access to elite schools rather than failing ones - the distribution of income and wealth looms larger and larger. Where all good things are bought and sold, having money makes all the difference in the world.\n\nMy only quibble is that he left out clean drinking water and the non-destruction of lands that have been in families for generations. The impacts of mountaintop removal coal mining are horrifying, and simply would not be allowed to happen in rich areas of the country. For the price of a few high wage but incredibly dangerous and hard jobs, coal companies have been allowed to do almost anything they want to the residents of Appalachia."
  },
  {
    "objectID": "posts/nonmarket_values/index.html",
    "href": "posts/nonmarket_values/index.html",
    "title": "Non-Market Values and Policy Analysis",
    "section": "",
    "text": "Many of the things that are most valuable in life are not things that can be traded on the market. Love, friendship, the respect of your peers, and a sense of belonging are all incredibly important parts of life, and they’re also impossible to incorporate into economic analysis. Now, since they’re not economic goods, it normally wouldn’t be a problem that they aren’t subject to economic analysis. Unfortunately, economists have not been content to restrict themselves to economic goods, but instead attempt to provide analysis of public policy choices using economic methods.\nWhenever I start criticizing markets people often read the criticism as being of all markets, all the time. That couldn’t be further from my position. I’m confident that markets are the way to go when it comes to commodity exchanges. The price of a new phone or a haircut absolutely should be set by the market (1). But not everything is a commodity. We’re currently debating the role of markets in education and health care. Neither of those are entirely reducible to commodity exchange. For the sake of space, I’ll just do a brief analysis of education and leave health care for another time.\nThe value of education can be thought of in multiple parts:\n\nThe credential (e.g. diploma or certificate).\nThe skill set\nThe network\nThe individual education\n\nCredential and skill sets clearly have a market value. You can decide if it’s worthwhile to learn the skills and get the credentials that will allow you to be a certified electrician based on the current market value of electrical services. The market value of having a network you meet in school is a little harder to evaluate, but it can be done. The part I’ve labeled ‘individual education,’ for lack of a better term, does not have a market value. The study of art, music, literature, and philosophy is not done for its marketable value. Nor is it, as some economists have suggested, done as a consumption good, the way you might buy some clothing or a vacation. Instead, this is education in the pure liberal arts sense of the term it is designed to change you in ways that have nothing to do with market value. The goal is to make you a better citizen, a better neighbor, parent, and more generally, a better person. It’s the part of education that’s worthwhile even if you decide that what you really want to do with your life is stay at home and never work in the paid market again.\nI should note that even those parts of education that can be thought of in market terms often don’t work well in practical terms without society putting in a lot of effort to make those markets work. The education market is plagued by uncertainty, asymmetric information, and inter-generational transfers. Markets do not just exist in some pure form, but are instead created by society, and making an education market work would be difficult even for purely technical skills, and is impossible for the non-market value that is also produced by education.\nOne of the problems with doing analysis solely based on market values is that over time, this can actually change the value structure of society. Market norms can actually crowd out and replace social norms. People primed to think about economics as individual self-interest act more selfishly and show less concern for others. As behavioral economist Daniel Kahnemahn puts it, “living in a culture that surrounds us with reminders of money may shape our behavior and our attitudes in ways that we do not know about and of which we may not be proud.”\nMarket norms often win out for three reasons. First, they are convenient and agreed upon. Second, it serves the interest of people with high market values to base policy analysis on market values. Finally, there is a lack of agreement on how to incorporate non-market values into analysis.\nOne of the most promising approaches to integrating market and non-market values into a single framework is the capabilities approach of Amartya Sen and Martha Nussbaum. Sen argues that theories of justice can be evaluated in terms of the information they deem relevant. The problem with most methods of evaluating well-being is that they artificially restrict the information considered. Sen then lays out his capabilities approach,\n\n…for many evaluative purposes, the appropriate ‘space’ is neither that of utilities nor that of primary goods but that of substantive freedoms – the capabilities – to choose a life one has reason to value…\n\n\nThe concept of ‘functionings,’ which has distinctly Aristotelian roots, reflects the various things a person may value doing or being…\n\n\nA person’s ‘capability’ refers to the alternative combinations of functionings that are feasible for her to achieve. Capability is thus a kind of freedom: the substantive freedom to achieve alternative functioning combinations (or, less formally put, the freedom to achieve various lifestyles.”\n\nBy substantive freedom Sen is distinguishing himself from the formal freedom emphasized by libertarian thinkers. Formal freedom refers only to an absence of legal barriers and not the actual (substantive) ability to do something. For example, the freedom to eat does very little good without the substantive ability to produce or purchase food. The appeal of capabilities has a collection of potential functionings is that it does not restrict us to only part of human life, but includes all of the functionings (market and non-market) that we have reason to value.\nWeighting the importance of various capabilities or even simply developing a list of relevant capabilities is a process for democratic discourse. As Sen argues, “To insist on the mechanical comfort of having just one homogenous “good thing” would be to deny our humanity as reasoning creatures.”\nIt’s important to be up-front about the fact that ignoring the capabilities methodology because it doesn’t have a clear weighting procedure is inconsistent, as most other methods for evaluating human well-being rely on implicit weighting. Surely it is better to be explicit about what we are valuing – even if we are reliant on democratic discourse to work out the details – than to simply issue an implicit judgment about what matters. Sen writes,\n\nSince the preference for market-price-based evaluation is quite strong among economists, it is also important to point out that all variables other than commodity holdings (important matters such as mortality, morbidity, education, liberties, and recognized rights get – implicitly – a zero direct weight in evaluations based exclusively on the real-income approach. They can get some indirect weight if – and only to the extent that – they enlarge real incomes and commodity holdings. The confounding of welfare comparison with real-income comparison exacts a heavy price.)\n\nThere is also arguably a great deal of merit in such an explicitly non-technocratic approach. Sen has given us a way to include non-market values in analysis, and his approach has been very influential in Europe beginning to experiment with new indexes of social well-being (and, of course, the U.N. human development index). There remains more work to be done on evaluating capabilities and applying this as an aid to decision making instead of macro-level evaluation after policies have been put in place. I’d love to hear your ideas on how that can be done!\n\nI need to qualify this, because some might read it as the libertarian take on markets in which markets spring up from nowhere and somehow still function. Markets are implemented by human society (usually but not always through government) and therefore subject to certain rules of exchange like health and safety laws. So a more detailed reading would suggest that the price of a haircut should be set on a market, but the rules of that market are, and should be, a subject of public debate."
  },
  {
    "objectID": "posts/partitioned_regression/index.html",
    "href": "posts/partitioned_regression/index.html",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "",
    "text": "Partitioned regression is a helpful way to gain more intuition for how regression works. What does it mean when we say that regression allows us to adjust for (or control for) other variables? By looking at a regression in multiple pieces we can gain a better understanding of what’s happening and also produce a pretty cool visualization of our key result. (Partitioned regression could also come in handy if you ever have to run a regression on a computer with limited RAM, but that’s not our focus here)."
  },
  {
    "objectID": "posts/partitioned_regression/index.html#getting-started",
    "href": "posts/partitioned_regression/index.html#getting-started",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "Getting Started",
    "text": "Getting Started\nLike most Python projects, we’ll start by loading some libraries.\n\nNumpy is standard for numerical arrays, and Pandas for dataframes and dataframe manipulation.\nAltair is a visualization library. It’s not as well known as matplotlib or Plotly, but I like the aesthetics of the plots it produces and I find it’s grammar of graphics a bit more intuitive.\npalmerpenguins is an easy way to load the demonstration data set I’ll be using here. You could also download it as a .csv file from here\nI’m using scikit-learn (sklearn) for the regression because it’s a useful package to learn for additional work in Python. Statsmodels is another choice that would have worked well for everything in this post.\n\n\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nfrom palmerpenguins import load_penguins\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\n\nLoading the penguins data and showing the first few rows\n\ndf = load_penguins()\n\n# if you download the .csv instead of using the library\n# df = pd.read_csv(\"palmer_penguins.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n\nPenguins illustrated by Allison Horst\n\n\nThe penguins dataset is a small dataset that’s useful for doing demonstrations for everyone who is tired of the iris dataset and thinks penguins are cute."
  },
  {
    "objectID": "posts/partitioned_regression/index.html#preparing-the-data",
    "href": "posts/partitioned_regression/index.html#preparing-the-data",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "Preparing the data",
    "text": "Preparing the data\nBefore we can run any regressions we need to clean up the data a bit. There’s a row that is NA that we can drop and we have some categorical variables that can’t be used directly. We’ll remove the NA data and transform the categorical variables into a series of dichotomous variables that have the same information but can be used in our analysis.\n\ndf = df.dropna().reset_index() # if you don't reset the index then merging on index later will result in data mismatches and destroy your data silently. \nenc = preprocessing.OneHotEncoder(sparse_output = False) # important to have sparse_output = False for the array to be easily put back into a dataframe afterwards\nencoded_results = enc.fit_transform(df[['sex', 'species']])\n\n# names are not automatically preserved in this process, so if you want feature names you need to bring them back out. \ndf2 = pd.DataFrame(encoded_results, columns = enc.get_feature_names_out())\n\n# putting the dichotomous variables in along with everything else\n# this still has the original categorial versions, so check that everything lines up correctly\ndf = pd.concat([df, df2], axis = 1)\n\n#instead of using scikit-learns preprocessing features you could do this manually with np.where\n#df['male'] = np.where(df['sex'] == 'male', 1, 0)"
  },
  {
    "objectID": "posts/partitioned_regression/index.html#partitioned-regression",
    "href": "posts/partitioned_regression/index.html#partitioned-regression",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "Partitioned regression",
    "text": "Partitioned regression\nLet’s say we’re interested in the relationship between bill depth and bill length. Bill length will be our dependent (or target) variable. We think there are things other than bill depth that are related to bill length, so we want to adjust for those when considering the relationship between depth and length. I’m going to put all of those other variables into a matrix called X. (For the dichotomous variables one category has to be left out).\nThen we’re going to run three different regressions. First we’ll regress bill length on all the X variables. Then we’ll also regress bill depth on all of the X variables. Finally, we’ll regress the residuals from the first regression on the residuals from the second regression. The residuals represent what is left unexplained about the dependent variable after accounting for the control variables. And by regressing both bill length and bill depth on the same set of control variables, we get residuals that can be thought of as bill length and bill depth after adjusting for everything in X. That lets us see the relationship between bill length and bill depth after accounting for anything else we think is relevant.\n\ny = df['bill_length_mm'] # target variable\nz = df['bill_depth_mm'] # effect we're interested in\nX = df[['flipper_length_mm', 'body_mass_g', 'sex_female', 'species_Adelie', 'species_Chinstrap']] # other variables we want to adjust for\n\n\nmodel1 = LinearRegression().fit(X, y)\n#residuals aren't actually saved by scikit-learn, but we can create them from the original data and the predictions\nresiduals_y_on_X = (y - model1.predict(X))\n\nmodel2 = LinearRegression().fit(X, z)\nresiduals_z_on_X = (z - model2.predict(X))\n\n#need to reshape for scikit learn to work with a single feature input\nz_resids = residuals_z_on_X.to_numpy().reshape(-1, 1)\ny_resids = residuals_y_on_X.to_numpy().reshape(-1, 1)\n\npart_reg_outcome = LinearRegression().fit(z_resids, y_resids)\n\n#has to be np.round, not round. And has to be [0, 0] not [0] for a 1d array\nprint(\"The regression coefficient using partitioned regression is {}\".format(np.round((part_reg_outcome.coef_[0, 0]), 3)))\n\nThe regression coefficient using partitioned regression is 0.313\n\n\nWe can also verify that we’d get the same result from an ordinary linear regression\n\n#add the bill depth variable back into the X array\nX2 = df[['bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex_female', 'species_Adelie', 'species_Chinstrap']]\n\n#here we can just use [0] for some reasons\nlr_outcome = LinearRegression().fit(X2, y)\nprint(\"The regression coefficient using linear regression is {}\".format(np.round(lr_outcome.coef_[0], 3)))\n\nThe regression coefficient using linear regression is 0.313\n\n\nOne advantage of the partitioned regression is that it allows us to look at the relationship visually. Instead of just having the point estimate, standard error, and any test statistics (e.g. p-value) we can visually inspect a full scatterplot of the data. I’ve added a regression line, and the slope of the line is equal to the regression coefficients found above. You can visually see from this plot that it isn’t a very strong relationship.\n\nplt_df = pd.DataFrame(data = {'Adjusted Bill Length': residuals_y_on_X, 'Adjusted Bill Depth': residuals_z_on_X})\n\nsp_pr = alt.Chart(plt_df, title = \"Bill Depth and Bill Length (Adjusted)\").mark_circle().encode(\n    alt.X('Adjusted Bill Depth', scale = alt.Scale(zero = False)),\n    alt.Y('Adjusted Bill Length', scale = alt.Scale(zero = False)),\n)\n\nplt_pr = sp_pr + sp_pr.transform_regression('Adjusted Bill Depth', 'Adjusted Bill Length').mark_line()\n\nplt_pr\n\n\n\n\n\n\nA disadvantage of using scikit learn is that it doesn’t give us traditional regression statistics. The easiest way to get those is through statsmodels, which shows the expected 0.313 coefficent and tells us the standard error is 0.154 with a p-value of .043. This tells us it is actually a statistically significant relationship, so without the visual evidence from the scatterplot above, we might assume it’s a stronger relationship than it actually is. It is unlikely to have occured purely by chance, but that doesn’t mean it’s necessarily tightly correlated or has a large effect size.\n\nimport statsmodels.api as sm\n\n#unlike scikit learn, statsmodels does not add a constant for you unless you specify that you want one. \nX2 = sm.add_constant(X2)\nest = sm.OLS(y, X2).fit()\nest.summary2()\n\n\n\n\nModel:\nOLS\nAdj. R-squared:\n0.836\n\n\nDependent Variable:\nbill_length_mm\nAIC:\n1482.2547\n\n\nDate:\n2023-04-08 18:25\nBIC:\n1508.9117\n\n\nNo. Observations:\n333\nLog-Likelihood:\n-734.13\n\n\nDf Model:\n6\nF-statistic:\n282.3\n\n\nDf Residuals:\n326\nProb (F-statistic):\n7.50e-126\n\n\nR-squared:\n0.839\nScale:\n4.9162\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n23.4506\n4.8836\n4.8019\n0.0000\n13.8433\n33.0579\n\n\nbill_depth_mm\n0.3130\n0.1541\n2.0316\n0.0430\n0.0099\n0.6160\n\n\nflipper_length_mm\n0.0686\n0.0232\n2.9608\n0.0033\n0.0230\n0.1141\n\n\nbody_mass_g\n0.0011\n0.0004\n2.5617\n0.0109\n0.0003\n0.0019\n\n\nsex_female\n-2.0297\n0.3892\n-5.2153\n0.0000\n-2.7953\n-1.2641\n\n\nspecies_Adelie\n-6.4044\n1.0304\n-6.2154\n0.0000\n-8.4314\n-4.3773\n\n\nspecies_Chinstrap\n3.1612\n0.9927\n3.1844\n0.0016\n1.2082\n5.1141\n\n\n\n\n\n\n\n\nOmnibus:\n32.842\nDurbin-Watson:\n2.068\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n90.331\n\n\nSkew:\n0.430\nProb(JB):\n0.000\n\n\nKurtosis:\n5.402\nCondition No.:\n173513\n\n\n\n\n\nFinally, we might interested in how different this picture is from the unadjusted relationship between bill length and depth if we had not taken into account other variables\n\nsp_lr = alt.Chart(df, title = \"Bill Depth and Bill Length (Unadjusted)\").mark_circle().encode(\n    alt.X('bill_depth_mm', title = 'Bill Depth', scale = alt.Scale(zero = False)),\n    alt.Y('bill_length_mm', title = 'Bill Length', scale = alt.Scale(zero = False)),\n)\n\nplt_lr = sp_lr + sp_lr.transform_regression('bill_depth_mm', 'bill_length_mm').mark_line()\n\nplt_lr\n\n\n\n\n\n\nThis difference and sign reversal is mostly because of the relationships between species, bill length, and bill depth. But that’s a subject for a post about Simpson’s paradox."
  },
  {
    "objectID": "posts/race_in_the_us/index.html",
    "href": "posts/race_in_the_us/index.html",
    "title": "What we know about race and racism in the United States",
    "section": "",
    "text": "Public discussions around race often focus on individual events that have focused national attention, often without enough context on the systematic roots of racism and why these events continue to happen. There is a vast academic literature on the impact of race in the United States. One of the problems with highlighting single case like the Trayvon Martin case is that it cannot tell us much about the overall situation in the United States. Even if we were completely sure of what happened, knowledge of one death in a country where 44 people are killed each day is of limited informational value. (All cases are important and have personal and emotional value, but individually none of them really give us the big picture).\nOvert racism is down to around 20 to 30 percent among whites. Unfortunately, overt racism is not the only problem. The problem is implicit and aversive racism that leads to systems that exercise power in racially biased ways. Unconscious biases and impressions still turn into public policy. As OSU professor Corrine McConnaughy writes:\n\nStopping at overt racism, however, is stopping far too short. Research on aversive racism uses implicit measurement strategies to show that even those white Americans that express racially egalitarian views are not immune from holding—and acting upon—racial prejudice. Negative implicit views are most likely to produce discriminatory or harmful behavior toward blacks when there is no social monitoring of the behavior—that is, no one is “watching”—and the behavior can be justified or rationalized based on a factor other than race.\n\nSoss, Fording, and Schram go into more detail on how race impacts policy-making. They argue that it proceeds in three basic steps.\n\nIn order to design effective policies in a complex world, policy-makers rely on social classifications and group reputations.\nIf racial minorities are a salient group in a policy decision (as for example welfare reform, prison reform, zero tolerance policies, mandatory minimums, etc) then race will be used as a basis for social classification in order to target differences that are perceived as relevant to policy goals.\nThe likelihood of this leading to racially patterned (or biased) policy outcomes is dependent upon three factors: a) prevailing stereotypes, b) policy actors holding those stereotypes, and c) the presence of stereotype consistent cues.\n\nSoss, Fording, and Schram go on to show how this model played out in welfare reform, with the perception of black laziness peaking in 1996 during welfare reform after the abundant presence of social and media cues to reinforce that stereotype. They also show welfare reform playing out at the state and county (in FL) levels, and the clear relationships between race and welfare reform and sanctioning policy.\nThere is also abundant observational as well as experimental evidence of racial bias in the economy today. Simply looking at employment data, black individuals with some college have a higher unemployment rate than whites who dropped out of high school. As an experiment, researchers sent out identical resumes with different names in order to test for bias in hiring. They found that, “Job applicants with white names needed to send about 10 resumes to get one callback; those with African-American names needed to send around 15 resumes to get one callback.”\nIn the absence of policy bias it is hard to explain the massive disparity in academic and economic achievement between blacks and whites, or the disparity in incarceration rates. One in three black men will spend time in prison in their lifetimes, compared to one in 17 white men (Sentencing Project). Even if this only reflected a higher rate of criminal activity one would still need to ask why so many young black men find themselves in situations that lead to criminal activity. I’ll get to the historical background in a moment, but first it’s worth pointing out that only 61 percent of the racial disparity is due to differing amounts of criminal activity, while the rest is due to biases within the criminal justice system (Sentencing Project).\nThe story of the creation of the urban ghetto is well-documented and relatively well known. The division of neighborhoods into white and black stems from a time of explicit racism. That division, and the attending economic and academic disparities, continues to this day. Less well known is the role of the G.I. Bill, which was implemented in a way that jump-started the creation of the post WWII white middle class while failing to do the same for black veterans. Nick Kotz reports the history well in the NY Times:\n\nSouthern Congressional leaders made certain that the programs were directed not by Washington but by local white officials, businessmen, bankers and college administrators who would honor past practices. As a result, thousands of black veterans in the South — and the North as well — were denied housing and business loans, as well as admission to whites-only colleges and universities. They were also excluded from job-training programs for careers in promising new fields like radio and electrical work, commercial photography and mechanics. Instead, most African-Americans were channeled toward traditional, low-paying ”black jobs” and small black colleges, which were pitifully underfinanced and ill equipped to meet the needs of a surging enrollment of returning soldiers.\n\n\nThe statistics on disparate treatment are staggering. By October 1946, 6,500 former soldiers had been placed in nonfarm jobs by the employment service in Mississippi; 86 percent of the skilled and semiskilled jobs were filled by whites, 92 percent of the unskilled ones by blacks. In New York and northern New Jersey, ”fewer than 100 of the 67,000 mortgages insured by the G.I. Bill supported home purchases by nonwhites.” Discrimination continued as well in elite Northern colleges. The University of Pennsylvania, along with Columbia the least discriminatory of the Ivy League colleges, enrolled only 46 black students in its student body of 9,000 in 1946. The traditional black colleges did not have places for an estimated 70,000 black veterans in 1947. At the same time, white universities were doubling their enrollments and prospering with the infusion of public and private funds, and of students with their G.I. benefits.\n\nMy grandfather came home from WWII, went to college on the GI Bill, got a job at IBM and sent my father and uncle to good private schools and then on to college. This educational privilege was passed on to me, and for all I know is the difference between me and a smart young Black man who went to bad public high schools and was never encouraged to go to college since no one in his family had. (Or perhaps went to college and dropped out due to lack of support).\nThe explicit racism in our society that persisted over hundreds of years still shapes us today. To ignore that is to ignore history. The history of explicit racism is still very recent in our society, with most of us having parents and all of us having grandparents that encountered it directly and were shaped by it. Hundreds of years of slavery, segregation, and discrimination are not going to be overcome less than 50 years after the civil rights movement."
  },
  {
    "objectID": "posts/racing_lou_season1/index.html",
    "href": "posts/racing_lou_season1/index.html",
    "title": "Racing Louisville in their first NWSL season",
    "section": "",
    "text": "The NWSL Challenge Cup kicked off earlier this week, so I took quick look at some of the stats from last season. I got the data from FBREF and made some quick graphs - I’m including the code here, but feel free to ignore it if you’re only interested in the football statistics.\nI wanted to know if the NWSL had teams that were focused on offense or defense, so I looked first at average goals scored and allowed per game. On average, teams score 1.15 goals per game, so I added those as reference lines.\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/natekratzer/nwsl/main/data/team/2021_season_ovr.csv\")\ndf['goals_scored_per_match'] = (df['GF']/df['MP']).round(2)\ndf['goals_allowed_per_match'] = (df['GA']/df['MP']).round(2)\ndf['mean']= df['GF'].mean()/24 #24 games in the season\n\nfig = px.scatter(df, \n                 x=\"goals_scored_per_match\", \n                 y = \"goals_allowed_per_match\",\n                 labels = dict(goals_scored_per_match = \"Scored\", goals_allowed_per_match = \"Allowed\"),\n                 template = 'simple_white',\n                 title = \"NWSL Goals Per Match, 2021\",\n                 text = 'Abbr',\n                 width = 500,\n                 height = 500\n                 )\n\nfig = fig.update_traces(textposition = 'top center')\nfig = fig.update_xaxes(range = [0.6, 1.8], nticks = 7)\nfig = fig.update_yaxes(range = [0.6, 1.8], nticks = 7,\n                 scaleanchor = \"x\", # make the y axis tied to X\n                 scaleratio = 1)\n\nfig = fig.add_hline(y = 1.15, opacity = 1, line_width = 2, line_dash = 'dash', line_color = 'grey')\nfig = fig.add_vline(x = 1.15, opacity = 1, line_width = 2, line_dash = 'dash', line_color = 'grey')\n\nfig = fig.add_annotation(x = 1.6, y = 0.63, text = \"Data from fbref.com\", showarrow = False)\n\nfig.show()\nFor the most part teams weren’t really good at one end and not the other. The closest any team came to that is Houston, which is above average on offense and below average on defense. For the most part though, offensive and defensive skill go together.\nWe’d expect average goals to matter a lot, but soccer is a pretty high variance sport, so I also wanted to know how well goal differential predicted results. Here results are the points that determine standings (3 pts for a win, 1 for a draw, 0 for a loss).\nfig = px.scatter(df, \n                 x= \"GD\", \n                 y = \"Pts\",\n                 trendline='ols',\n                 labels = dict(GD = \"Goal Differential\", Pts = \"Points\"),\n                 template = 'simple_white',\n                 title = \"NWSL Goals and Results, 2021\",\n                 text = 'Abbr'\n                 )\n\nfig = fig.update_traces(textposition = 'top center')      \n\nfig.show()\nAs we’d expect they track pretty neatly. Washington and Chicago had slightly better seasons than you’d expect from goal differential alone, but nothing wild."
  },
  {
    "objectID": "posts/racing_lou_season1/index.html#racing-louisville-and-homefield-advantage",
    "href": "posts/racing_lou_season1/index.html#racing-louisville-and-homefield-advantage",
    "title": "Racing Louisville in their first NWSL season",
    "section": "Racing Louisville and Homefield Advantage",
    "text": "Racing Louisville and Homefield Advantage\nRacing Louisville is my team, so I also pulled some of their game specific data and here again started looking at goals. In this case I was curious about how much of a homefield advantage they have.\n\ndf2 = pd.read_csv(\"https://raw.githubusercontent.com/natekratzer/nwsl/main/data/team/lou_games.csv\")\n\ndf2 = df2[df2['Comp'] == 'NWSL'] #exclude challenge cup which is in this dataset\n\n# Reformat to long\ngoals_df = df2[['Venue', 'GF', 'GA']].melt(id_vars = ['Venue'], value_vars = ['GF', 'GA'])\n\n# recode GF and GA to Scored and Allowed\nold_list = ['GF', 'GA']\nnew_list = ['Scored', 'Allowed']\ngoals_df['variable'] = goals_df['variable'].replace(old_list, new_list)\n\n# Group by and summarize into new dataframe\ngrouped_df = (goals_df.groupby(['Venue', 'variable'])['value']\n                      .mean()\n                      .to_frame(name='Goals')\n                      .reset_index())\n                      \n# Visualize\nfig = px.bar(grouped_df,\n             x = 'variable',\n             y = 'Goals',\n             color = 'variable',\n             facet_col = 'Venue',\n             labels = dict(variable = 'Allowed/Scored', Goals = 'Goals Per Match'),\n             template = 'simple_white',\n             title = \"Racing Louisville Struggles with Defense on the Road\")\n\nfig.show()\n\n                                                \n\n\nHere we do see a clear offense/defense distinction, which is that Louisville’s defense collapses during road games. The offense is slightly worse (0.75 goals per match compared to 1.0 at home), but the defense gives up over 2 goals a game on average during away matches.\nNot surprisingly, Louisville also wound up with a much worse away record (1-3-8) than home record (4-4-4)\n\nrecord_df = (df2.groupby(['Venue', 'Result'])['Date']\n                .count()\n                .to_frame(name = 'Matches')\n                .reset_index())\n\nfig = px.bar(record_df,\n             x = 'Result',\n             y = 'Matches',\n             color = 'Result',\n             facet_col = 'Venue',\n             #labels = dict(variable = 'Allowed/Scored', Goals = 'Goals Per Match'),\n             template = 'simple_white',\n             title = \"Racing Louisville is Much Better at Home\")\n\nfig.show()"
  },
  {
    "objectID": "posts/rational_fools/index.html",
    "href": "posts/rational_fools/index.html",
    "title": "Rational Fools: Amartya Sen’s Critique of Economic Theory",
    "section": "",
    "text": "The formalizing of self-interest as an economic principle was largely the work of Francis Edgeworth. It is sometimes wrongly traced back to the work of Adam Smith. While Smith wrote about self-interest, he actually had a much, much more nuanced view of both when people would behave out of self-interest and when self-interested behavior could be good for society then he is usually given credit for. (He most certainly did not claim either that individuals are always self-interested or that self-interest always leads to optimal outcomes, and cited limits to self-interest in both Theory of Moral Sentiments and Wealth of Nations).\nAmartya Sen, one of the few economists to also be a good philosopher, starts his critique by reviewing the work of Edgeworth. As it turns out, even though Edgeworth gave us the math for self-interest, he didn’t think it was a very good assumption except in the cases of (1) war and (2) contract. While this is a substantial restriction, it still covers a lot of ground, so it’s worth looking at how Edgeworth came to this conclusion.\nEdgeworth gets there by rejecting the utilitarian premise that individuals act to maximize the greatest good for the greatest number. Having rejected that he concludes that individuals act not to maximize overall utility, but instead to maximize only their own utility. Sen notes that the rejection of utility is not sufficient grounds to adopt self-interest,\n\nbetween the claims of oneself and the claims of all lie the claims of a variety of groups-for example, families, friends, local communities, peer groups, and economic and social classes. The concepts of family responsibility, business ethics, class consciousness, and so on, relate to these intermediate areas of concern, and the dismissal of utilitarianism as a descriptive theory of behavior does not leave us with egoism as the only alternative.\n\nEdgeworth then shows that under assumptions of self-interest and in the absence of externalities, self-interest can lead to an equilibrium that maximizes the satisfaction of individual preferences. This is a position in the ‘core’ area of the economy. Sen’s reply is simple, ‘who cares?’\n\nBeing in the core, however, is not as such a momentous achievement from the point of view of social welfare. A person who starts off ill-endowed may stay poor and deprived even after the transactions, and if being in the core is all that competition offers, the property-less person may be forgiven for not regarding this achievement as a “big deal.”\n\nMoving on from Edgeworth, the argument for self-interest and rationality has sometimes been given in the form of revealed preference. The basic idea is to determine one’s preferences, which can’t be directly observed, by asserting that they are the same as one’s behavior, which can be observed. As Sen notes, “Behavior, it appears, is to be “explained in terms of preferences, which are in turn defined only by behavior.” This comes close to simply being a definition that is devoid of any new content, but the idea of revealed preference still rests on the claim that preferences and behavior are consistent over time. This makes it falsifiable. And, in fact, it has been falsified.\nSen notes that revealed preference fails not only by assuming too much in terms of preferences being identical to behavior, but also by assuming too little in terms of other possible sources of information about preferences. As Sen puts it,\n\n…choice may reflect a compromise among a variety of considerations of which personal welfare may be just one. The complex psychological issues underlying choice have recently been forcefully brought out by a number of penetrating studies dealing with consumer decisions and production activities.It is very much an open question as to whether these behavioral characteristics can be at all captured within the formal limits of consistent choice on which the welfare-maximization approach depends.\n\nIn my view the question is no longer open. Scholars like Kahnemahn, Ariely, and Thaler have shown that human behavior cannot be captured by the welfare-maximization framework, but Sen was writing in 1977, so it was more of an open question then.\nAfter this critique, Sen then begins to build up his alternative vision. Sen distinguishes between cases of sympathy and cases of commitment. Sympathy involves an individual being linked to others, feeling sad when they are sad and happy when they are happy. In this sense, sympathy does not upset the basic model of preferences too much, it simply adds a new component. I prefer it when the people I care about are happy, and so I might give up some amount of my self-interested happiness in exchange for the happiness I feel when they are happy. (Since Sen’s writing this has been added to some models, particularly of charitable giving, usually under the name ‘warm glow’).\nCommitment involves placing an ethical stance ahead of one’s own well-being. In Sen’s words, “One way of defining commitment is in terms of a person choosing an act that he believes will yield a lower level of personal welfare to him than an alternative that is also available to him.”\nThis may seem insignificant, but it’s important to remember that there are actually a lot of activities that we engage in frequently that don’t yield obvious benefits. Recycling, for instance, is a case of ignoring the economic logic that my individual recycling effort cannot make a difference. In theory, each person should free-ride on the efforts of others to preserve the environment, and yet I recycle, eat less meat, bike to work, etc., not because I believe the marginal benefits to these things outweighs the marginal cost, but rather because it is the right thing to do. Voting, obeying social norms, giving to charity, doing favors for strangers, tipping the server at a restaurant you’ll never return to, and many other cases also follow this same pattern.\nNow, it is true that it’s difficult to separate out sympathy from commitment. One could argue that I enjoy doing the right thing, and therefore I’m still maximizing my own well-being. First, this reduces preferences to observed behavior, as mistake already critiqued above. The motive to do the right thing is different than the motive to enjoy oneself, and lumping them together is a categorical mistake. Second, we can think about the use of pre-commitment devices to combat weakness of will. Because preferences vary over time one can commit oneself to a certain course of action, in part because one knows ahead of time that ‘the right thing’ and ‘the preferred thing at the time of the decision might vary.’ As a simple example, by not having a parking pass for work, I encourage myself to bike, even when it’s raining or otherwise unpleasant outside. The use of pre-commitment devices seems to be a pretty clear case of placing a form of commitment ahead of individual well-being. Third, as Sen points out we can actually desire to have preferences different than the ones we currently have (i.e. I wish I liked vegetarian foods more, or I wish I cared more about others). These sorts of meta-preferences mean we can attempt to commit to changing our current preferences and thus bring our individual desires closer to our ethical ideal.\nAs Sen argues, economic man is significantly worse off because of a failure to recognize these distinctions:\n\nA person thus described may be “rational” in the limited sense of revealing no inconsistencies in his choice behavior, but if he has no use for these distinctions between quite different concepts, he must be a bit of a fool. The purely economic man is indeed close to being a social moron.\n\nAll of these considerations have a lot of impact on how we think about setting up a society to that is both just and encourages human flourishing. Sen cautions not to make the same mistake Edgeworth made when rejecting utility and concluding that rejection implied self-interest (egoism). Sen writes, “The rejection of egoism as description of motivation does not, therefore, imply the acceptance of some universalized morality as the basis of actual behavior.” Instead, it means human behavior is, as it always has been, a complex phenomena driven by multiple motivations, including socialization.\nSen wrote in 1977, and while there’s been a lot of interesting behavioral economics that takes Sen’s critique seriously and explores actual human behavior, there’s still a tragic gap between what Sen and empirical observation teaches us, and what is taught in economic classrooms across the United States."
  },
  {
    "objectID": "posts/simpsons_paradox/index.html",
    "href": "posts/simpsons_paradox/index.html",
    "title": "Simpson’s Paradox",
    "section": "",
    "text": "Simpson’s paradox is not actually a paradox, but it is an interesting result in statistical analysis with an important lesson for data scientists. The level of aggregation of your data and analysis can entirely change the results. Simpson’s paradox is the idea that a relationship that holds at one level of aggregation may not exist or may even go in the opposite direction at other levels of aggregation.\nTo demonstrate with a concrete example, let’s look at some Palmer Penguins data and the relationship between bill length and bill depth."
  },
  {
    "objectID": "posts/simpsons_paradox/index.html#import-libraries-and-load-data",
    "href": "posts/simpsons_paradox/index.html#import-libraries-and-load-data",
    "title": "Simpson’s Paradox",
    "section": "Import libraries and load data",
    "text": "Import libraries and load data\n\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nfrom palmerpenguins import load_penguins\n\ndf = load_penguins()\ndf.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007"
  },
  {
    "objectID": "posts/simpsons_paradox/index.html#plot-the-data",
    "href": "posts/simpsons_paradox/index.html#plot-the-data",
    "title": "Simpson’s Paradox",
    "section": "Plot the data",
    "text": "Plot the data\n\n#ungrouped graph\nsp_ungrouped = alt.Chart(df, title = \"Bill Depth and Bill Length in Penguins\").mark_circle().encode(\n    alt.X('bill_depth_mm', title = \"Bill Depth\", scale = alt.Scale(zero = False)),\n    alt.Y('bill_length_mm', title = \"Bill Length\", scale = alt.Scale(zero = False))\n)\n\n#x value first, then y \nplt_ungrouped = sp_ungrouped + sp_ungrouped.transform_regression('bill_depth_mm', 'bill_length_mm').mark_line()\n\n#grouped graph\nsp_grouped = alt.Chart(df, title = \"Bill Depth and Bill Length by Species of Penguin\").mark_circle().encode(\n    alt.X('bill_depth_mm', title = \"Bill Depth\", scale = alt.Scale(zero = False)),\n    alt.Y('bill_length_mm', title = \"Bill Length\", scale = alt.Scale(zero = False)),\n    color = 'species'\n)\n\n#x value first, then y \nplt_grouped = sp_grouped + sp_grouped.transform_regression('bill_depth_mm', 'bill_length_mm', groupby = ['species']).mark_line()"
  },
  {
    "objectID": "posts/simpsons_paradox/index.html#misleading-analysis-from-high-level-data",
    "href": "posts/simpsons_paradox/index.html#misleading-analysis-from-high-level-data",
    "title": "Simpson’s Paradox",
    "section": "Misleading analysis from high level data",
    "text": "Misleading analysis from high level data\n\nplt_ungrouped\n\n\n\n\n\n\nWhen we look at bill length and bill depth in an initial scatterplot it seems as though penguins with deeper bills tend to also have shorter bills. While this is technically true in our selection of penguins, it’s also very misleading because we haven’t done anything to account for the different species of penguins that make up our sample."
  },
  {
    "objectID": "posts/simpsons_paradox/index.html#the-effect-reverses-when-accounting-for-penguin-species",
    "href": "posts/simpsons_paradox/index.html#the-effect-reverses-when-accounting-for-penguin-species",
    "title": "Simpson’s Paradox",
    "section": "The effect reverses when accounting for penguin species",
    "text": "The effect reverses when accounting for penguin species\n\nplt_grouped\n\n\n\n\n\n\nThe underlying scatterplots here are the identical, but we see very different relationships between bill depth and length. Within each penguin species, penguins that have bigger bills tend to have bigger bills in terms of both length and depth. But Adelie penguins tend to have deep and short bills compared to Gentoo penguins, which are longer and narrower. Even though we have individual penguin data, we’d still be mislead if we didn’t account for the groups. This is also a place where traditional statistical techniques can help us if we’re building a model that includes species in the data. A major problem here is that you don’t always know which variables might be missing for your dataset, and so it’s important to approach with a research-informed mental model of what your analysis should look like to help avoid drawing poor conclusions from incomplete information."
  },
  {
    "objectID": "posts/theory_of_moral_sentiments/index.html",
    "href": "posts/theory_of_moral_sentiments/index.html",
    "title": "A Theory of Moral Sentiments",
    "section": "",
    "text": "Adam Smith is largely responsible for starting the field of economics as an academic subject. His magnum opus The Wealth of Nations is considered the first work of modern economics, and Smith is sometimes referred to as “the father of modern economics.” I tell you this because I think Smith would be at least mildly ashamed of his academic progeny. Smith is perhaps best known for his famous statement on mutually beneficial trade:\n\nIt is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner, but from their own interest. We address ourselves, not to their humanity, but to their self-love, and never talk to them of our necessities, but of their advantages.\n\nSmith continues on to write (entirely correctly) about why it makes sense for people to specialize in tasks like butchery or baking and then trade with each other. However, the economics profession as a whole (with notable exceptions) has taken the idea of self-interest much farther than Smith intended. In addition to The Wealth of Nations, Smith’s other major work was A Theory of Moral Sentiments, which Smith opens with these words:\n\nHow selfish soever man may be supposed, there are evidently some principles in his nature, which interest him in the fortune of others, and render their happiness necessary to him, though he derives nothing from it except the pleasure of seeing it.\n\nIn fact, Smith goes on to argue that the entire basis of morality is sympathy, or ability to project ourselves into the others place and to feel with them either the pain or joy that they are experiencing:\n\nAnd hence it is, that to feel much for others and little for ourselves, that to restrain our selfish, and to indulge our benevolent affections, constitutes the perfection of human nature; and can alone produce among mankind that harmony of sentiments and passions in which consists their whole grace and propriety.\n\nSmith the moral philosopher seems radically different from Smith the economist, but Smith’s positions are not contradictory. Some have suggested that The Wealth of Nations, being written after A Theory of Moral Sentiments, represents a change in the direction of Smith’s thought, but in reality the same concerns the Smith expresses in his moral philosophy can be found in his economics. Smith’s analysis of human behavior is much, much more complex and layered than modern classical economics gives him credit for.\nSmith, for instance, argues that stockholders often want policies that are not in the interest of the general public. Amartya Sen points out that while Smith believed that self-interest was sufficient to motivate market exchanges, there are problems involved in setting up markets equitably and managing externalities and public goods that self-interest is unable to deal with. To quote Sen:\n\nBut in dealing with other problems, those of distribution and equity and rule-following for generating productive efficiency – Smith emphasized broader motivations. In these broad contexts, while prudence remained “of all virtues that which is most helpful to the individual,” he explained why “humanity, generosity, and public spirit, are the qualities most useful to others.” The variety of motivations that we have reason to accommodate is, in fact, quite central to Smith’s remarkably rich analysis of human behavior.\n\nEconomists today often draw a distinction between self-interest and selfishness. The self can be interested in ends that are not selfish. To some extent, this is a semantics games, but it is one that Smith explicitly rejects.\n\nThose who are found of deducing all our sentiments from certain refinements of self-love, think themselves at no loss to account, according to their own principles, both for this pleasure and this pain. Man, say they, conscious of his own weakness, and of the need which he has for others, rejoices whenever he observes that they adopt his own passions, because he is then assured of assistance; and grieves whenever he observes the contrary, because he is then assured of opposition. But both the pleasure and the pain are felt so instantaneously, and often upon such frivolous occasions, that it seems evident that neither of them can be derived from any such self-interested consideration.\n\nFortunately for us, behavioral economics is confirming experimentally most of what Smith already knew. Today, we know that exposure to money and to economics makes people more likely to act selfishly. Smith, however, is already a step ahead of us (he’s pretty spry for a dead guy). He knew that not only was this bad for society, it was bad for the individuals as well.\n\nSociety and conversation, therefore, are the most powerful remedies for restoring the mind to its tranquility, if, at any time, it has unfortunately lost it; as well as the best preservatives of that equal and happy temper, which is so necessary to self-satisfaction and enjoyment.\n\nSmith was, no doubt, wrong about many things, but economics could be much further ahead if it had simply paid attention to its founding father. Today, you can get a PhD in economics without ever reading Smith. This is a tragedy for those of us who want to have an economics discipline that recognizes the rich variety in human behavior and is willing to grapple with the tough philosophical issues and value judgments that underlie the calculations and empirical methodologies."
  },
  {
    "objectID": "posts/what_is_education_for/index.html",
    "href": "posts/what_is_education_for/index.html",
    "title": "What is Education for?",
    "section": "",
    "text": "I did not major in philosophy to get a good job. I also did not major in philosophy in order to do well on standardized tests and get into graduate school, although philosophy majors, in fact, do extremely well on the GRE and LSAT.\nI have lost count of the number of articles I have read analyzing the education system in terms of its profitability. Each week a new publication decides to ask if a college education is worth the money…and by worth the money they always seem to mean an analysis of lifetime earnings potential. Education is seen primarily or sometimes even only as job training.\nMeaningful employment is an important part of human life, but it is not the only part of human life. The Canadian ecologist Stan Rowe suggest that the proper goal of education is “understanding what it means to be human in a living world.” The importance of this should be obvious, but Rowe goes on to note wryly, “After all, well-educated people, not illiterates, are wrecking the planet. Schools and universities are morally bankrupt [and] most research is worthless busywork…”\nRowe is harsher than I wish to be on schools and universities, but there are certainly elements of truth to his critique. In my field as a graduate student in public policy I have read all too many papers that are not well thought-out, but rather rely on sloppy assumptions and fancy mathematical modeling.\nFrustrating as poor research is, it is not the most severe problem facing academia. The crisis is the push towards only the practical and immediately applicable, while relegating everything else to the status of less-important subjects. STEM (Science, Technology, Engineering, Math) are important, but so too are History, English, Philosophy, Sociology and all of the humanities. STEM often (at the undergraduate level) teaches a certain type of thinking, which is a very effective and practical way to solve problems. STEM fields seek answers, while the humanities focus first on training students to ask the correct questions, and to take an extremely broad view of any problem. A lot of damage has been done by narrow, practical solutions. The technology we have is an engineering marvel, and the economic abundance we possess is a tribute to the efficiency of solving practical problems. And yet for all our abundance we still have massive poverty and environmental degradation, as well as a society that is becoming increasingly polarized, distrustful, and distant. Perhaps we have been asking ourselves the wrong questions?\nAs Martin Luther King Jr. wrote:\n\nThe function of education, therefore, is to teach one to think intensively and to think critically. But education which stops with efficiency may prove the greatest menace to society. The most dangerous criminal may be the man gifted with reason, but with no morals.\n\nThere are, of course, great difficulties in the idea that schools ought to be responsible for teaching any form of morality. To be clear at the outset, my argument is that schools ought to teach individuals to ask moral questions, not that schools should provide answers. My first philosophy class quite literally changed my life on a daily basis, because it made me ask a new question: “What should I eat?” Like most 18 year olds I had always been content to eat whatever was tasty and reasonably priced, without much thought about where the food came from.\nThere are two other articles I still remember from that class and refer to frequently. One was by Peter Singer on morality and global poverty, and the other was on white privilege by Peggy McIntosh. (Both are short, and available for free online, so I strongly encourage you to read them).\nWhat, you may be wondering, is the value of a question without an answer? There is not ‘an answer’ to the question of ‘What should I eat?’ and yet pondering that question as nonetheless affected my unavoidable answer to the daily question ‘What do I eat?’ In a similar manner there is not an answer to the question, “What is ethical/moral?” but by pondering it we may nonetheless improve our behavior.\nThe ultimate purpose of education is to bring up human beings, not merely workers. I majored in philosophy because I thought it would make me a better person, one who might have a chance at understanding what it means to be human in a living world. There are certainly other ways to do this (my personal enjoyment of philosophy is also a big part of why I chose to major in it), and my point is not about individual majors, but rather a general mindset towards education. Until we understand education as part of bringing up human beings and not merely an economic investment in future salary returns our attempts to reform education are likely to go astray, and may even harm society."
  },
  {
    "objectID": "posts/what_I_believe/index.html",
    "href": "posts/what_I_believe/index.html",
    "title": "What I Believe",
    "section": "",
    "text": "Update on 2023-03-18: Statements below do not necessarily reflect my current beliefs, but I enjoyed getting to re-read what I wrote a decade ago when transferring content over from my old blog, so I decided to transfer this post over."
  },
  {
    "objectID": "posts/what_I_believe/index.html#between-relativism-and-universalism",
    "href": "posts/what_I_believe/index.html#between-relativism-and-universalism",
    "title": "What I Believe",
    "section": "Between Relativism and Universalism",
    "text": "Between Relativism and Universalism\nFirst, it is important to start by saying that getting one’s beliefs correct is not, repeat not, the key to salvation (and we’ll discuss what I mean by salvation in a bit). One of the worst things that has happened to the church is that works-righteousness, the idea of earning ones way into heaven, was replaced not with grace, but with beliefs-righteousness, the idea that if one only believes the correct things fervently enough and without doubt one can earn their way into heaven. Like works-righteousness, beliefs-righteousness leaves one with a constant feeling of guilt. Unlike works-righteousness, which at least led to good works being done, beliefs-righteousness leads to petty theological squabbles and lots of unnecessary hatred. Grace, by definition, is unearned. Apparently, however, this is too difficult of a concept for most people to grasp and so they decided they must be required to do something… if not works, then beliefs.\nThere’s one other thing to clear up at the outset. No one has all of it figured out. But before you panic and starting screaming “MORAL RELATIVISM!!” at the top of your lungs, the fact that no one belief is correct does not mean that anything goes. We’ll get into right and wrong more later in the post, but for now think of it like building a house. There is no one correct way to build a house; it depends on climate, personal preference, society, how many people are going to live there, etc. There are, however, plenty of wrong ways to build a house. If the house collapses, catches on fire, explodes, is cramped and uncomfortable, or is just generally the result of shoddy craftsmanship then it is a bad house. If your religious beliefs cause you to be full of hatred, hurt others, and/or are generally incoherent, self-contradictory, and not well thought out, then you have a bad set of religious beliefs. This, by the way, is not only true of religious beliefs. In most of life there is more than one correct way to do things, and there are also several incorrect ways. My claim is not that I have The Truth, but rather that I have a relatively well thought out and helpful belief system. It is (hopefully) a truth, just not The Truth."
  },
  {
    "objectID": "posts/what_I_believe/index.html#why-believe-anything-at-all",
    "href": "posts/what_I_believe/index.html#why-believe-anything-at-all",
    "title": "What I Believe",
    "section": "Why Believe Anything At All?",
    "text": "Why Believe Anything At All?\nI begin with a short joke from David Foster Wallace:\n\nThere are these two young fish swimming along and they happen to meet an older fish swimming the other way, who nods at them and says “Morning, boys. How’s the water?” And the two young fish swim on for a bit, and then eventually one of them looks over at the other and goes, “What the hell is water?”\n\nIt is so inescapably true as to almost be banal that everything is about perspective, and our perspective limits us at least as much as it enlightens us. Perhaps the most important thing to understand is that you have a perspective. And there’s not much you can do about it. No one has put it better than Wallace in his commencement speech to Kenyon:\n\nHere is just one example of the total wrongness of something I tend to be automatically sure of: everything in my own immediate experience supports my deep belief that I am the absolute centre of the universe; the realest, most vivid and important person in existence. We rarely think about this sort of natural, basic self-centredness because it’s so socially repulsive. But it’s pretty much the same for all of us. It is our default setting, hard-wired into our boards at birth. Think about it: there is no experience you have had that you are not the absolute centre of. The world as you experience it is there in front of YOU or behind YOU, to the left or right of YOU, on YOUR TV or YOUR monitor. And so on. Other people’s thoughts and feelings have to be communicated to you somehow, but your own are so immediate, urgent, real.\n\nOur default perspective is formed by our immediate experience. Now it’s possible to expand that a little bit. By listening to other people and actively cultivating empathy you might be able to get a taste, but only a taste, of what life is like for other people. The cold, hard truth, is that you will have to make decisions based on what you think is best. Whether you choose to believe in a God, the gods, virtue, human progress, sacred scripture, or nothing at all is up to you. There is no certainty available. The one thing that is true is that you will have to choose, because trying not to choose is, in itself, a choice. It’s a choice to stick with the default setting, with our human wiring. Wallace writes,\n\nThere is no such thing as not worshipping. Everybody worships. The only choice we get is what to worship. And the compelling reason for maybe choosing some sort of god or spiritual-type thing to worship–be it JC or Allah, be it YHWH or the Wiccan Mother Goddess, or the Four Noble Truths, or some inviolable set of ethical principles–is that pretty much anything else you worship will eat you alive. If you worship money and things, if they are where you tap real meaning in life, then you will never have enough, never feel you have enough. It’s the truth. Worship your body and beauty and sexual allure and you will always feel ugly.\n\n\nWorship power, you will end up feeling weak and afraid, and you will need ever more power over others to numb you to your own fear. Worship your intellect, being seen as smart, you will end up feeling stupid, a fraud, always on the verge of being found out. But the insidious thing about these forms of worship is not that they’re evil or sinful, it’s that they’re unconscious. They are default settings.\n\nOur default settings rely on our own experiences of reality. Our own power, beauty, intellect, money, fame, things, place in society, and so on. There’s a good reason to move on from that default though and to consciously choose something to believe. Belief in this sense is not blind, but rather a conscious decision, a knowing leap of faith.\nI’m going to move on to what I believe instead of just why I believe, but I encourage you to read Wallace’s whole speech (or listen to it)."
  },
  {
    "objectID": "posts/what_I_believe/index.html#being-mystery-and-meaning",
    "href": "posts/what_I_believe/index.html#being-mystery-and-meaning",
    "title": "What I Believe",
    "section": "Being, Mystery, and Meaning",
    "text": "Being, Mystery, and Meaning\nGod does not exist. At least, not in the way that we normally think about existence. God does not exist in the same way that a cat or a building can be said to exist.\nGod is instead the source of existence and of meaning. The mysterious and unknown answer to the unanswerable questions: Why is there being at all instead of nothing? Why am I here?\nThese core philosophical/theological questions are ones that cannot, ultimately, be answered. I have found Gabriel Marcel helpful here in making an (admittedly somewhat arbitrary) distinction between problem and mystery:\n\nA problem is something which I meet, which I find completely before me, but which I can therefore lay siege to and reduce. But a mystery is something in which I am myself involved, and it can therefore only be thought of as a sphere where the distinction between what is in me and what is before me loses its meaning and initial validity.\n\nA problem, conceived of this way, is like a jigsaw puzzle. All of the pieces are in front of you, and no matter who works on the puzzle the pieces will eventually fit together in the same way. A mystery, however, involves you. You cannot solve it because you are a part of it, and therefore you cannot attain an objective perspective. You can investigate a mystery, but it will change as you change.\nWhen Albert Einstein spoke of religion, he spoke of mystery:\n\nThe most beautiful experience we can have is the mysterious. It is the fundamental emotion which stands at the cradle of all true art and science. Whoever does not know it and can no longer wonder, no longer marvel, is as good as dead, and his eyes are dimmed. It was the experience of mystery – even if mixed with fear – that engendered religion. A knowledge of the existence of something we cannot penetrate, our perceptions of the profoundest reason and the most radiant beauty, which only in their most primitive forms are accessible to our minds – it is this knowledge and this emotion that constitute true religiosity; in this sense, and, and in this alone, I am a deeply religious man.\n\nFormal religion is born, at least in part, from this religious experience. All too often, religion, like many human institutions, has failed miserably. Ironically, religion has sometimes even denied this religious experience of awe in favor of fear. Nonetheless, religion (broadly construed) has something essential to offer, a way of living that infuses our lives with meaning and allows us to see the sacred in the everyday."
  },
  {
    "objectID": "posts/what_I_believe/index.html#choosing-a-religion",
    "href": "posts/what_I_believe/index.html#choosing-a-religion",
    "title": "What I Believe",
    "section": "Choosing a Religion",
    "text": "Choosing a Religion\nThere are two main reasons that I have chosen to remain Christian, and more specifically Lutheran (ELCA). To understand them you need to think of religion not as a set of beliefs but more in terms of a language and a story.\nThinking of religion as a language, I stay Lutheran because it is the religion in which I am fluent. Within the faith tradition I grew up in there are powerful stories of exile, exodus, and resurrection. There are stories and parables of the ways in which a power beyond my understanding can transform death into life, and bring healing into the dark and broken corners of human existence. There are many examples of this sort of religious language being applied to life, but one of the most beautiful I have come across is Wendell Berry’s reflections on topsoil:\n\nThe topsoil exists as such because it is ceaselessly transforming death into life, ceaselessly supplying food and water to all that lives in it and from it;….if we are to live well on and from our land, we must live by faith in the ceaselessness of these processes and by faith in our own willingness and ability to collaborate with them. Christ’s prayer for “daily bread” is an affirmation of such faith, just as it is a repudiation of faith in “much goods laid up.” Our life and livelihood are the gift of topsoil and of our willingness and ability to care for it, to grow good wheat, to make good bread; they do not derive from stockpiles of raw materials or accumulations of purchasing power.\n\nThe stories of the Bible have a great deal of meaning for me, and help me to understand the world around me. This is not to say that I do not borrow from other religions from time to time, much the way an English speaker might borrow a German word that expresses some interesting concept that does not exist in English. Nonetheless, Christianity is the language I am most fluent in, and therefore I remain Christian.\nThe other important reason is because I am a part of the story of Christianity. Quite simply, I am Christian because I grew up Christian. I used to see that as being an entirely insufficient reason because I was looking for some sort of universal and logical validation for my beliefs. Such certainty would be nice, but it simply is not available to human beings. Human beings are social creatures that construct meaning largely by telling stories. We tell stories about our own lives, about each other, about our ancestors, and about our religious beliefs. The stories of Christianity are now my stories."
  },
  {
    "objectID": "posts/what_I_believe/index.html#a-few-words-on-salvation",
    "href": "posts/what_I_believe/index.html#a-few-words-on-salvation",
    "title": "What I Believe",
    "section": "A Few Words on Salvation",
    "text": "A Few Words on Salvation\nSalvation is not about life after death. It’s about this life. Let me say that again. Salvation is not about the afterlife, it’s about the here and now. It is, once again, about how you perceive the world. Here’s the thing about the fish and the water. The water is there whether the fish know it or not. Salvation is a real possibility, a way of being in the world and a way of relating to other people. A way to avoid being eaten alive by the default gods of power, fame, wealth, and intellect. By the way, I didn’t just make that up, it comes in part from my faith tradition:\n\nThe Kingdom of God is not coming with things that can be observed; nor will they say, ‘Look, here it is!’ or ‘There it is!’ For, in fact, the kingdom of God is among you. — Luke 17:20-21\n\nNow, a lot of things have happened in Christian theology over the years, and there is no one definitive way to interpret the Bible. Nonetheless, Jesus clearly taught a way of life that had a lot to do with love, compassion, and prayer, and not as much to do with religious dogma (particularly when that dogma got in the way of compassion).\nI have absolutely no idea what happens when we die, and don’t much care to speculate (although I will say I’m fairly sure it’s not a theology quiz). When I say that salvation is in this life I am not making a statement either way about the possibility of life after death. I am making a statement about this life, and how we have the very real possibility of living in awe of the mystery of life, the universe, and everything."
  },
  {
    "objectID": "posts/louisville_zoning/index.html#affordable-housing-and-municipal-zoning",
    "href": "posts/louisville_zoning/index.html#affordable-housing-and-municipal-zoning",
    "title": "End Exclusionary Zoning",
    "section": "",
    "text": "It is illegal to build affordable housing in most of Louisville. While there are a lot of contributing factors to the shortage of affordable housing in the U.S., one major reason it exists is because of policies we have intentionally put in place to reduce the supply of housing out of fear of a possible decline in property values and having to share ‘our neighborhoods’ with people of different classes and races. It is illegal to redline a map and explicitly exclude people from buying in certain neighborhoods on the basis of race, but it is a legal and widespread practice to use zoning ordinances to restrict the supply of affordable housing. In practice, zoning restrictions ensure high levels of racial and economic segregation.\n\n\nMulti-family housing includes a lot more than just large apartment complexes. There’s a wide range of types of housing that can include multiple families. A review of Louisville’s current housing situation refers to this as the ‘missing middle housing’. Louisville currently has limited areas zoned for large apartment complexes and huge swathes of land dedicate exclusively to single family housing.\n\n\n\nAn Illustration of the types of Housing made illegal by Louisville’s current zoning\n\n\nSingle family housing starts out less affordable because it requires more land per person, but additional zoning restrictions around minimum lot sizes and required parking spaces also drive up the cost. If you want to learn a little more about zoning and housing prices in general, this is a good 10 minute video explaining the key ways restricted zoning makes housing less affordable."
  },
  {
    "objectID": "posts/myth_of_waste_fraud_and_abuse/index.html",
    "href": "posts/myth_of_waste_fraud_and_abuse/index.html",
    "title": "The Perpetual Myth of Waste, Fraud, and Abuse",
    "section": "",
    "text": "One of the things that makes a discussion of waste, fraud, and abuse difficult is that there is, inevitably, at least some level of spending that could be classified as waste in any sufficiently large organization. This makes it difficult for politicans to defend against charges of waste, fraud, and abuse, as they fear looking foolish when an incident does come to light.\nWaste, Fraud, and Abuse are all real things and are not the myth I’m referring to in the title. The myth is that they exist in sufficent quantities that cutting them will allow us to have both low taxes and high quality government services.\n\nlook at history of other politicians who have looked into waste, fraud, and abuse\nGAO\nwhy Elon hasn’t found much\n\n## The Government Accountability Office\nThe most common response from Democrats has been to say something about supporting cutting waste, fraud, and abuse, but only if it is done well. It already has been done well!"
  },
  {
    "objectID": "posts/myth_of_waste_fraud_and_abuse/index.html#a-short-history-of-waste-fraud-and-abuse",
    "href": "posts/myth_of_waste_fraud_and_abuse/index.html#a-short-history-of-waste-fraud-and-abuse",
    "title": "The Perpetual Myth of Waste, Fraud, and Abuse",
    "section": "",
    "text": "One of the things that makes a discussion of waste, fraud, and abuse difficult is that there is, inevitably, at least some level of spending that could be classified as waste in any sufficiently large organization. This makes it difficult for politicans to defend against charges of waste, fraud, and abuse, as they fear looking foolish when an incident does come to light.\nWaste, Fraud, and Abuse are all real things and are not the myth I’m referring to in the title. The myth is that they exist in sufficent quantities that cutting them will allow us to have both low taxes and high quality government services.\n\nlook at history of other politicians who have looked into waste, fraud, and abuse\nGAO\nwhy Elon hasn’t found much\n\n## The Government Accountability Office\nThe most common response from Democrats has been to say something about supporting cutting waste, fraud, and abuse, but only if it is done well. It already has been done well!"
  },
  {
    "objectID": "posts/myth_of_waste_fraud_and_abuse/index.html#efficiency-is-a-lot-more-than-just-eliminating-waste-fraud-and-abuse",
    "href": "posts/myth_of_waste_fraud_and_abuse/index.html#efficiency-is-a-lot-more-than-just-eliminating-waste-fraud-and-abuse",
    "title": "The Perpetual Myth of Waste, Fraud, and Abuse",
    "section": "Efficiency is a lot more than just eliminating waste, fraud, and abuse",
    "text": "Efficiency is a lot more than just eliminating waste, fraud, and abuse\nAn underdiscussed irony of this whole charade is that the processes and procedures required to reduce or eliminate waste, fraud, and abuse are actually very inefficent.\nTo prevent government contracts from being given out as personal favors the government has clear standards for contracts and an involved approval process that requires several people to approve each contract. This is, in an immediate sense, extremely time consuming and inefficient relative to a system where one person has control and can quickly decide on a contractor to use and execute a contract. But systems that are efficient in the sense of being able to move quickly are often prone to abuse because they have fewer veto points that provide checks against individuals abusing the system. More process and procedures means slow ‘inefficient’ outcomes, but less waste, fraud, and abuse.\n(NY Times Article)[https://www.nytimes.com/2025/03/13/upshot/musk-doge-changes-deletions.html?unlocked_article_code=1.304.pJHf.DTibHql8U3db&smid=url-share]"
  }
]