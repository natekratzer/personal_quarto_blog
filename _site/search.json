[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a data scientist with an interest in public policy, statistics, and coding. This blog is mostly to help me organize my thoughts, although I may sometimes also use it to share policy analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nate Kratzer",
    "section": "",
    "text": "Partitioned Regression with Palmer Penguins and Scikit-Learn\n\n\n\n\n\n\n\nPython\n\n\nPenguins\n\n\nStatistics\n\n\n\n\nUsing partitioned regression to gain a better understanding of how linear regression works.\n\n\n\n\n\n\nMar 18, 2023\n\n\n\n\n\n\n  \n\n\n\n\nThink before adding more variables to that analysis\n\n\n\n\n\n\n\nPython\n\n\nCausal Inference\n\n\nPolicy\n\n\n\n\nAn introduction to thinking about causal models for data analysis. The purpose is to demonstrate that the popular approach of simply gathering as much data as you can and controlling for it via regression or other methods is not a good one, and is actively misleading in many cases. We should instead carefully think about plausible causal models using tools like diagrams (directed acyclic graphs, or DAGs) and then do data analysis in accordance with those models.\n\n\n\n\n\n\nMar 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHow to Use Census Microdata to Analyze High Speed Internet in Kentucky\n\n\n\n\n\n\n\nR\n\n\nCensus\n\n\nInternet\n\n\nPolicy\n\n\n\n\nThis post is a start to finish descriptive analysis of high speed internet access in Kentucky, including tables, graphs, and maps. All of the detail of cleaning the data and iterating while exploring the data is included. This makes for a rather lengthy post, but it also makes it relatively unique in including all of those steps. We go through five attempts at making a table of high speed internet before finally getting it right! There’s quite a bit of cleaning work and then also a detour into calculating standard errors via bootstrap so we can correctly display uncertainty in our visuals.\n\n\n\n\n\n\nSep 13, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/census_internet_microdata/index.html",
    "href": "posts/census_internet_microdata/index.html",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "",
    "text": "This post is a start to finish descriptive analysis of high speed internet access in Kentucky, including tables, graphs, and maps. All of the detail of cleaning the data and iterating while exploring the data is included. This makes for a rather lengthy post, but it also makes it relatively unique in including all of those steps. We go through five attempts at making a table of high speed internet before finally getting it right! There’s quite a bit of cleaning work and then also a detour into calculating standard errors via bootstrap so we can correctly display uncertainty in our visuals.\nCensus microdata is individual level data provided by the census. Most references to census data refer to tables the census bureau has already made out of their surveys, but the mostly raw survey data is available at the individual level, and that’s what we’ll use for this analysis. While the focus is on the census data, I do show the code used for analysis. I did decide to hide the code used to make all the tables (it gets rather lengthy), but you can see that code on Github if interested."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#getting-wrong-answers-by-not-knowing-the-data",
    "href": "posts/census_internet_microdata/index.html#getting-wrong-answers-by-not-knowing-the-data",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Getting wrong answers by not knowing the data",
    "text": "Getting wrong answers by not knowing the data\nNow that we have a high speed internet category we can group the data and count up how many responses are in each group. I’ll also pivot the dataframe to make it easy to calculate percent with high speed internet.\n\n# Count numbers with and without high speed internet\ndf_group <- df %>%\n  group_by(hspd_int, YEAR) %>%\n  summarize(count = n(), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = YEAR,\n              names_from = hspd_int,\n              values_from = count) %>%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  ) \n\n\n\n\n\n\n\n  \n    \n      Table 1: Quick Analysis\n    \n    \n      These results are wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n87%\n13%\n28%\n28.3K\n4.27K\n12.4K\n    2014\n86%\n14%\n27%\n28.1K\n4.70K\n12.1K\n    2015\n86%\n14%\n26%\n28.7K\n4.52K\n11.5K\n    2016\n82%\n18%\n20%\n29.2K\n6.49K\n9.02K\n    2017\n81%\n19%\n19%\n29.4K\n7.08K\n8.77K\n    2018\n80%\n20%\n17%\n30.3K\n7.35K\n7.86K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's incorrect analysis of IPUMS data. Used as an example of a mistake to avoid.\n    \n  \n  \n\n\n\n\nThis table is actually wrong for multiple reasons, but the first one we’ll take care of is the failure to use weights. Survey data is often weighted to make it representative of the population. The census bureau provides a PERWT variable that should be used as the weight for each person in the file. There’s also a HHWT variable for household level analysis. We’ll stick with weighting the data by the number of people. One of the really nice features of the PERWT variable is that it sums to the population. That means our tables can show both an overall number of people and the percentage of people.\n\n# Count numbers with and without high speed internet\ndf_group <- df %>%\n  group_by(hspd_int, YEAR) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = YEAR,\n              names_from = hspd_int,\n              values_from = count) %>%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  )\n\n\n\n\n\n\n\n  \n    \n      Table 2: Quick Analysis with Weights\n    \n    \n      These results are still wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n86%\n14%\n27%\n2.76M\n454K\n1.18M\n    2014\n84%\n16%\n26%\n2.74M\n509K\n1.17M\n    2015\n86%\n14%\n25%\n2.85M\n479K\n1.10M\n    2016\n80%\n20%\n19%\n2.87M\n708K\n855K\n    2017\n80%\n20%\n18%\n2.90M\n735K\n817K\n    2018\n79%\n21%\n16%\n2.97M\n790K\n709K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's incorrect analysis of IPUMS data. Used as an example of a mistake to avoid.\n    \n  \n  \n\n\n\n\nThis is better. The second problem is harder to spot. There are 3 hints in the data:\n\nThere is a very high percentage of NA responses. There are more NA answers than there are people who say they don’t have high speed access.\nPercent of of people with high speed access is going down over time, which isn’t what I’d expect to see. That doesn’t mean it’s wrong - sometimes the data shows high level trends we don’t expect. However, it’s always worth a second look when you get a counterintuitive result.\nThese numbers look very high for Kentucky.\n\nA sensible guess is that people who say they don’t have internet access at all aren’t then asked about high speed internet and show up as an NA value when we want to code them as not having high speed interent.\nSo let’s get to know the data a bit better by adding in internet access. We’ll do the same analysis, but I’ll add internet as another id variable just like year. We can see right away that the answers we have above are only including cases where individuals have internet.\n\ndf <- df %>%\n  mutate(\n    int = case_when(\n      CINETHH == 0 ~ NA_character_,\n      CINETHH == 1 | CINETHH == 2 ~ \"Yes\",\n      CINETHH == 3 ~ \"No\",\n      TRUE ~ NA_character_\n    )\n  )\n\ndf_group <- df %>%\n  group_by(hspd_int, int, YEAR) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(\n    id_cols = c(YEAR, int),\n    names_from = hspd_int,\n    values_from = count\n  ) %>%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  )\n\n\n\n\n\n\n\n  \n    \n      Table 3: Exploring Internet Data\n    \n    \n      These results are exploratory. But not wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    \n      Has Internet Access\n    \n    2013\n86%\n14%\n5%\n2.76M\n454K\n185K\n    2014\n84%\n16%\n6%\n2.74M\n509K\n193K\n    2015\n86%\n14%\n6%\n2.85M\n479K\n217K\n    2016\n80%\n20%\n3%\n2.87M\n708K\n129K\n    2017\n80%\n20%\n3%\n2.90M\n735K\n124K\n    2018\n79%\n21%\n3%\n2.97M\n790K\n125K\n    \n      No Internet Access\n    \n    2013\nNA\nNA\nNA\nNA\nNA\n866K\n    2014\nNA\nNA\nNA\nNA\nNA\n842K\n    2015\nNA\nNA\nNA\nNA\nNA\n753K\n    2016\nNA\nNA\nNA\nNA\nNA\n596K\n    2017\nNA\nNA\nNA\nNA\nNA\n560K\n    2018\nNA\nNA\nNA\nNA\nNA\n452K\n    \n      Internet Access is NA\n    \n    2013\nNA\nNA\nNA\nNA\nNA\n126K\n    2014\nNA\nNA\nNA\nNA\nNA\n130K\n    2015\nNA\nNA\nNA\nNA\nNA\n129K\n    2016\nNA\nNA\nNA\nNA\nNA\n131K\n    2017\nNA\nNA\nNA\nNA\nNA\n132K\n    2018\nNA\nNA\nNA\nNA\nNA\n132K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nWhen we split it out this way we can see that the groups without any internet access are all NA values. So what we were looking at was the percentage of people with internet who have high speed internet. What we want is the percentage of all people who have high speed internet. We can fix the way we create our categories by saying that anyone who has no internet also has no high speed internet.\n\ndf <- df %>%\n  mutate(\n    #create a categorical variable for having any internet\n    int = case_when(\n      CINETHH == 0 ~ NA_character_,\n      CINETHH == 1 | CINETHH == 2 ~ \"Yes\",\n      CINETHH == 3 ~ \"No\",\n      TRUE ~ NA_character_\n    ),\n    # change how high speed internet is defined so that houses w/o any interent are counted as 'No' instead of NA\n    hspd_int = case_when(\n      CIHISPEED == 00 & int != \"No\" ~ NA_character_,\n      CIHISPEED == 20 | int == \"No\" ~ \"No\",\n      CIHISPEED >= 10 & CIHISPEED < 20 ~ \"Yes\",\n      TRUE ~ NA_character_\n    )\n  )\n\n# Count numbers with and without high speed internet\ndf_group <- df %>%\n  group_by(hspd_int, YEAR) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(\n    id_cols = c(YEAR),\n    names_from = hspd_int,\n    values_from = count\n  ) %>%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  )\n\n\n\n\n\n\n\n  \n    \n      Table 4: Almost right!\n    \n    \n      These results are still a little wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n68%\n32%\n7%\n2.76M\n1.32M\n311K\n    2014\n67%\n33%\n7%\n2.74M\n1.35M\n324K\n    2015\n70%\n30%\n8%\n2.85M\n1.23M\n346K\n    2016\n69%\n31%\n6%\n2.87M\n1.30M\n260K\n    2017\n69%\n31%\n6%\n2.90M\n1.29M\n256K\n    2018\n71%\n29%\n6%\n2.97M\n1.24M\n257K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's incorrect analysis of IPUMS data. Used as an example of a mistake to avoid.\n    \n  \n  \n\n\n\n\nThese results look much better, although there’s still one more thing to do about those NA results."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#group-quarters-in-the-census",
    "href": "posts/census_internet_microdata/index.html#group-quarters-in-the-census",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Group Quarters in the Census",
    "text": "Group Quarters in the Census\nThe census data includes individuals living in group quarters (mostly prisons, senior living centers, and dorms, but includes any sort of communal living arrangement). However, all census questions about appliances and utilities (the category that internet access falls under) are NA for group quarters. So we’ll add one more line to filter out individuals living in group quarters (a common practice when working with Census microdata). The code below adds a filter for Group Quarters. Since this table is showing correct results I’ll also add a little additional formatting to make it stand out from the others.\nI’ll also note that the way the Census Bureau constructs weights is very convenient for getting totals. While I’m focusing on the percent of people who have internet access, the Yes and No columns are accurate estimates of the population with and without access.\n\n# Count numbers with and without high speed internet\ndf_group <- df %>%\n  filter(GQ == 1 | GQ ==2 | GQ == 5) %>%\n  group_by(hspd_int, YEAR) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = c(YEAR), names_from = hspd_int, values_from = count) %>%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd,\n         percent_NA = (`NA` / (Yes + No + `NA`)))\n\n\n\n\n\n\n\n  \n    \n      Table 5: High Speed Internet in Kentucky\n    \n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n68%\n32%\n4%\n2.76M\n1.32M\n185K\n    2014\n67%\n33%\n5%\n2.74M\n1.35M\n193K\n    2015\n70%\n30%\n5%\n2.85M\n1.23M\n217K\n    2016\n69%\n31%\n3%\n2.87M\n1.30M\n129K\n    2017\n69%\n31%\n3%\n2.90M\n1.29M\n124K\n    2018\n71%\n29%\n3%\n2.97M\n1.24M\n125K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nThat removed about half of our NA values. It might be nice to know a bit more about the missing data, but at around 3 percent of observations it’s unlikely to change our substantive conclusions. I suspect these are cases where there wasn’t an answer for that question. We’ll keep an eye on NA values as we do the analysis, because as we get into questions like how internet access varies by race, income, age, and education we’ll want to know if NA answers are more or less likely in any of those categories."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#checking-against-data.census.gov",
    "href": "posts/census_internet_microdata/index.html#checking-against-data.census.gov",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Checking against data.census.gov",
    "text": "Checking against data.census.gov\nTo do a quick check against the way the census bureau itself analyzes the data I looked at data.census.gov for 2018 in Kentucky. An important note is that their data is for households, and so their numeric counts look quite different because I’m counting number of people. They also have a breakdown where cellular is included in broadband, which I do not want, as a cell phone is not really an adequate work or study device. So to get to what I have we need to add “Broadband such as cable, fiber optic or DSL” and “Satellite Internet service”, which gets us to 70.8% compared to the 70.5% in this analysis. The difference is small and most likely the result of their analysis being weighted to the household level rather than the person level. (Internet is measured at the household level and the same for every person in the household, but by choosing to weight it at the person level I am a) letting us talk in terms of people, b) giving more weight to larger households, c) making it possible to break down internet access by categories that do vary within households, like age)."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#standard-errors",
    "href": "posts/census_internet_microdata/index.html#standard-errors",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Standard Errors",
    "text": "Standard Errors\nKnow that we know the data we’d also like to know how uncertain our sample is so that we know if movements over time are real or just a result of noisy data. There are a few ways to do this. The survey package does an excellent job with complex survey designs, but does require learning a new syntax to use. The alternative I’ll use here is a method known as bootstrap. IPUMS suggests using bootstrap might be the best way to get standard errors on census microdata. The basic idea of the bootstrap is to resample the existing data and use the sampling error from that as an estimate for sampling error in the overall population. Let’s do an example with high speed internet in 2018 to see how it works. The output here will be the mean and standard deviation for Kentucky. (We’ll use the standard error to calculate confidence intervals once we start displaying actual results.)\n\n#set seed\nset.seed(42)\n\n# Filter to just 2018\n# Exclude NA values\n# Recode as numeric vector of 1 and 0\n# The numeric 1 and 0 form will make it much easier to get means without pivoting, which matters a lot when doing this 1000 times\ndf2018 <- df %>%\n  filter(YEAR == 2018 & !is.na(hspd_int)) %>%\n  mutate(hspd_num = if_else(hspd_int == \"Yes\", 1, 0)) %>%\n  select(hspd_num, PERWT)\n\n# Write a function so I can map over it.\n# In this case, we need the function to do the same thing X number of times and assign an ID that we can use as a grouping variable\ncreate_samples <- function(sample_id){\n  df_out <- df2018[sample(nrow(df2018), nrow(df2018), replace = TRUE) , ] %>%\n    as_tibble()\n  df_out$sample_id <- sample_id\n  return(df_out)\n}\n\nnlist <- as.list(seq(1, 1000, by = 1))\nsamples <- purrr::map_df(nlist, create_samples)\n\nsample_summary <- samples %>%\n  group_by(sample_id) %>%\n  mutate(ind_weight = PERWT / sum(PERWT),\n         hspd_weight = hspd_num * ind_weight) %>% # PERWT is population and doesn't sum to 1. Rescale it to sum to one\n  summarize(group_mean = sum(hspd_weight),\n            weight_check = sum(ind_weight), .groups = \"drop\") # Check that my weights add up to one\n\ndisplay_tbl <- tibble(\n  mean = mean(sample_summary$group_mean),\n  sd = sd(sample_summary$group_mean)\n) \n\n\n\n\n\n\n\n  \n  \n    \n      mean\n      sd\n    \n  \n  \n    0.7053407\n0.002866898\n  \n  \n  \n\n\n\n\nWe can also take a look at our bootstrap graphically. We want to check that the distribution of the sample is roughly normal. If it’s not, that means we didn’t do enough bootstrap samples for the Central Limit Theorem to kick in.\n\n#Check that the distribution is normal and than the middle of the distribution is close to the 70.5% we estimated had internet access above\nggplot(sample_summary, aes(group_mean)) +\n  geom_density() + theme_bw() +\n  labs(title = \"Bootstrapped means of High Speed Internet Access\",\n       x = \"Mean\", \n       y = \"Kernel Density\")"
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#checking-our-results-against-the-survey-package",
    "href": "posts/census_internet_microdata/index.html#checking-our-results-against-the-survey-package",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Checking our results against the survey package",
    "text": "Checking our results against the survey package\nAbove we found a mean of 0.705 for 2018 and and standard error of 0.0029 based on our bootstrap analysis. It’s worth checking that this is the same result we’d get using an analytic approach (instead of bootstrap).\n\n# Here we're assuming a simple design. \n# Survey requires the creation of a design object and then has functions that work with that object.\n# You can get more complicated, which is when the survey package would be most useful.\nsvy_df <- svydesign(ids = ~ 1, weights = ~PERWT, data = df2018)\n\n# Taking the mean and standard error from our design object\nhint_tbl <- svymean(~hspd_num, design = svy_df)\n\nhint_tbl <- as_tibble(hint_tbl)\nnames(hint_tbl) <- c(\"mean\", \"sd\") #The names weren't coerced correctly when transforming into a tibble. \n\n\n\n\n\n\n\n  \n  \n    \n      mean\n      sd\n    \n  \n  \n    0.7051774\n0.00293509\n  \n  \n  \n\n\n\n\nThese results are very similar. Following the IPUMS recommendation we’ll continue on with the bootstrap, but it’s good to know the results are the same for practical purposes. So now instead of just doing 2018, we’ll need to do every year. We already know the mean values for every year, and they’re still saved in the df_wide variable right now. So let’s write a function for bootstrap that will let us find standard errors for every year or for any other grouping we choose."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#writing-a-bootstrap-function",
    "href": "posts/census_internet_microdata/index.html#writing-a-bootstrap-function",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Writing a bootstrap function",
    "text": "Writing a bootstrap function\n\n# Create a helper function\n# It needs to have a way to recieve the dataframe from the function that calls it, so we've added a second argument\ncreate_samples <- function(sample_id, df){\n  \n  df_out <- df[sample(nrow(df), nrow(df), replace = TRUE) , ] %>%\n    as_tibble()\n  \n  df_out$sample_id <- sample_id\n  \n  return(df_out)\n}\n\n# Need to be able to take in grouping variables so that the summaries can be specific to the groups\n# Using the embrace {{}} notation allows use to pass in unquoted variables to the function. \n\nbootstrap_pums <- function(df, num_samples, group_vars) {\n  \n  nlist <- as.list(seq(1, num_samples, by = 1))\n  samples <- purrr::map_df(nlist, create_samples, df)\n  \n  sample_summary <- samples %>%\n    group_by( sample_id, across( {{group_vars}} )) %>%\n    mutate(ind_weight = PERWT / sum(PERWT),\n           hspd_weight = hspd_n * ind_weight) %>% # PERWT sums to population instead of to 1. Rescale it to sum to 1.\n    summarize(group_mean = sum(hspd_weight), .groups = \"drop\") # Not dropping .groups here results in problems in the next group_by call.\n  \n  sample_sd <- sample_summary %>%\n    group_by( across( {{ group_vars }} )) %>%\n    summarize(sd = sd(group_mean), .groups = \"drop\")\n}\n\n# We do need to prep the data a little so that we're not carrying through the whole dataframe.\ndf_in <- df %>%\n   filter(!is.na(hspd_int)) %>%\n   mutate(hspd_n = if_else(hspd_int == \"Yes\", 1, 0)) %>%\n   select(hspd_n, PERWT, YEAR)\n\n# And finally we can call the function\nboot_results <- bootstrap_pums(df = df_in, num_samples = 500, group_vars = YEAR)\n\nNow that we have our bootstrap standard errors we can combine them with the data and plot them. We’ll use 95% confidence intervals, which we get by multiplying the standard error by 1.96 (the number of standard deviations that corresponds to a 95% confidence interval).\n\ndf_plt <- df_wide %>%\n  full_join(boot_results, by = \"YEAR\") %>%\n  transmute(Year = YEAR,\n            Percent = 100 * percent_hspd,\n            me = 100 * 1.96 * sd)\n  \nplt_int <- ggplot(df_plt, aes(x = Year, y = Percent)) +\n  geom_errorbar(aes(ymin = Percent - me, ymax = Percent + me), width = .1) +\n  geom_line() +\n  geom_point() +\n  theme_bw() +\n  labs(title = \"High Speed Internet Access\") +\n  theme(legend.position = \"bottom\")\n\nplt_int"
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#race-poverty-age-and-geography",
    "href": "posts/census_internet_microdata/index.html#race-poverty-age-and-geography",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Race, Poverty, Age, and Geography",
    "text": "Race, Poverty, Age, and Geography\nGoing a little deeper we can use microdata to get results by custom groupings. I’ll show examples for race, poverty, age, and geography, but for any group you can construct with available Census data you can produce an estimate for their internet acess.\n\nRace\nNext we’ll build a table by race and year.\n\n# Let's build a table first and then we'll do the standard errors\n\n# Coding a race variable using case_when\ndf <- df %>%\n  mutate(race = case_when(\n            RACE == 1 ~ \"White\",\n            RACE == 2 ~ \"Black\",\n            RACE > 3 & RACE < 7 ~ \"Asian\",\n            HISPAN > 0 & HISPAN < 5 ~ \"Hispanic\",\n            TRUE ~ \"All Others\"\n          ))\n\ndf_group <- df %>%\n  group_by(hspd_int, race, YEAR) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = c(race, YEAR), names_from = hspd_int, values_from = count) %>%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd,\n         percent_NA = (`NA` / (Yes + No + `NA`)))\n\n\n\n\n\n\n\n  \n    \n      Table 6: High Speed Internet Access\n    \n    \n      By Race and Ethnicity\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    \n      Asian\n    \n    2013\n84%\n16%\n4%\n42.2K\n8.03K\n2.06K\n    2014\n76%\n24%\n2%\n41.6K\n12.9K\n1.19K\n    2015\n83%\n17%\n3%\n48.8K\n10.3K\n1.76K\n    2016\n77%\n23%\n10%\n43.8K\n13.0K\n6.55K\n    2017\n80%\n20%\n1%\n52.5K\n12.9K\n703\n    2018\n82%\n18%\n3%\n52.8K\n11.7K\n1.66K\n    \n      Black\n    \n    2013\n64%\n36%\n9%\n192K\n106K\n28.7K\n    2014\n58%\n42%\n9%\n171K\n122K\n30.3K\n    2015\n62%\n38%\n12%\n183K\n113K\n40.5K\n    2016\n62%\n38%\n5%\n204K\n127K\n15.6K\n    2017\n64%\n36%\n2%\n216K\n122K\n8.04K\n    2018\n62%\n38%\n3%\n200K\n122K\n9.90K\n    \n      Hispanic\n    \n    2013\n48%\n52%\n4%\n21.6K\n23.8K\n1.90K\n    2014\n38%\n62%\n12%\n15.7K\n25.2K\n5.66K\n    2015\n47%\n53%\n10%\n17.2K\n19.5K\n4.01K\n    2016\n59%\n41%\n4%\n23.0K\n15.8K\n1.59K\n    2017\n57%\n43%\n1%\n24.7K\n18.5K\n421\n    2018\n59%\n41%\n1%\n30.9K\n21.1K\n616\n    \n      White\n    \n    2013\n68%\n32%\n4%\n2.45M\n1.16M\n148K\n    2014\n68%\n32%\n4%\n2.44M\n1.16M\n152K\n    2015\n70%\n30%\n4%\n2.54M\n1.07M\n164K\n    2016\n69%\n31%\n3%\n2.54M\n1.13M\n102K\n    2017\n70%\n30%\n3%\n2.54M\n1.11M\n113K\n    2018\n71%\n29%\n3%\n2.61M\n1.06M\n108K\n    \n      All Other Races\n    \n    2013\n72%\n28%\n6%\n58.3K\n22.3K\n4.90K\n    2014\n70%\n30%\n4%\n65.9K\n27.7K\n3.91K\n    2015\n71%\n29%\n7%\n59.6K\n24.3K\n6.19K\n    2016\n75%\n25%\n4%\n63.2K\n21.3K\n3.12K\n    2017\n71%\n29%\n2%\n68.9K\n28.2K\n2.23K\n    2018\n72%\n28%\n5%\n73.9K\n29.4K\n5.28K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nWhile we do see high NA values in some years, overall they’re at reasonable levels and seem to be lowest in 2018. This table is quite long though. While that works for exploring the data, for an actual display we’d probably want to focus on just 2018 and show the history in a graph. Below is the table filtered to just 2018 and slightly reformatted.\n\n\n\n\n\n\n  \n    \n      Table 7: High Speed Internet Access in 2018\n    \n    \n      By Race and Ethnicity\n    \n  \n  \n    \n      Race/Ethnicity\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    All Others\n72%\n28%\n5%\n73.9K\n29.4K\n5.28K\n    Asian\n82%\n18%\n3%\n52.8K\n11.7K\n1.66K\n    Black\n62%\n38%\n3%\n200K\n122K\n9.90K\n    Hispanic\n59%\n41%\n1%\n30.9K\n21.1K\n616\n    White\n71%\n29%\n3%\n2.61M\n1.06M\n108K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nNow let’s add standard errors and graph the data. I’ll write the code for graphing the data as a function since we will use it again.\n\n# We do need to prep the data a little so that we're not carrying through the whole dataframe.\ndf_in <- df %>%\n  filter(!is.na(hspd_int)) %>%\n  mutate(hspd_n = if_else(hspd_int == \"Yes\", 1, 0)) %>%\n  select(hspd_n, PERWT, YEAR, race)\n\n# And we can call the bootstrap function\nboot_results <- bootstrap_pums(df = df_in, num_samples = 500, group_vars = c(YEAR, race))\n\ndf_plt <- df_wide %>%\n  full_join(boot_results, by = c(\"race\", \"YEAR\")) %>%\n  transmute(Year = YEAR,\n            Race = race,\n            Percent = 100 * percent_hspd,\n            me = 100 * 1.96 * sd) %>%\n  filter(Race != \"All Others\") # When plotting All Others overlaps White and having five lines makes it quite hard to read. \n\n# At this point I'll introduce a function to plot multiple groups over time, since we'll use this again \nplt_by <- function(df, group_var, title_text = \"\") {\n  \n  plt <- ggplot(data = df, aes(x = Year, y = Percent, group = {{group_var}}, colour = {{group_var}})) +\n    geom_errorbar(aes(ymin = Percent - me, ymax = Percent + me), width = .1) +\n    geom_point() +\n    geom_line() +\n    theme_bw() +\n    labs(title = title_text, x = \"Year\", y = \"Percent\") +\n    theme(legend.position = \"bottom\")\n\n  plt\n}\n\nplt_race <- plt_by(df_plt, Race, title_text = \"High Speed Internet Access by Race and Ethnicity\")\n\nplt_race\n\n\n\n\n\n\nPoverty Status\nI’m going to skip showing the code for Poverty, Age, and Geography because it’s extremely similar to the code used for Race. The poverty variable in the IPUMS data is measured in income as a percent of the poverty line. So for this analysis I code under 100 as being in poverty, between 100 and 200 percent of the poverty line as near poverty, and above 200 percent as not being in poverty.\n\n\n\n\n\n\n\n\n\n  \n    \n      Table 8: High Speed Internet Access in 2018\n    \n    \n      By Poverty Status\n    \n  \n  \n    \n      Poverty Status\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    In Poverty\n53%\n47%\n4%\n376K\n335K\n30.6K\n    Near Poverty\n62%\n38%\n4%\n499K\n311K\n29.7K\n    Not in Poverty\n78%\n22%\n2%\n2.09M\n595K\n64.8K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nAge\nThe age varible in IPUMS is the most straightforward, it’s just numeric age. It is worth pointing out that this is based on individual ages, while high speed internet is a household level variable. If we really wanted to do a deep dive into just age, we’d want to look at the age composition of the whole household. The really nice thing about microdata is you can slice and dice it any way you think is appropriate. So if you want to look at households that have individuals under 18 and over 65, you can. If you want to look at households with no one under 65, you can. I’ve just taken a high level cut of the data here though.\n\n\n\n\n\n\n\n\n\n  \n    \n      Table 9: High Speed Internet Access in 2018\n    \n    \n      By Age Group\n    \n  \n  \n    \n      Age Group\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    18 and under\n74%\n26%\n2%\n760K\n263K\n26.0K\n    19 to 64\n72%\n28%\n3%\n1.81M\n691K\n76.7K\n    65+\n58%\n42%\n3%\n398K\n288K\n22.4K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nGeography\nIPUMs provides categorizations of where people live as either being in a prinicipal city, in the metro but not the principal city, or outside of a metro area. I have chosen to present this as the more familiar (and shorter) urban, suburban, and rural. IPUMS also has a direct measure of density that would be useful in an analysis - but for tables and graphs the categorial variable is better.\n\n\n\n\n\n\n\n\n\n  \n    \n      Table 10: High Speed Internet Access in 2018\n    \n    \n      By Metropolitan Status\n    \n  \n  \n    \n      Metropolitan Status\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    City\n85%\n15%\n3%\n257K\n46.2K\n8.16K\n    Mixed/Unknown\n68%\n32%\n3%\n1.60M\n752K\n74.7K\n    Rural\n67%\n33%\n3%\n694K\n341K\n35.4K\n    Suburbs\n81%\n19%\n1%\n420K\n102K\n6.77K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#school-age-children",
    "href": "posts/census_internet_microdata/index.html#school-age-children",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "School Age Children",
    "text": "School Age Children\nAs I’ve mentioned, a nice feature of using microdata is that you can look at the data in ways that aren’t available in the premade Census tabulations. With a lot of school districts using online learning, a look at the percent of school age children (ages 5-18) without high speed internet access at home could be useful for policymakers. Here we can see that there are parts of Western Kentucky where up to 60% do not have high speed access at home.\n\n#need to recreate the sf object for the joins to work correctly\nky_sf <- st_as_sf(ky_shp)\n\n# Calculate internet access at the PUMA level\ndf_group <- df %>%\n  filter(YEAR == 2018 & AGE >= 5 & AGE <= 18) %>%\n  group_by(hspd_int, PUMA) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = PUMA, names_from = hspd_int, values_from = count) %>%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd)\n\nint_puma <- tibble(\n  PUMA = df_wide$PUMA,\n  int_per = df_wide$percent_no * 100,\n  int_num = df_wide$No\n  )\n\nky_sf <- full_join(ky_sf, int_puma, by = \"PUMA\")\n\nggplot(ky_sf) + \n  geom_sf(aes(fill=int_per)) +\n  scale_fill_gradient(low = \"blue\", high = \"purple\", name = \"Percent\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        panel.border = element_blank()) +\n  labs(title = \"Children ages 5-18 in Households without High Speed Internet Access\",\n       caption = \"Map is shaded by percent of children without high speed access in each Public Use Microdata Area\")\n\n\n\n\n\nLouisville (Jefferson County) with Labels\n\n#need to recreate the sf object for the joins to work correctly\nky_sf <- st_as_sf(ky_shp) %>%\n  filter(PUMA %in% c(\"1701\", \"1702\", \"1703\", \"1704\", \"1705\", \"1706\"))\n\n# Calculate internet access at the PUMA level\n# Filter to only the PUMAs in Jefferson County\ndf_group <- df %>%\n  filter(YEAR == 2018 & AGE >= 5 & AGE <= 18 & PUMA %in% c(\"1701\", \"1702\", \"1703\", \"1704\", \"1705\", \"1706\")) %>%\n  group_by(hspd_int, PUMA) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = PUMA, names_from = hspd_int, values_from = count) %>%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd)\n\nint_puma <- tibble(\n  PUMA = df_wide$PUMA,\n  int_per = df_wide$percent_no * 100,\n  int_num = formattable::comma(round(df_wide$No, -2), digits = 0)\n  )\n\nky_sf <- full_join(ky_sf, int_puma, by = \"PUMA\")\n\nggplot(ky_sf) + \n  geom_sf(aes(fill=int_per)) +\n  geom_sf_label(aes(label = int_num, fontface = \"bold\")) +\n  scale_fill_gradient(low = \"blue\", high = \"purple\", name = \"Percent\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        panel.border = element_blank()) +\n  labs(title = \"Children ages 5-18 in Households without High Speed Internet Access in Jefferson County\",\n       caption = \"Map is shaded by percent of children without access in each Public Use Microdata Area and \n       the number of children without access is given by the label rounded to the nearest 100\")\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data"
  },
  {
    "objectID": "posts/collider_bias/index.html",
    "href": "posts/collider_bias/index.html",
    "title": "Think before adding more variables to that analysis",
    "section": "",
    "text": "Human beings think in terms of stories and in terms of how the actions they take impact the things around them. It’s our natural default way of thinking, and generally it’s pretty useful.\nDoing data analysis doesn’t stop us from thinking in terms of stories and causation, but it should make us careful. With the increase in data and in the computing power to process it all, there have been claims that all we need in order to understand and act in the world is to listen to the data. But data does not speak for itself! It is interpretted by humans who will think about it through the lens of causality.\nThis is an introduction to thinking about causal models for data analysis. The purpose is to demonstrate that the popular approach of simply gathering as much data as you can and controlling for it via regression or other methods is not a good one, and is actively misleading in many cases. We should instead carefully think about plausible causal models using tools like diagrams (directed acyclic graphs, or DAGs) and then do data analysis in accordance with those models.\n\n\n\nLet’s start with an example where using regression does make sense. I have noticed that the sports teams I like are more likely to lose when I am watching them on TV. This is true, but the idea that my watching them causes them to lose is not plausible. So either I’m mistaken in my data collection, very unlucky in my fanship (I am a fan of Cleveland sports teams, so this does seem likely), or there’s something else that explains the connection between my watching and my team losing. We can draw a simple diagram of what we’ve observed so far.\n(I’m using Mermaid and will put the code for each diagram above them so that it’s easy to recreate and edit later).\ngraph LR; A[Watch Game]–>B[Lose Game]\n\nOnce we know what the proper causal model looks like, we can see that the conclusion that my watching games caused my teams to lose was based on an incomplete view - or more technically it suffered from omitted variable bias. The analysis left out an important variable that impacted things. Once we control for opponent quality, the relationship between my watching and my team losing should go back to zero.\n\n\n\nThe idea of drawing out the diagram before doing the analysis can be applied to more important cases, like the ongoing dispute around the wage gap between men and women. Here, I’m taking an example from the excellent book Causal Inference: The Mixtape by Scott Cunningham.\nWhen companies are accused of paying women less one of their first lines of defense is to argue that if you account for the occupational differences within the company between men and women the wage gap vanishes or at least shrinks dramatically. Cunningham (and I) think this is a poor causal model and an inadequate defense. This is important, so we’re going to consider several causal models and look directly at what they tell us using some simulated data under different specifications. Using simulated data gives us the advantage of knowing the truth of the data - so to speak - we’ll create it to have certain causal relationships and then we’ll see how the different models capture (and fail to capture) those relationships.\nI’ll start with the causal diagram that we’re going to use to simulate our data. It’s a bit complicated, but we’ll take it piece by piece as we move through the data simulation and modeling.\ngraph LR; D[Discrimination] –> E[Earnings] D –> O[Occupation] O –> E A[Ability] -.-> O A -.-> E\n\n\nimport numpy as np # for generating arrays with random numbers\nimport pandas as pd # dataframes\nimport statsmodels.api as sm # to run the actual ols model\n\nnp.random.seed(42) # to make it reproducible\n\nWe’re going to first generate a labor force where half of it is discriminated against (e.g. women being paid less, the well known gender gap in wages) and has ability randomly distributed. In the causal model sketched above both Discrimination and Ability are root causes - they’re not caused by anything else in the diagram. (Both obviously have causes outside of the system we’re currently considering). So that’s the place we’ll start.\n\ngenerated_data = {\n    'discrimination'  : np.random.randint(low = 0, high = 2, size = 10000, dtype = int), #the high argument is not inclusive, so this is randomly generating 0s and 1s. \n    'ability' : np.random.normal(size = 10000),\n}\n\ndf = pd.DataFrame(data = generated_data)\n\nNow we need to generate some other variables of interest. We’re looking at the impact of discrimination, so let’s set that to be experienced by half of the labor force. We’re going to assume that discrimination affects both wages and choice of occupation. Here we’re worried about occupations in terms of higher and lower pay scales, so let’s set occupations to be positively associated with ability and negatively associated with discrimination.\nFinally, wages are negatively associated with discrimination and positively associated with both occupation and ability.\n\ndf['occupation'] = 1 + 2 * df['ability'] - 2 * df['discrimination'] + np.random.normal(size = 10000)\ndf['wage'] = 1 - 1 * df['discrimination'] + 1 * df['occupation'] + 2 * df['ability'] + np.random.normal(size = 10000)\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      discrimination\n      ability\n      occupation\n      wage\n    \n  \n  \n    \n      count\n      10000.000000\n      10000.000000\n      10000.000000\n      10000.000000\n    \n    \n      mean\n      0.498700\n      -0.008041\n      -0.009388\n      0.471065\n    \n    \n      std\n      0.500023\n      1.004178\n      2.449597\n      4.545405\n    \n    \n      min\n      0.000000\n      -3.922400\n      -10.018905\n      -18.328506\n    \n    \n      25%\n      0.000000\n      -0.674327\n      -1.640437\n      -2.517222\n    \n    \n      50%\n      0.000000\n      -0.007682\n      -0.022777\n      0.482132\n    \n    \n      75%\n      1.000000\n      0.668901\n      1.633467\n      3.501387\n    \n    \n      max\n      1.000000\n      3.529055\n      9.500154\n      16.731628\n    \n  \n\n\n\n\nNow that we have our simulated data with specified causal relationships, let’s look at a few different regression models. We’ll first look at a model that only includes discrimination as a cause of wages.\n\n# Set up matrices for regression\nY = df['wage']\nX1 = df['discrimination']\n\n# add constant for the intercept of the model\nX1 = sm.add_constant(X1)\n\n# specify the model\nmodel1 = sm.OLS(Y, X1)\n\n# fit the model\nresults1 = model1.fit()\n\n# Look at results\n# results1.summary()\nresults1.params\n\nconst             1.952182\ndiscrimination   -2.969956\ndtype: float64\n\n\nWhat we’re mainly interested in is the coefficient on discrimination. Here we see that being discriminated against has a strong negative impact on wages earned. (Don’t worry about the const (constant) term, it’s not important in this example).\nThis isn’t a surprise based on how we set up the data. It also correctly reflects that in the real world if you just divide wages by gender you will find a large gender gap.\nThe dispute comes in when we talk about controlling for occupation, or a model that looks like this:\ngraph LR; D[Discrimination] –> E[Earnings] D –> O[Occupation] O –> E\n\nIn this model it looks like being discriminated against might raise wages slightly. We know that’s not right since we know we set up the data to have discrimination decrease earnungs. The problem is that when we added occupation to the model we opened up a brand new causal pathway from discrimination to earnings. It’s the one that runs from Discrimination–>Occupation–>Ability–>Earnings in our original causal model.\nWhen we controlled for occupation we did two things:\n\nIgnored the fact that occupational choice is also a result of discrimination and as a defense of pay discrimination it would then be the mechanism by which discrimination happens, not a defense that discrimination isn’t happening.\nOpened up a causal pathway that made our estimates worse.\n\n\n\nX3 = df[['discrimination', 'occupation', 'ability']]\nX3 = sm.add_constant(X3)\nmodel3 = sm.OLS(Y, X3)\nresults3 = model3.fit()\nresults3.params\n\nconst             0.988717\ndiscrimination   -0.986841\noccupation        1.025762\nability           1.975298\ndtype: float64\n\n\nA major problem is that in the real world we can’t observe ability directly and put it in a regression model. Another issue is that this causal model is still very incomplete. Nonetheless, the way the sign flips back and forth depending on the model is hopefully an illustration of why it’s so important to have a theoretical model and not just throw in as much data as possible.\nData is a powerful way to tell stories, but data by itself never tells us everything we need to know. We have to interpret it carefully and think hard about the underlying models of the world we’re bringing to the data when we interpret it.\nTwo things to remember from this post:\n\nThink about the causal model before doing statistics or machine learning\nDon’t believe companies that say the gender gap goes away if you control for other things. That’s only true if you believe the causal model underlying their analysis - and you probably shouldn’t."
  },
  {
    "objectID": "posts/partitioned_regression/index.html",
    "href": "posts/partitioned_regression/index.html",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "",
    "text": "Partitioned regression is a helpful way to gain more intuition for how regression works. What does it mean when we say that regression allows us to adjust for (or control for) other variables? By looking at a regression in multiple pieces we can gain a better understanding of what’s happening and also produce a pretty cool visualization of our key result. (Partitioned regression could also come in handy if you ever have to run a regression on a computer with limited RAM, but that’s not our focus here)."
  },
  {
    "objectID": "posts/partitioned_regression/index.html#getting-started",
    "href": "posts/partitioned_regression/index.html#getting-started",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "Getting Started",
    "text": "Getting Started\nLike most Python projects, we’ll start by loading some libraries.\n\nNumpy is standard for numerical arrays, and Pandas for dataframes and dataframe manipulation.\nAltair is a visualization library. It’s not as well known as matplotlib or Plotly, but I like the aesthetics of the plots it produces and I find it’s grammar of graphics a bit more intuitive.\npalmerpenguins is an easy way to load the demonstration data set I’ll be using here. You could also download it as a .csv file from here\nI’m using scikit-learn (sklearn) for the regression because it’s a useful package to learn for additional work in Python. Statsmodels is another choice that would have worked well for everything in this post.\n\n\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nfrom palmerpenguins import load_penguins\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\n\nLoading the penguins data and showing the first few rows\n\ndf = load_penguins()\n\n# if you download the .csv instead of using the library\n# df = pd.read_csv(\"palmer_penguins.csv\")\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      2007\n    \n  \n\n\n\n\n\n\n\nPenguins illustrated by Allison Horst\n\n\nThe penguins dataset is a small dataset that’s useful for doing demonstrations for everyone who is tired of the iris dataset and thinks penguins are cute."
  },
  {
    "objectID": "posts/partitioned_regression/index.html#preparing-the-data",
    "href": "posts/partitioned_regression/index.html#preparing-the-data",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "Preparing the data",
    "text": "Preparing the data\nBefore we can run any regressions we need to clean up the data a bit. There’s a row that is NA that we can drop and we have some categorical variables that can’t be used directly. We’ll remove the NA data and transform the categorical variables into a series of dichotomous variables that have the same information but can be used in our analysis.\n\ndf = df.dropna().reset_index() # if you don't reset the index then merging on index later will result in data mismatches and destroy your data silently. \nenc = preprocessing.OneHotEncoder(sparse_output = False) # important to have sparse_output = False for the array to be easily put back into a dataframe afterwards\nencoded_results = enc.fit_transform(df[['sex', 'species']])\n\n# names are not automatically preserved in this process, so if you want feature names you need to bring them back out. \ndf2 = pd.DataFrame(encoded_results, columns = enc.get_feature_names_out())\n\n# putting the dichotomous variables in along with everything else\n# this still has the original categorial versions, so check that everything lines up correctly\ndf = pd.concat([df, df2], axis = 1)\n\n#instead of using scikit-learns preprocessing features you could do this manually with np.where\n#df['male'] = np.where(df['sex'] == 'male', 1, 0)"
  },
  {
    "objectID": "posts/partitioned_regression/index.html#partitioned-regression",
    "href": "posts/partitioned_regression/index.html#partitioned-regression",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "Partitioned regression",
    "text": "Partitioned regression\nLet’s say we’re interested in the relationship between bill depth and bill length. Bill length will be our dependent (or target) variable. We think there are things other than bill depth that are related to bill length, so we want to adjust for those when considering the relationship between depth and length. I’m going to put all of those other variables into a matrix called X. (For the dichotomous variables one category has to be left out).\nThen we’re going to run three different regressions. First we’ll regress bill length on all the X variables. Then we’ll also regress bill depth on all of the X variables. Finally, we’ll regress the residuals from the first regression on the residuals from the second regression. The residuals represent what is left unexplained about the dependent variable after accounting for the control variables. And by regressing both bill length and bill depth on the same set of control variables, we get residuals that can be thought of as bill length and bill depth after adjusting for everything in X. That lets us see the relationship between bill length and bill depth after accounting for anything else we think is relevant.\n\ny = df['bill_length_mm'] # target variable\nz = df['bill_depth_mm'] # effect we're interested in\nX = df[['flipper_length_mm', 'body_mass_g', 'sex_female', 'species_Adelie', 'species_Chinstrap']] # other variables we want to adjust for\n\n\nmodel1 = LinearRegression().fit(X, y)\n#residuals aren't actually saved by scikit-learn, but we can create them from the original data and the predictions\nresiduals_y_on_X = (y - model1.predict(X))\n\nmodel2 = LinearRegression().fit(X, z)\nresiduals_z_on_X = (z - model2.predict(X))\n\n#need to reshape for scikit learn to work with a single feature input\nz_resids = residuals_z_on_X.to_numpy().reshape(-1, 1)\ny_resids = residuals_y_on_X.to_numpy().reshape(-1, 1)\n\npart_reg_outcome = LinearRegression().fit(z_resids, y_resids)\n\n#has to be np.round, not round. And has to be [0, 0] not [0] for a 1d array\nprint(\"The regression coefficient using partitioned regression is {}\".format(np.round((part_reg_outcome.coef_[0, 0]), 3)))\n\nThe regression coefficient using partitioned regression is 0.313\n\n\nWe can also verify that we’d get the same result from an ordinary linear regression\n\n#add the bill depth variable back into the X array\nX2 = df[['bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex_female', 'species_Adelie', 'species_Chinstrap']]\n\n#here we can just use [0] for some reasons\nlr_outcome = LinearRegression().fit(X2, y)\nprint(\"The regression coefficient using linear regression is {}\".format(np.round(lr_outcome.coef_[0], 3)))\n\nThe regression coefficient using linear regression is 0.313\n\n\nOne advantage of the partitioned regression is that it allows us to look at the relationship visually. Instead of just having the point estimate, standard error, and any test statistics (e.g. p-value) we can visually inspect a full scatterplot of the data. I’ve added a regression line, and the slope of the line is equal to the regression coefficients found above. You can visually see from this plot that it isn’t a very strong relationship.\n\nplt_df = pd.DataFrame(data = {'Adjusted Bill Length': residuals_y_on_X, 'Adjusted Bill Depth': residuals_z_on_X})\n\nsp = alt.Chart(plt_df, title = \"Bill Depth and Bill Length (Adjusted)\").mark_circle().encode(\n    alt.X('Adjusted Bill Depth', scale = alt.Scale(zero = False)),\n    alt.Y('Adjusted Bill Length', scale = alt.Scale(zero = False)),\n)\n\nsp + sp.transform_regression('Adjusted Bill Depth', 'Adjusted Bill Length').mark_line()\n\n\n\n\n\n\nA disadvantage of using scikit learn is that it doesn’t give us traditional regression statistics. The easiest way to get those is through statsmodels, which shows the expected 0.313 coefficent and tells us the standard error is 0.154 with a p-value of .043. This tells us it is actually a statistically significant relationship, so without the visual evidence from the scatterplot above, we might assume it’s a stronger relationship than it actually is. It is unlikely to have occured purely by chance, but that doesn’t mean it’s necessarily tightly correlated or has a large effect size.\n\nimport statsmodels.api as sm\n\n#unlike scikit learn, statsmodels does not add a constant for you unless you specify that you want one. \nX2 = sm.add_constant(X2)\nest = sm.OLS(y, X2).fit()\nest.summary2()\n\n\n\n\n        Model:               OLS         Adj. R-squared:     0.836  \n\n\n  Dependent Variable:  bill_length_mm         AIC:         1482.2547\n\n\n         Date:        2023-03-18 11:51        BIC:         1508.9117\n\n\n   No. Observations:         333         Log-Likelihood:    -734.13 \n\n\n       Df Model:              6           F-statistic:       282.3  \n\n\n     Df Residuals:           326       Prob (F-statistic): 7.50e-126\n\n\n      R-squared:            0.839            Scale:         4.9162  \n\n\n\n\n                     Coef.  Std.Err.    t     P>|t| [0.025  0.975] \n\n\n  const             23.4506  4.8836  4.8019  0.0000 13.8433 33.0579\n\n\n  bill_depth_mm     0.3130   0.1541  2.0316  0.0430 0.0099  0.6160 \n\n\n  flipper_length_mm 0.0686   0.0232  2.9608  0.0033 0.0230  0.1141 \n\n\n  body_mass_g       0.0011   0.0004  2.5617  0.0109 0.0003  0.0019 \n\n\n  sex_female        -2.0297  0.3892  -5.2153 0.0000 -2.7953 -1.2641\n\n\n  species_Adelie    -6.4044  1.0304  -6.2154 0.0000 -8.4314 -4.3773\n\n\n  species_Chinstrap 3.1612   0.9927  3.1844  0.0016 1.2082  5.1141 \n\n\n\n\n     Omnibus:    32.842  Durbin-Watson:    2.068\n\n\n  Prob(Omnibus):  0.000 Jarque-Bera (JB): 90.331\n\n\n       Skew:      0.430     Prob(JB):      0.000\n\n\n     Kurtosis:    5.402  Condition No.:   173513\n\n\n\n\nFinally, we might interested in how different this picture is from the unadjusted relationship between bill length and depth if we had not taken into account other variables\n\nsp = alt.Chart(df, title = \"Bill Depth and Bill Length (Unadjusted)\").mark_circle().encode(\n    alt.X('bill_depth_mm', title = 'Bill Depth', scale = alt.Scale(zero = False)),\n    alt.Y('bill_length_mm', title = 'Bill Length', scale = alt.Scale(zero = False)),\n)\n\nsp + sp.transform_regression('bill_depth_mm', 'bill_length_mm').mark_line()\n\n\n\n\n\n\nThis difference and sign reversal is mostly because of the relationships between species, bill length, and bill depth. But that’s a subject for a post about Simpson’s paradox."
  }
]