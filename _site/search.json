[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a data scientist with an interest in public policy, statistics, and coding. This blog is mostly to help me organize my thoughts, although I may sometimes also use it to share policy analysis.\nI’ve also included a handful of posts about policy, economics, and religion from an old and deprecated blog that I used to run."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nate Kratzer",
    "section": "",
    "text": "Partitioned Regression with Palmer Penguins and Scikit-Learn\n\n\n\n\n\n\n\nPython\n\n\nPenguins\n\n\nStatistics\n\n\n\n\nUsing partitioned regression to gain a better understanding of how linear regression works.\n\n\n\n\n\n\nMar 18, 2023\n\n\n\n\n\n\n  \n\n\n\n\nThink before adding more variables to that analysis\n\n\n\n\n\n\n\nPython\n\n\nCausal Inference\n\n\nPolicy\n\n\nStatistics\n\n\n\n\nAn introduction to thinking about causal models for data analysis. The purpose is to demonstrate that the popular approach of simply gathering as much data as you can and controlling for it via regression or other methods is not a good one, and is actively misleading in many cases. We should instead carefully think about plausible causal models using tools like diagrams (directed acyclic graphs, or DAGs) and then do data analysis in accordance with those models.\n\n\n\n\n\n\nMar 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRacing Louisville in their first NWSL season\n\n\n\n\n\n\n\nPython\n\n\nSports\n\n\nData Viz\n\n\n\n\nA quick look back at how Racing Louisville played in their first season in the NWSL\n\n\n\n\n\n\nMar 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHow to Use Census Microdata to Analyze High Speed Internet in Kentucky\n\n\n\n\n\n\n\nR\n\n\nCensus\n\n\nInternet\n\n\nPolicy\n\n\n\n\nThis post is a start to finish descriptive analysis of high speed internet access in Kentucky, including tables, graphs, and maps. All of the detail of cleaning the data and iterating while exploring the data is included. This makes for a rather lengthy post, but it also makes it relatively unique in including all of those steps. We go through five attempts at making a table of high speed internet before finally getting it right! There’s quite a bit of cleaning work and then also a detour into calculating standard errors via bootstrap so we can correctly display uncertainty in our visuals.\n\n\n\n\n\n\nSep 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\nMeritocracy is Unjust\n\n\n\n\n\n\n\nPhilosophy\n\n\nEconomics\n\n\n\n\nMeritocracy tends to confuse a very practical sense of merit with a more abstract and moral one. An individual may deserve a high-paying job or admission to a selective college because they are productive or qualified. However, in a moral sense, individuals do not merit the skills and abilities they are born with, nor do they merit the environments they were born into that allowed them to develop those skills.\n\n\n\n\n\n\nOct 25, 2016\n\n\n\n\n\n\n  \n\n\n\n\nA Theory of Moral Sentiments\n\n\n\n\n\n\n\nEconomics\n\n\nPhilosophy\n\n\n\n\nReflections on Adam Smith’s Theory of Moral Sentiments and economics education today\n\n\n\n\n\n\nSep 3, 2013\n\n\n\n\n\n\n  \n\n\n\n\nMarket Norms are Crowding out Social Norms\n\n\n\n\n\n\n\nPhilosophy\n\n\nEconomics\n\n\n\n\nReflecting on how market norms are squeezing out social norms to the detriment of society\n\n\n\n\n\n\nJul 13, 2013\n\n\n\n\n\n\n  \n\n\n\n\nThe Bible and Public Policy\n\n\n\n\n\n\n\nReligion\n\n\n\n\nAn overview of what the Bible has to say about public policy\n\n\n\n\n\n\nDec 20, 2012\n\n\n\n\n\n\n  \n\n\n\n\nThe Moral Limits of Markets\n\n\n\n\n\n\n\nPhilosophy\n\n\nEconomics\n\n\n\n\nA review of Michael Sandel’s book ‘What Money Can’t Buy: The Moral Limits of Markets’\n\n\n\n\n\n\nMay 26, 2012\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html",
    "href": "posts/bible_and_public_policy/index.html",
    "title": "The Bible and Public Policy",
    "section": "",
    "text": "I am frequently asked what Jesus has to do with public policy. It is a difficult question to answer, because Jesus didn’t live in a participatory democracy in which he could clearly let us know his stand on the political controversies of the day. He lived in an occupied land where ultimate political power rested with the Roman empire and the proximate political power was the Sanhedrin (the Jewish court) that ruled over Israel. Since the Sanhedrin was a religious body, this meant that Jesus, like all Jews before him lived in a theocracy.\nNeedless to say, it is a little difficult to figure out how to apply the actions of an oppressed powerless Jew living two thousand years ago in a conquered theocracy to today’s politics. But there are a few things we can do to bridge that gap.\nFirst, we need to remember that politics is simply the way in which we choose to order our cities. In a democracy we tend to think of partisan politics, but the actual word comes from the Greek ‘polis’ which simply meant the city-state. Politics is how we organize as a community, whether it is a democracy, theocracy, autocracy, or kingdom it is still a political order.\nSecond, we need to remember that Jesus’ life and death did not happen in isolation. We have the entire rest of the Bible to give context to Jesus’s life and ministry. And since this is a broad and important question, I’m going to start at the very beginning."
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html#from-genesis-through-the-kings",
    "href": "posts/bible_and_public_policy/index.html#from-genesis-through-the-kings",
    "title": "The Bible and Public Policy",
    "section": "From Genesis through the Kings",
    "text": "From Genesis through the Kings\nIn the beginning, God created the world out of love and God saw that the world was good and the world was God’s. (Gen 1:31) Then came the fall. Human beings gave into temptation to be God-like and were banished from paradise (Gen 3). This dual nature of human beings as being both good and made in the image of God (Gen 1:27) as well as fallen is a basis for the a theological anthropology that holds that each human being is at once both saint and sinner.\nThe stories of the patriarchs confirm both the saint and the sinner in humanity and end with the Hebrews being enslaved in Egypt as a result of Joseph’s use of the famine to put all of the land under the control of Pharaoh. (Gen 48:20-27)\nBy bringing the Hebrew people out of Egypt God is revealed as a God who liberates God’s people from oppression and leads them out of slavery into the promised land (Exodus 20:2, Deuteronomy 7:8 and others). Once there God gives them laws to live by and we get our first glimpse of what a society ordered by God looks like. There is no King, because God is their king (1 Samuel 12:12). The laws cover religion, politics, and economics. The economics laws in particular reveal a remarkable egalitarianism:\n\nDuring the harvest, anything that falls to the ground is to be left for the poor to come and gather (Deuteronomy 24:19-22).\nEvery seven years debts are to be forgiven(Deuteronomy 15:1).\nSlaves are to be freed every seven years (Exodus 21:2-4, Deuteronomy 15:12).\nEvery fifty years land is to be restored to its original owners (Leviticus 25: 8-28).\nThere is to be no oppression of strangers living among them (Exodus 22:21, Exodus 23:9)\nDo not oppress the widow or the orphan (Exodus 22: 22-24).\nThe terms of lending are not to include interest payments or take as collateral the borrowers means of production and shelter (Exodus 22:25-27, Deuteronomy 24:6).\n\nEventually the people grow restless and desire a king. God tries to talk them out of it (1 Samuel 8:10-18, 1 Samuel 12:11-25, Deuteronomy 17:14-20) but relents and anoints Saul to be their king (1 Samuel 15:1) and then regrets appointing Saul a mere ten verses later (1 Samuel 15:10-11). Skipping over a wonderful succession narrative we then wind up with King David and the united Kingdom of Israel. King David reigned about 1,000 years before Jesus’ birth and from a political and economic perspective it was clearly the golden age of Israel. However, it all fell apart under King Solomon due to excessive taxation and the fact that the kingdom was only tentatively united to begin with. Israel split into a Northern Kingdom (Israel) and a Southern Kingdom (Judah). In 722 BCE the Northern Kingdom was conquered by the Assyrians and ceased to exist."
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html#the-prophets",
    "href": "posts/bible_and_public_policy/index.html#the-prophets",
    "title": "The Bible and Public Policy",
    "section": "The Prophets",
    "text": "The Prophets\nIt is during the 8th Century BCE that the tradition of prophetic criticism of unjust government begins to emerge (Nathan had criticized King David for personal immorality regarding Bethsheba, but this is the beginning of a more general critique of justice).\nAmos and Hosea prophesy in the Northern Kingdom and Isaiah and Micah in the south. Isaiah 1:13-17 sums it up well:\n\n…bringing offerings is futile; incense is an abomination to me….I cannot endure solemn assemblies with iniquity…When you stretch out your hands, I will hide my eyes from you; even though you make many prayers I will not listen; your hands are full of blood…cease to do evil, learn to do good, seek justice, rescue the oppressed, defend the orphan, plead for the widow.\n\nBut Isaiah 1:13-17 is hardly alone. It is impossible to do justice to all the 8th century prophets had to say about justice, but here is a (by no means complete) collection of verses broken down by subject*:\n\nConcentrated wealth (Isaiah 5:7-8, Amos 6:4-8, Micah 6:10-16)\nOppression/Exploitation of the poor (Isaiah 3:13-15, Isaiah 32:1-8, Hosea 12:7-8, Amos 2:6-8, Amos 4:1, Amos 5:11-12, 15, Amos 8:4-6, Micah 2:1-9, Micah 3:1-8, Micah 7:3-4)\nPeace (Isaiah 2:4, Hosea 10:13, Amos 3:10, Micah 4:3)\nLiberation (Isaiah 14:3-5, Isaiah 31:8-9, Isaiah 35:5-7, Amos 9:7)\nJustice (Isaiah 11:1-9, Isaiah 16: 3-5, Amos 5:23-24, Micah 6:8)\n\nAround or slightly after the time at which this critical prophetic tradition was developing there was also a new line of theological thought that is given its fullest expression in the books of Job and Ecclesiastes. The wisdom sages realized that sometimes bad things happen to good people and started asking questions we still ask today. This realization that being either poor or wealthy is not a direct manifestation of God’s will put the plight of the poor in a new light.\nIn the 6th century BCE (600 years before Christ) the Southern Kingdom was defeated by Babylon and sent into exile. This lead to a new series of prophets that were concerned with the theological implications of their defeat and removal from their sacred land. (Jeremiah’s idea of God writing a covenant on the heart of each Israelite probably owes something to the exile from sacred land (Jeremiah 31:31-34)). However, these prophets and those who oversaw the return from exile in the later half of the century were also concerned with just governance.\nThe most notable new development might be Jeremiah saying that to know the Lord is to do justice to the poor and needy (Jeremiah 22:16). Again, this is a rich period that deserves more attention than I can give it in this brief overview, but here are some notable verses again broken down by subject:\n\nOppression/Exploitation of the poor (Ezekiel 35:17-20, Isaiah 65:19-25)\nPeace (Jeremiah 29:7)\nLiberation (Isaiah 65:19-25, Isaiah 61:1-2, 8, Ezekiel 37:14, Isaiah 41:17)\nJustice (Jeremiah 7:5-6, Jeremiah 22:13-16, Isaiah chapters 58 and 59)\n\nAfter the Persians defeated the Babylonians the exile was ended, but Israel was now under Persian rule. When Ezra and Nehemiah rebuilt after the Exile they followed not only the religious laws, but also the economic ones (Nehemiah 5). They failed to include the resident alien and forbid intermarriage, but were rebuked by the storytelling of books like Jonah and Ruth, stories that cast aliens as heroes and even as the ancestor of King David.\nIn the 4th Century BCE Israel was conquered by the Macedonian empire of Alexander the Great, but as the Macedonian Empire declined Israel managed to seize about a century of freedom (the revolt of Judas Maccabeus and the Hasmonean dynasty) before being conquered by the Romans in the 1st century BCE."
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html#jesus",
    "href": "posts/bible_and_public_policy/index.html#jesus",
    "title": "The Bible and Public Policy",
    "section": "Jesus",
    "text": "Jesus\nIt is against all of this background that we can know begin to explore the political implications of Jesus’ ministry. The most common (though not the only) messianic notion of the time period was that the Messiah would be sent by God to restore the Davidic Kingdom, the golden age of Israel. Jesus, however, was clearly a nonviolent messiah (Matthew 26:52, Matthew 5:38-42, Luke 9:54) who did not call forth God’s wrath on Rome in order to free Israel from oppression. Instead, Jesus proclaimed the coming of the Kingdom of God.\nJesus is consistently engaged in debate with the pharisees (religious leaders) of his day over questions of the law and the prophets and their meaning for the present day. To speak in broad terms, within these debates Jesus tended to argue that compassion is more important than ritual purity. The summation of this thesis is found in Matthew 23:23 (cf. Luke 11:42) “For you tithe mint, dill, and cumin, but have neglected the weightier matters of the law: justice and mercy and faith.” Jesus sums up the law and prophets saying that they all hang on love of God and love of neighbor (Matthew 22:37-40, Luke 10:25-28). Jesus parable of the good Samaritan (Luke 10:29-37) shows that loving ones neighbor is even more important than preserving one’s ritual purity (the priest and the levite passed by in part because contact with a dead body would make them unclean). Some of Jesus’ other relevant teachings include***:\n\nTelling a young man that to be perfect he must sell all he has, give the money to the poor, and follow Jesus (Matthew 19:21, Mark 20-22, Luke 18:18-25)\nLove your enemies (Matthew 5:38-48, Luke 6:27-36)\nForgive debts (Matthew 6:12, 18:21-35, Luke 16:1-13)\nBear good fruit (Matthew 3:8-10, 7:16-20, 12:33-35, 21:43, Luke 6:43-45)\nBlessed are the poor, hungry, mourning, meek, merciful, pure in heart, peacemakers, and persecuted (Matthew 5:1-12, Luke 6:21-26)\nTreat others as you would like to be treated (Matthew 7:12)\nCare for sinners and spend time among them (Matthew 9:10-13, Luke 7:40-50, 19:5-10)\nThe signs of God’s kingdom are that the lame walk, the hungry are feed, and the blind see. (Matthew 11:4-6, Luke 4:18-19, 7:22-23)\nThe Sabbath is made for human beings; compassion and human need are more important. (Matthew 12:1-8, Mark 2:23-28, 3:1-5, Luke 13:12-15, 14:3-6)\nIt is what comes out of your mouth, not what goes in, that matters (Matthew 15:16-20, Mark 7:14-23)\nSupporting your elderly parents monetarily is more important than temple offerings (Matthew 15:3-9, Mark 7:6-12)\nForbidding divorce in a culture where divorce left women economically desolate (Matthew 5:31, Matthew 19:9, Luke 16:18)\nTax collectors and prostitutes are more righteous than religious leaders (Matthew 21:31)\nI came as a servant, not a master (Mark 10:45)\nThe powerful are brought down and the hungry are fed (Luke 1:52-53)\nShare your coats and food with those who have none (Luke 3:11)\nAbundant possessions will not help you (Luke 12:13-21)\n“None of you can become my disciple if you do not give up all your possessions” (Luke 14:33)\nZacchaeus is praised for giving half of his possessions to the poor (Luke 19:8-9)\nCondemning the scribes because “they devour widows’ houses” (Luke 20:47)\n\nJesus’ life also witnessed to compassion with numerous healings and several instances of feeding the multitudes. It is the driving out of the money changersfrom the temple that leads to Jesus’ arrest (Matthew 21:12-13, Mark 11:17-18, Luke 19:45-48). Jesus has now threatened the monetary base of the religious and political authority and is handed over to the Romans to be put to death as a political prisoner. The theology of Christ’s death is a source of dispute but all theories I am aware of stress that it is an act of compassion and a means of reconciling God’s people back to God (Colossians 1:20, 2 Corinthians 5: 17-19)."
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html#the-early-christian-community-and-the-kingdom-of-god",
    "href": "posts/bible_and_public_policy/index.html#the-early-christian-community-and-the-kingdom-of-god",
    "title": "The Bible and Public Policy",
    "section": "The early Christian Community and the Kingdom of God",
    "text": "The early Christian Community and the Kingdom of God\nFinally, we must consider the practices of the early Christian community as we attempt to form a coherent vision of the Kingdom of God as preached by Jesus. The early followers of Christ experienced Pentecost and then lived together holding all of their possessions in common, selling whatever they had and distributing the proceeds to anyone who had need (Acts 2:43-47). None of them had any private property (Acts 4:32) and when Ananias tries to join but holds back property from the group he falls down and dies (Acts 5:1-6). These early followers of Christ were not yet known as Christians, but were called followers of the way (Acts 9:2). Though in later years much of the emphasis of Christians would shift from following Jesus’ teachings by living in a certain way to a system more focused on beliefs about Jesus, in the early years Jesus’ followers sought to live in a certain way, the way that Jesus taught.\nIt is in exploring the way of life that Jesus taught that we finally find what that way of life has to do with our current public policy. The Kingdom of God as proclaimed by Jesus is one that is:\n\nalready present (Luke 7:22).\ncollaborative (Luke 17:20-21).\nnonviolent (Matthew 26:52).\n\nThe Kingdom of God is already present inasmuch as individuals are willing to collaborate with it, to enter into it and to live it out. It is also nonviolent, it does not overthrow power with violence but instead seeks to subvert that power (Matthew 5:38-41). The way of Jesus is to enter into this Kingdom that is typified by the teachings of Jesus, and the prophets, and the law. A kingdom is, of course, a form of political order. The Kingdom of God is one in which God is king, and clearly God is a very different kind of king than King David or any of the other kings of earth with whom we are familiar. Based on the overview given above there are a few characteristics that stand out to me:\n\nAn overwhelming concern with and compassion for the poor and vulnerable.\nA rejection of physical wealth and power as status symbols or a way to be secure.\nA desire for proper relationships with God and among human beings.\nPeace that comes through justice, not violence.\n\nThe kingdom of God is collaborative, which means that we can’t do it without God. But it also means that God is waiting for us to accept and enter into the Kingdom of God by living it out in our churches and communities. No political order should ever be identified with the Kingdom, because all human beings are both saints and sinners. However, all political orders should be judged against the Kingdom as a standard, an ‘impossible but relevant ideal’ for which Christians are called to strive. This, then, is what Jesus has to do with public policy. Jesus invites us to enter the Kingdom, and by striving towards the Kingdom we are irrevocably committed to striving towards an impossible ideal in our community life and our social and political order."
  },
  {
    "objectID": "posts/bible_and_public_policy/index.html#policy-in-a-nation-with-multiple-religious-views",
    "href": "posts/bible_and_public_policy/index.html#policy-in-a-nation-with-multiple-religious-views",
    "title": "The Bible and Public Policy",
    "section": "Policy in a nation with multiple religious views",
    "text": "Policy in a nation with multiple religious views\nOne of the frequent objections made to using the Bible as a source of public policy is that the United States is not a Christian nation and should not take its policy guidance from sacred scripture. This is an entirely correct reply to an argument that I have never made. The Bible is supposed to guide Christian views of public policy, but not to guide the U.S. government’s view of public policy.\nReligious pluralism does not require Christians (or any other religious groups) to ignore the public policy implications of their theology and scripture, it simply requires that if they wish to be persuasive in public debate they use arguments that will resonate with their audience. If I were speaking to a large group about immigration reform I would talk about the economic benefits to immigration as well as a fairness argument arising from the lottery of birthplace. If I were speaking to a Christian congregation about immigration, I would start with theology (and also probably mention economics).\nThe Bible really does have a lot to say about public policy, but no one who doesn’t consider the Bible to be their sacred scripture needs to listen.\n\nFootnotes\n*In reading the prophets it is important to know that ‘the gate’ was the cite of legal proceedings. When the prophets call for justice in the gate or condemn injustice at the gate they are taking about the judicial system. Also, there is naturally quite a bit of overlap among the categories.\n**The book of Isaiah is generally accepted by scholars as the work of three different prophets within the school of thought of Isaiah, the 8th century prophet. The other two are unnamed and prophesied during the exile and immediately after the exile (early and late 6th century BCE).\n*** I have included teachings about Jesus that were said by Mary and John the Baptist"
  },
  {
    "objectID": "posts/census_internet_microdata/index.html",
    "href": "posts/census_internet_microdata/index.html",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "",
    "text": "This post is a start to finish descriptive analysis of high speed internet access in Kentucky, including tables, graphs, and maps. All of the detail of cleaning the data and iterating while exploring the data is included. This makes for a rather lengthy post, but it also makes it relatively unique in including all of those steps. We go through five attempts at making a table of high speed internet before finally getting it right! There’s quite a bit of cleaning work and then also a detour into calculating standard errors via bootstrap so we can correctly display uncertainty in our visuals.\nCensus microdata is individual level data provided by the census. Most references to census data refer to tables the census bureau has already made out of their surveys, but the mostly raw survey data is available at the individual level, and that’s what we’ll use for this analysis. While the focus is on the census data, I do show the code used for analysis. I did decide to hide the code used to make all the tables (it gets rather lengthy), but you can see that code on Github if interested."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#getting-wrong-answers-by-not-knowing-the-data",
    "href": "posts/census_internet_microdata/index.html#getting-wrong-answers-by-not-knowing-the-data",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Getting wrong answers by not knowing the data",
    "text": "Getting wrong answers by not knowing the data\nNow that we have a high speed internet category we can group the data and count up how many responses are in each group. I’ll also pivot the dataframe to make it easy to calculate percent with high speed internet.\n\n# Count numbers with and without high speed internet\ndf_group <- df %>%\n  group_by(hspd_int, YEAR) %>%\n  summarize(count = n(), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = YEAR,\n              names_from = hspd_int,\n              values_from = count) %>%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  ) \n\n\n\n\n\n\n\n  \n    \n      Table 1: Quick Analysis\n    \n    \n      These results are wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n87%\n13%\n28%\n28.3K\n4.27K\n12.4K\n    2014\n86%\n14%\n27%\n28.1K\n4.70K\n12.1K\n    2015\n86%\n14%\n26%\n28.7K\n4.52K\n11.5K\n    2016\n82%\n18%\n20%\n29.2K\n6.49K\n9.02K\n    2017\n81%\n19%\n19%\n29.4K\n7.08K\n8.77K\n    2018\n80%\n20%\n17%\n30.3K\n7.35K\n7.86K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's incorrect analysis of IPUMS data. Used as an example of a mistake to avoid.\n    \n  \n  \n\n\n\n\nThis table is actually wrong for multiple reasons, but the first one we’ll take care of is the failure to use weights. Survey data is often weighted to make it representative of the population. The census bureau provides a PERWT variable that should be used as the weight for each person in the file. There’s also a HHWT variable for household level analysis. We’ll stick with weighting the data by the number of people. One of the really nice features of the PERWT variable is that it sums to the population. That means our tables can show both an overall number of people and the percentage of people.\n\n# Count numbers with and without high speed internet\ndf_group <- df %>%\n  group_by(hspd_int, YEAR) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = YEAR,\n              names_from = hspd_int,\n              values_from = count) %>%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  )\n\n\n\n\n\n\n\n  \n    \n      Table 2: Quick Analysis with Weights\n    \n    \n      These results are still wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n86%\n14%\n27%\n2.76M\n454K\n1.18M\n    2014\n84%\n16%\n26%\n2.74M\n509K\n1.17M\n    2015\n86%\n14%\n25%\n2.85M\n479K\n1.10M\n    2016\n80%\n20%\n19%\n2.87M\n708K\n855K\n    2017\n80%\n20%\n18%\n2.90M\n735K\n817K\n    2018\n79%\n21%\n16%\n2.97M\n790K\n709K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's incorrect analysis of IPUMS data. Used as an example of a mistake to avoid.\n    \n  \n  \n\n\n\n\nThis is better. The second problem is harder to spot. There are 3 hints in the data:\n\nThere is a very high percentage of NA responses. There are more NA answers than there are people who say they don’t have high speed access.\nPercent of of people with high speed access is going down over time, which isn’t what I’d expect to see. That doesn’t mean it’s wrong - sometimes the data shows high level trends we don’t expect. However, it’s always worth a second look when you get a counterintuitive result.\nThese numbers look very high for Kentucky.\n\nA sensible guess is that people who say they don’t have internet access at all aren’t then asked about high speed internet and show up as an NA value when we want to code them as not having high speed interent.\nSo let’s get to know the data a bit better by adding in internet access. We’ll do the same analysis, but I’ll add internet as another id variable just like year. We can see right away that the answers we have above are only including cases where individuals have internet.\n\ndf <- df %>%\n  mutate(\n    int = case_when(\n      CINETHH == 0 ~ NA_character_,\n      CINETHH == 1 | CINETHH == 2 ~ \"Yes\",\n      CINETHH == 3 ~ \"No\",\n      TRUE ~ NA_character_\n    )\n  )\n\ndf_group <- df %>%\n  group_by(hspd_int, int, YEAR) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(\n    id_cols = c(YEAR, int),\n    names_from = hspd_int,\n    values_from = count\n  ) %>%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  )\n\n\n\n\n\n\n\n  \n    \n      Table 3: Exploring Internet Data\n    \n    \n      These results are exploratory. But not wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    \n      Has Internet Access\n    \n    2013\n86%\n14%\n5%\n2.76M\n454K\n185K\n    2014\n84%\n16%\n6%\n2.74M\n509K\n193K\n    2015\n86%\n14%\n6%\n2.85M\n479K\n217K\n    2016\n80%\n20%\n3%\n2.87M\n708K\n129K\n    2017\n80%\n20%\n3%\n2.90M\n735K\n124K\n    2018\n79%\n21%\n3%\n2.97M\n790K\n125K\n    \n      No Internet Access\n    \n    2013\nNA\nNA\nNA\nNA\nNA\n866K\n    2014\nNA\nNA\nNA\nNA\nNA\n842K\n    2015\nNA\nNA\nNA\nNA\nNA\n753K\n    2016\nNA\nNA\nNA\nNA\nNA\n596K\n    2017\nNA\nNA\nNA\nNA\nNA\n560K\n    2018\nNA\nNA\nNA\nNA\nNA\n452K\n    \n      Internet Access is NA\n    \n    2013\nNA\nNA\nNA\nNA\nNA\n126K\n    2014\nNA\nNA\nNA\nNA\nNA\n130K\n    2015\nNA\nNA\nNA\nNA\nNA\n129K\n    2016\nNA\nNA\nNA\nNA\nNA\n131K\n    2017\nNA\nNA\nNA\nNA\nNA\n132K\n    2018\nNA\nNA\nNA\nNA\nNA\n132K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nWhen we split it out this way we can see that the groups without any internet access are all NA values. So what we were looking at was the percentage of people with internet who have high speed internet. What we want is the percentage of all people who have high speed internet. We can fix the way we create our categories by saying that anyone who has no internet also has no high speed internet.\n\ndf <- df %>%\n  mutate(\n    #create a categorical variable for having any internet\n    int = case_when(\n      CINETHH == 0 ~ NA_character_,\n      CINETHH == 1 | CINETHH == 2 ~ \"Yes\",\n      CINETHH == 3 ~ \"No\",\n      TRUE ~ NA_character_\n    ),\n    # change how high speed internet is defined so that houses w/o any interent are counted as 'No' instead of NA\n    hspd_int = case_when(\n      CIHISPEED == 00 & int != \"No\" ~ NA_character_,\n      CIHISPEED == 20 | int == \"No\" ~ \"No\",\n      CIHISPEED >= 10 & CIHISPEED < 20 ~ \"Yes\",\n      TRUE ~ NA_character_\n    )\n  )\n\n# Count numbers with and without high speed internet\ndf_group <- df %>%\n  group_by(hspd_int, YEAR) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(\n    id_cols = c(YEAR),\n    names_from = hspd_int,\n    values_from = count\n  ) %>%\n  mutate(\n    percent_hspd = (Yes / (Yes + No)),\n    percent_no = 1 - percent_hspd,\n    percent_NA = (`NA` / (Yes + No + `NA`))\n  )\n\n\n\n\n\n\n\n  \n    \n      Table 4: Almost right!\n    \n    \n      These results are still a little wrong!\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n68%\n32%\n7%\n2.76M\n1.32M\n311K\n    2014\n67%\n33%\n7%\n2.74M\n1.35M\n324K\n    2015\n70%\n30%\n8%\n2.85M\n1.23M\n346K\n    2016\n69%\n31%\n6%\n2.87M\n1.30M\n260K\n    2017\n69%\n31%\n6%\n2.90M\n1.29M\n256K\n    2018\n71%\n29%\n6%\n2.97M\n1.24M\n257K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered to represent the results an analyst might get if they ignored NA values. NA is calculated separately based on all the data.\n    \n    \n      Source: Author's incorrect analysis of IPUMS data. Used as an example of a mistake to avoid.\n    \n  \n  \n\n\n\n\nThese results look much better, although there’s still one more thing to do about those NA results."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#group-quarters-in-the-census",
    "href": "posts/census_internet_microdata/index.html#group-quarters-in-the-census",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Group Quarters in the Census",
    "text": "Group Quarters in the Census\nThe census data includes individuals living in group quarters (mostly prisons, senior living centers, and dorms, but includes any sort of communal living arrangement). However, all census questions about appliances and utilities (the category that internet access falls under) are NA for group quarters. So we’ll add one more line to filter out individuals living in group quarters (a common practice when working with Census microdata). The code below adds a filter for Group Quarters. Since this table is showing correct results I’ll also add a little additional formatting to make it stand out from the others.\nI’ll also note that the way the Census Bureau constructs weights is very convenient for getting totals. While I’m focusing on the percent of people who have internet access, the Yes and No columns are accurate estimates of the population with and without access.\n\n# Count numbers with and without high speed internet\ndf_group <- df %>%\n  filter(GQ == 1 | GQ ==2 | GQ == 5) %>%\n  group_by(hspd_int, YEAR) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = c(YEAR), names_from = hspd_int, values_from = count) %>%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd,\n         percent_NA = (`NA` / (Yes + No + `NA`)))\n\n\n\n\n\n\n\n  \n    \n      Table 5: High Speed Internet in Kentucky\n    \n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    2013\n68%\n32%\n4%\n2.76M\n1.32M\n185K\n    2014\n67%\n33%\n5%\n2.74M\n1.35M\n193K\n    2015\n70%\n30%\n5%\n2.85M\n1.23M\n217K\n    2016\n69%\n31%\n3%\n2.87M\n1.30M\n129K\n    2017\n69%\n31%\n3%\n2.90M\n1.29M\n124K\n    2018\n71%\n29%\n3%\n2.97M\n1.24M\n125K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nThat removed about half of our NA values. It might be nice to know a bit more about the missing data, but at around 3 percent of observations it’s unlikely to change our substantive conclusions. I suspect these are cases where there wasn’t an answer for that question. We’ll keep an eye on NA values as we do the analysis, because as we get into questions like how internet access varies by race, income, age, and education we’ll want to know if NA answers are more or less likely in any of those categories."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#checking-against-data.census.gov",
    "href": "posts/census_internet_microdata/index.html#checking-against-data.census.gov",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Checking against data.census.gov",
    "text": "Checking against data.census.gov\nTo do a quick check against the way the census bureau itself analyzes the data I looked at data.census.gov for 2018 in Kentucky. An important note is that their data is for households, and so their numeric counts look quite different because I’m counting number of people. They also have a breakdown where cellular is included in broadband, which I do not want, as a cell phone is not really an adequate work or study device. So to get to what I have we need to add “Broadband such as cable, fiber optic or DSL” and “Satellite Internet service”, which gets us to 70.8% compared to the 70.5% in this analysis. The difference is small and most likely the result of their analysis being weighted to the household level rather than the person level. (Internet is measured at the household level and the same for every person in the household, but by choosing to weight it at the person level I am a) letting us talk in terms of people, b) giving more weight to larger households, c) making it possible to break down internet access by categories that do vary within households, like age)."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#standard-errors",
    "href": "posts/census_internet_microdata/index.html#standard-errors",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Standard Errors",
    "text": "Standard Errors\nKnow that we know the data we’d also like to know how uncertain our sample is so that we know if movements over time are real or just a result of noisy data. There are a few ways to do this. The survey package does an excellent job with complex survey designs, but does require learning a new syntax to use. The alternative I’ll use here is a method known as bootstrap. IPUMS suggests using bootstrap might be the best way to get standard errors on census microdata. The basic idea of the bootstrap is to resample the existing data and use the sampling error from that as an estimate for sampling error in the overall population. Let’s do an example with high speed internet in 2018 to see how it works. The output here will be the mean and standard deviation for Kentucky. (We’ll use the standard error to calculate confidence intervals once we start displaying actual results.)\n\n#set seed\nset.seed(42)\n\n# Filter to just 2018\n# Exclude NA values\n# Recode as numeric vector of 1 and 0\n# The numeric 1 and 0 form will make it much easier to get means without pivoting, which matters a lot when doing this 1000 times\ndf2018 <- df %>%\n  filter(YEAR == 2018 & !is.na(hspd_int)) %>%\n  mutate(hspd_num = if_else(hspd_int == \"Yes\", 1, 0)) %>%\n  select(hspd_num, PERWT)\n\n# Write a function so I can map over it.\n# In this case, we need the function to do the same thing X number of times and assign an ID that we can use as a grouping variable\ncreate_samples <- function(sample_id){\n  df_out <- df2018[sample(nrow(df2018), nrow(df2018), replace = TRUE) , ] %>%\n    as_tibble()\n  df_out$sample_id <- sample_id\n  return(df_out)\n}\n\nnlist <- as.list(seq(1, 1000, by = 1))\nsamples <- purrr::map_df(nlist, create_samples)\n\nsample_summary <- samples %>%\n  group_by(sample_id) %>%\n  mutate(ind_weight = PERWT / sum(PERWT),\n         hspd_weight = hspd_num * ind_weight) %>% # PERWT is population and doesn't sum to 1. Rescale it to sum to one\n  summarize(group_mean = sum(hspd_weight),\n            weight_check = sum(ind_weight), .groups = \"drop\") # Check that my weights add up to one\n\ndisplay_tbl <- tibble(\n  mean = mean(sample_summary$group_mean),\n  sd = sd(sample_summary$group_mean)\n) \n\n\n\n\n\n\n\n  \n  \n    \n      mean\n      sd\n    \n  \n  \n    0.7053407\n0.002866898\n  \n  \n  \n\n\n\n\nWe can also take a look at our bootstrap graphically. We want to check that the distribution of the sample is roughly normal. If it’s not, that means we didn’t do enough bootstrap samples for the Central Limit Theorem to kick in.\n\n#Check that the distribution is normal and than the middle of the distribution is close to the 70.5% we estimated had internet access above\nggplot(sample_summary, aes(group_mean)) +\n  geom_density() + theme_bw() +\n  labs(title = \"Bootstrapped means of High Speed Internet Access\",\n       x = \"Mean\", \n       y = \"Kernel Density\")"
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#checking-our-results-against-the-survey-package",
    "href": "posts/census_internet_microdata/index.html#checking-our-results-against-the-survey-package",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Checking our results against the survey package",
    "text": "Checking our results against the survey package\nAbove we found a mean of 0.705 for 2018 and and standard error of 0.0029 based on our bootstrap analysis. It’s worth checking that this is the same result we’d get using an analytic approach (instead of bootstrap).\n\n# Here we're assuming a simple design. \n# Survey requires the creation of a design object and then has functions that work with that object.\n# You can get more complicated, which is when the survey package would be most useful.\nsvy_df <- svydesign(ids = ~ 1, weights = ~PERWT, data = df2018)\n\n# Taking the mean and standard error from our design object\nhint_tbl <- svymean(~hspd_num, design = svy_df)\n\nhint_tbl <- as_tibble(hint_tbl)\nnames(hint_tbl) <- c(\"mean\", \"sd\") #The names weren't coerced correctly when transforming into a tibble. \n\n\n\n\n\n\n\n  \n  \n    \n      mean\n      sd\n    \n  \n  \n    0.7051774\n0.00293509\n  \n  \n  \n\n\n\n\nThese results are very similar. Following the IPUMS recommendation we’ll continue on with the bootstrap, but it’s good to know the results are the same for practical purposes. So now instead of just doing 2018, we’ll need to do every year. We already know the mean values for every year, and they’re still saved in the df_wide variable right now. So let’s write a function for bootstrap that will let us find standard errors for every year or for any other grouping we choose."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#writing-a-bootstrap-function",
    "href": "posts/census_internet_microdata/index.html#writing-a-bootstrap-function",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Writing a bootstrap function",
    "text": "Writing a bootstrap function\n\n# Create a helper function\n# It needs to have a way to recieve the dataframe from the function that calls it, so we've added a second argument\ncreate_samples <- function(sample_id, df){\n  \n  df_out <- df[sample(nrow(df), nrow(df), replace = TRUE) , ] %>%\n    as_tibble()\n  \n  df_out$sample_id <- sample_id\n  \n  return(df_out)\n}\n\n# Need to be able to take in grouping variables so that the summaries can be specific to the groups\n# Using the embrace {{}} notation allows use to pass in unquoted variables to the function. \n\nbootstrap_pums <- function(df, num_samples, group_vars) {\n  \n  nlist <- as.list(seq(1, num_samples, by = 1))\n  samples <- purrr::map_df(nlist, create_samples, df)\n  \n  sample_summary <- samples %>%\n    group_by( sample_id, across( {{group_vars}} )) %>%\n    mutate(ind_weight = PERWT / sum(PERWT),\n           hspd_weight = hspd_n * ind_weight) %>% # PERWT sums to population instead of to 1. Rescale it to sum to 1.\n    summarize(group_mean = sum(hspd_weight), .groups = \"drop\") # Not dropping .groups here results in problems in the next group_by call.\n  \n  sample_sd <- sample_summary %>%\n    group_by( across( {{ group_vars }} )) %>%\n    summarize(sd = sd(group_mean), .groups = \"drop\")\n}\n\n# We do need to prep the data a little so that we're not carrying through the whole dataframe.\ndf_in <- df %>%\n   filter(!is.na(hspd_int)) %>%\n   mutate(hspd_n = if_else(hspd_int == \"Yes\", 1, 0)) %>%\n   select(hspd_n, PERWT, YEAR)\n\n# And finally we can call the function\nboot_results <- bootstrap_pums(df = df_in, num_samples = 500, group_vars = YEAR)\n\nNow that we have our bootstrap standard errors we can combine them with the data and plot them. We’ll use 95% confidence intervals, which we get by multiplying the standard error by 1.96 (the number of standard deviations that corresponds to a 95% confidence interval).\n\ndf_plt <- df_wide %>%\n  full_join(boot_results, by = \"YEAR\") %>%\n  transmute(Year = YEAR,\n            Percent = 100 * percent_hspd,\n            me = 100 * 1.96 * sd)\n  \nplt_int <- ggplot(df_plt, aes(x = Year, y = Percent)) +\n  geom_errorbar(aes(ymin = Percent - me, ymax = Percent + me), width = .1) +\n  geom_line() +\n  geom_point() +\n  theme_bw() +\n  labs(title = \"High Speed Internet Access\") +\n  theme(legend.position = \"bottom\")\n\nplt_int"
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#race-poverty-age-and-geography",
    "href": "posts/census_internet_microdata/index.html#race-poverty-age-and-geography",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "Race, Poverty, Age, and Geography",
    "text": "Race, Poverty, Age, and Geography\nGoing a little deeper we can use microdata to get results by custom groupings. I’ll show examples for race, poverty, age, and geography, but for any group you can construct with available Census data you can produce an estimate for their internet acess.\n\nRace\nNext we’ll build a table by race and year.\n\n# Let's build a table first and then we'll do the standard errors\n\n# Coding a race variable using case_when\ndf <- df %>%\n  mutate(race = case_when(\n            RACE == 1 ~ \"White\",\n            RACE == 2 ~ \"Black\",\n            RACE > 3 & RACE < 7 ~ \"Asian\",\n            HISPAN > 0 & HISPAN < 5 ~ \"Hispanic\",\n            TRUE ~ \"All Others\"\n          ))\n\ndf_group <- df %>%\n  group_by(hspd_int, race, YEAR) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = c(race, YEAR), names_from = hspd_int, values_from = count) %>%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd,\n         percent_NA = (`NA` / (Yes + No + `NA`)))\n\n\n\n\n\n\n\n  \n    \n      Table 6: High Speed Internet Access\n    \n    \n      By Race and Ethnicity\n    \n  \n  \n    \n      Year\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    \n      Asian\n    \n    2013\n84%\n16%\n4%\n42.2K\n8.03K\n2.06K\n    2014\n76%\n24%\n2%\n41.6K\n12.9K\n1.19K\n    2015\n83%\n17%\n3%\n48.8K\n10.3K\n1.76K\n    2016\n77%\n23%\n10%\n43.8K\n13.0K\n6.55K\n    2017\n80%\n20%\n1%\n52.5K\n12.9K\n703\n    2018\n82%\n18%\n3%\n52.8K\n11.7K\n1.66K\n    \n      Black\n    \n    2013\n64%\n36%\n9%\n192K\n106K\n28.7K\n    2014\n58%\n42%\n9%\n171K\n122K\n30.3K\n    2015\n62%\n38%\n12%\n183K\n113K\n40.5K\n    2016\n62%\n38%\n5%\n204K\n127K\n15.6K\n    2017\n64%\n36%\n2%\n216K\n122K\n8.04K\n    2018\n62%\n38%\n3%\n200K\n122K\n9.90K\n    \n      Hispanic\n    \n    2013\n48%\n52%\n4%\n21.6K\n23.8K\n1.90K\n    2014\n38%\n62%\n12%\n15.7K\n25.2K\n5.66K\n    2015\n47%\n53%\n10%\n17.2K\n19.5K\n4.01K\n    2016\n59%\n41%\n4%\n23.0K\n15.8K\n1.59K\n    2017\n57%\n43%\n1%\n24.7K\n18.5K\n421\n    2018\n59%\n41%\n1%\n30.9K\n21.1K\n616\n    \n      White\n    \n    2013\n68%\n32%\n4%\n2.45M\n1.16M\n148K\n    2014\n68%\n32%\n4%\n2.44M\n1.16M\n152K\n    2015\n70%\n30%\n4%\n2.54M\n1.07M\n164K\n    2016\n69%\n31%\n3%\n2.54M\n1.13M\n102K\n    2017\n70%\n30%\n3%\n2.54M\n1.11M\n113K\n    2018\n71%\n29%\n3%\n2.61M\n1.06M\n108K\n    \n      All Other Races\n    \n    2013\n72%\n28%\n6%\n58.3K\n22.3K\n4.90K\n    2014\n70%\n30%\n4%\n65.9K\n27.7K\n3.91K\n    2015\n71%\n29%\n7%\n59.6K\n24.3K\n6.19K\n    2016\n75%\n25%\n4%\n63.2K\n21.3K\n3.12K\n    2017\n71%\n29%\n2%\n68.9K\n28.2K\n2.23K\n    2018\n72%\n28%\n5%\n73.9K\n29.4K\n5.28K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nWhile we do see high NA values in some years, overall they’re at reasonable levels and seem to be lowest in 2018. This table is quite long though. While that works for exploring the data, for an actual display we’d probably want to focus on just 2018 and show the history in a graph. Below is the table filtered to just 2018 and slightly reformatted.\n\n\n\n\n\n\n  \n    \n      Table 7: High Speed Internet Access in 2018\n    \n    \n      By Race and Ethnicity\n    \n  \n  \n    \n      Race/Ethnicity\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    All Others\n72%\n28%\n5%\n73.9K\n29.4K\n5.28K\n    Asian\n82%\n18%\n3%\n52.8K\n11.7K\n1.66K\n    Black\n62%\n38%\n3%\n200K\n122K\n9.90K\n    Hispanic\n59%\n41%\n1%\n30.9K\n21.1K\n616\n    White\n71%\n29%\n3%\n2.61M\n1.06M\n108K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\nNow let’s add standard errors and graph the data. I’ll write the code for graphing the data as a function since we will use it again.\n\n# We do need to prep the data a little so that we're not carrying through the whole dataframe.\ndf_in <- df %>%\n  filter(!is.na(hspd_int)) %>%\n  mutate(hspd_n = if_else(hspd_int == \"Yes\", 1, 0)) %>%\n  select(hspd_n, PERWT, YEAR, race)\n\n# And we can call the bootstrap function\nboot_results <- bootstrap_pums(df = df_in, num_samples = 500, group_vars = c(YEAR, race))\n\ndf_plt <- df_wide %>%\n  full_join(boot_results, by = c(\"race\", \"YEAR\")) %>%\n  transmute(Year = YEAR,\n            Race = race,\n            Percent = 100 * percent_hspd,\n            me = 100 * 1.96 * sd) %>%\n  filter(Race != \"All Others\") # When plotting All Others overlaps White and having five lines makes it quite hard to read. \n\n# At this point I'll introduce a function to plot multiple groups over time, since we'll use this again \nplt_by <- function(df, group_var, title_text = \"\") {\n  \n  plt <- ggplot(data = df, aes(x = Year, y = Percent, group = {{group_var}}, colour = {{group_var}})) +\n    geom_errorbar(aes(ymin = Percent - me, ymax = Percent + me), width = .1) +\n    geom_point() +\n    geom_line() +\n    theme_bw() +\n    labs(title = title_text, x = \"Year\", y = \"Percent\") +\n    theme(legend.position = \"bottom\")\n\n  plt\n}\n\nplt_race <- plt_by(df_plt, Race, title_text = \"High Speed Internet Access by Race and Ethnicity\")\n\nplt_race\n\n\n\n\n\n\nPoverty Status\nI’m going to skip showing the code for Poverty, Age, and Geography because it’s extremely similar to the code used for Race. The poverty variable in the IPUMS data is measured in income as a percent of the poverty line. So for this analysis I code under 100 as being in poverty, between 100 and 200 percent of the poverty line as near poverty, and above 200 percent as not being in poverty.\n\n\n\n\n\n\n\n\n\n  \n    \n      Table 8: High Speed Internet Access in 2018\n    \n    \n      By Poverty Status\n    \n  \n  \n    \n      Poverty Status\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    In Poverty\n53%\n47%\n4%\n376K\n335K\n30.6K\n    Near Poverty\n62%\n38%\n4%\n499K\n311K\n29.7K\n    Not in Poverty\n78%\n22%\n2%\n2.09M\n595K\n64.8K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nAge\nThe age varible in IPUMS is the most straightforward, it’s just numeric age. It is worth pointing out that this is based on individual ages, while high speed internet is a household level variable. If we really wanted to do a deep dive into just age, we’d want to look at the age composition of the whole household. The really nice thing about microdata is you can slice and dice it any way you think is appropriate. So if you want to look at households that have individuals under 18 and over 65, you can. If you want to look at households with no one under 65, you can. I’ve just taken a high level cut of the data here though.\n\n\n\n\n\n\n\n\n\n  \n    \n      Table 9: High Speed Internet Access in 2018\n    \n    \n      By Age Group\n    \n  \n  \n    \n      Age Group\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    18 and under\n74%\n26%\n2%\n760K\n263K\n26.0K\n    19 to 64\n72%\n28%\n3%\n1.81M\n691K\n76.7K\n    65+\n58%\n42%\n3%\n398K\n288K\n22.4K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data.\n    \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nGeography\nIPUMs provides categorizations of where people live as either being in a prinicipal city, in the metro but not the principal city, or outside of a metro area. I have chosen to present this as the more familiar (and shorter) urban, suburban, and rural. IPUMS also has a direct measure of density that would be useful in an analysis - but for tables and graphs the categorial variable is better.\n\n\n\n\n\n\n\n\n\n  \n    \n      Table 10: High Speed Internet Access in 2018\n    \n    \n      By Metropolitan Status\n    \n  \n  \n    \n      Metropolitan Status\n      \n        Percent\n      \n      \n        Number of People\n      \n    \n    \n      Yes\n      No\n      NA\n      Yes\n      No\n      NA\n    \n  \n  \n    City\n85%\n15%\n3%\n257K\n46.2K\n8.16K\n    Mixed/Unknown\n68%\n32%\n3%\n1.60M\n752K\n74.7K\n    Rural\n67%\n33%\n3%\n694K\n341K\n35.4K\n    Suburbs\n81%\n19%\n1%\n420K\n102K\n6.77K\n  \n  \n    \n      Yes and No percentages are calculated out of those who answered. NA is reported out of all the data to provide context on how much data is missing.\n    \n    \n      Source: Author's analysis of IPUMS data."
  },
  {
    "objectID": "posts/census_internet_microdata/index.html#school-age-children",
    "href": "posts/census_internet_microdata/index.html#school-age-children",
    "title": "How to Use Census Microdata to Analyze High Speed Internet in Kentucky",
    "section": "School Age Children",
    "text": "School Age Children\nAs I’ve mentioned, a nice feature of using microdata is that you can look at the data in ways that aren’t available in the premade Census tabulations. With a lot of school districts using online learning, a look at the percent of school age children (ages 5-18) without high speed internet access at home could be useful for policymakers. Here we can see that there are parts of Western Kentucky where up to 60% do not have high speed access at home.\n\n#need to recreate the sf object for the joins to work correctly\nky_sf <- st_as_sf(ky_shp)\n\n# Calculate internet access at the PUMA level\ndf_group <- df %>%\n  filter(YEAR == 2018 & AGE >= 5 & AGE <= 18) %>%\n  group_by(hspd_int, PUMA) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = PUMA, names_from = hspd_int, values_from = count) %>%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd)\n\nint_puma <- tibble(\n  PUMA = df_wide$PUMA,\n  int_per = df_wide$percent_no * 100,\n  int_num = df_wide$No\n  )\n\nky_sf <- full_join(ky_sf, int_puma, by = \"PUMA\")\n\nggplot(ky_sf) + \n  geom_sf(aes(fill=int_per)) +\n  scale_fill_gradient(low = \"blue\", high = \"purple\", name = \"Percent\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        panel.border = element_blank()) +\n  labs(title = \"Children ages 5-18 in Households without High Speed Internet Access\",\n       caption = \"Map is shaded by percent of children without high speed access in each Public Use Microdata Area\")\n\n\n\n\n\nLouisville (Jefferson County) with Labels\n\n#need to recreate the sf object for the joins to work correctly\nky_sf <- st_as_sf(ky_shp) %>%\n  filter(PUMA %in% c(\"1701\", \"1702\", \"1703\", \"1704\", \"1705\", \"1706\"))\n\n# Calculate internet access at the PUMA level\n# Filter to only the PUMAs in Jefferson County\ndf_group <- df %>%\n  filter(YEAR == 2018 & AGE >= 5 & AGE <= 18 & PUMA %in% c(\"1701\", \"1702\", \"1703\", \"1704\", \"1705\", \"1706\")) %>%\n  group_by(hspd_int, PUMA) %>%\n  summarize(count = sum(PERWT), .groups = \"drop\")\n\n# Pivot for easier percent calculations\ndf_wide <- df_group  %>%\n  pivot_wider(id_cols = PUMA, names_from = hspd_int, values_from = count) %>%\n  mutate(percent_hspd = (Yes / (Yes + No)),\n         percent_no = 1 - percent_hspd)\n\nint_puma <- tibble(\n  PUMA = df_wide$PUMA,\n  int_per = df_wide$percent_no * 100,\n  int_num = formattable::comma(round(df_wide$No, -2), digits = 0)\n  )\n\nky_sf <- full_join(ky_sf, int_puma, by = \"PUMA\")\n\nggplot(ky_sf) + \n  geom_sf(aes(fill=int_per)) +\n  geom_sf_label(aes(label = int_num, fontface = \"bold\")) +\n  scale_fill_gradient(low = \"blue\", high = \"purple\", name = \"Percent\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        panel.border = element_blank()) +\n  labs(title = \"Children ages 5-18 in Households without High Speed Internet Access in Jefferson County\",\n       caption = \"Map is shaded by percent of children without access in each Public Use Microdata Area and \n       the number of children without access is given by the label rounded to the nearest 100\")\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data"
  },
  {
    "objectID": "posts/collider_bias/index.html",
    "href": "posts/collider_bias/index.html",
    "title": "Think before adding more variables to that analysis",
    "section": "",
    "text": "Introduction\n\n\nOverview\nHuman beings think in terms of stories and in terms of how the actions they take impact the things around them. It’s our natural default way of thinking, and generally it’s pretty useful.\nDoing data analysis doesn’t stop us from thinking in terms of stories and causation, but it should make us careful. With the increase in data and in the computing power to process it all, there have been claims that all we need in order to understand and act in the world is to listen to the data. But data does not speak for itself! It is interpretted by humans who will think about it through the lens of causality.\nThis is an introduction to thinking about causal models for data analysis. The purpose is to demonstrate that the popular approach of simply gathering as much data as you can and controlling for it via regression or other methods is not a good one, and is actively misleading in many cases. We should instead carefully think about plausible causal models using tools like diagrams (directed acyclic graphs, or DAGs) and then do data analysis in accordance with those models.\n\n\nA Simple Example of Confounding\nLet’s start with an example where using regression does make sense. I have noticed that the sports teams I like are more likely to lose when I am watching them on TV. This is true, but the idea that my watching them causes them to lose is not plausible. So either I’m mistaken in my data collection, very unlucky in my fanship (I am a fan of Cleveland sports teams, so this does seem likely), or there’s something else that explains the connection between my watching and my team losing. We can draw a simple diagram of what we’ve observed so far.\n(I’m using Mermaid and will put the code for each diagram above them so that it’s easy to recreate and edit later).\ngraph LR; A[Watch Game]–>B[Lose Game]\n\nOnce we know what the proper causal model looks like, we can see that the conclusion that my watching games caused my teams to lose was based on an incomplete view - or more technically it suffered from omitted variable bias. The analysis left out an important variable that impacted things. Once we control for opponent quality, the relationship between my watching and my team losing should go back to zero.\n\n\nA Much More Important Example: Women’s Wages\nThe idea of drawing out the diagram before doing the analysis can be applied to more important cases, like the ongoing dispute around the wage gap between men and women. Here, I’m taking an example from the excellent book Causal Inference: The Mixtape by Scott Cunningham.\nWhen companies are accused of paying women less one of their first lines of defense is to argue that if you account for the occupational differences within the company between men and women the wage gap vanishes or at least shrinks dramatically. Cunningham (and I) think this is a poor causal model and an inadequate defense. This is important, so we’re going to consider several causal models and look directly at what they tell us using some simulated data under different specifications. Using simulated data gives us the advantage of knowing the truth of the data - so to speak - we’ll create it to have certain causal relationships and then we’ll see how the different models capture (and fail to capture) those relationships.\nI’ll start with the causal diagram that we’re going to use to simulate our data. It’s a bit complicated, but we’ll take it piece by piece as we move through the data simulation and modeling.\ngraph LR; D[Discrimination] –> E[Earnings] D –> O[Occupation] O –> E A[Ability] -.-> O A -.-> E\n\n\nimport numpy as np # for generating arrays with random numbers\nimport pandas as pd # dataframes\nimport statsmodels.api as sm # to run the actual ols model\n\nnp.random.seed(42) # to make it reproducible\n\nWe’re going to first generate a labor force where half of it is discriminated against (e.g. women being paid less, the well known gender gap in wages) and has ability randomly distributed. In the causal model sketched above both Discrimination and Ability are root causes - they’re not caused by anything else in the diagram. (Both obviously have causes outside of the system we’re currently considering). So that’s the place we’ll start.\n\ngenerated_data = {\n    'discrimination'  : np.random.randint(low = 0, high = 2, size = 10000, dtype = int), #the high argument is not inclusive, so this is randomly generating 0s and 1s. \n    'ability' : np.random.normal(size = 10000),\n}\n\ndf = pd.DataFrame(data = generated_data)\n\nNow we need to generate some other variables of interest. We’re looking at the impact of discrimination, so let’s set that to be experienced by half of the labor force. We’re going to assume that discrimination affects both wages and choice of occupation. Here we’re worried about occupations in terms of higher and lower pay scales, so let’s set occupations to be positively associated with ability and negatively associated with discrimination.\nFinally, wages are negatively associated with discrimination and positively associated with both occupation and ability.\n\ndf['occupation'] = 1 + 2 * df['ability'] - 2 * df['discrimination'] + np.random.normal(size = 10000)\ndf['wage'] = 1 - 1 * df['discrimination'] + 1 * df['occupation'] + 2 * df['ability'] + np.random.normal(size = 10000)\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      discrimination\n      ability\n      occupation\n      wage\n    \n  \n  \n    \n      count\n      10000.000000\n      10000.000000\n      10000.000000\n      10000.000000\n    \n    \n      mean\n      0.498700\n      -0.008041\n      -0.009388\n      0.471065\n    \n    \n      std\n      0.500023\n      1.004178\n      2.449597\n      4.545405\n    \n    \n      min\n      0.000000\n      -3.922400\n      -10.018905\n      -18.328506\n    \n    \n      25%\n      0.000000\n      -0.674327\n      -1.640437\n      -2.517222\n    \n    \n      50%\n      0.000000\n      -0.007682\n      -0.022777\n      0.482132\n    \n    \n      75%\n      1.000000\n      0.668901\n      1.633467\n      3.501387\n    \n    \n      max\n      1.000000\n      3.529055\n      9.500154\n      16.731628\n    \n  \n\n\n\n\nNow that we have our simulated data with specified causal relationships, let’s look at a few different regression models. We’ll first look at a model that only includes discrimination as a cause of wages.\n\n# Set up matrices for regression\nY = df['wage']\nX1 = df['discrimination']\n\n# add constant for the intercept of the model\nX1 = sm.add_constant(X1)\n\n# specify the model\nmodel1 = sm.OLS(Y, X1)\n\n# fit the model\nresults1 = model1.fit()\n\n# Look at results\n# results1.summary()\nresults1.params\n\nconst             1.952182\ndiscrimination   -2.969956\ndtype: float64\n\n\nWhat we’re mainly interested in is the coefficient on discrimination. Here we see that being discriminated against has a strong negative impact on wages earned. (Don’t worry about the const (constant) term, it’s not important in this example).\nThis isn’t a surprise based on how we set up the data. It also correctly reflects that in the real world if you just divide wages by gender you will find a large gender gap.\nThe dispute comes in when we talk about controlling for occupation, or a model that looks like this:\ngraph LR; D[Discrimination] –> E[Earnings] D –> O[Occupation] O –> E\n\nIn this model it looks like being discriminated against might raise wages slightly. We know that’s not right since we know we set up the data to have discrimination decrease earnungs. The problem is that when we added occupation to the model we opened up a brand new causal pathway from discrimination to earnings. It’s the one that runs from Discrimination–>Occupation–>Ability–>Earnings in our original causal model.\nWhen we controlled for occupation we did two things:\n\nIgnored the fact that occupational choice is also a result of discrimination and as a defense of pay discrimination it would then be the mechanism by which discrimination happens, not a defense that discrimination isn’t happening.\nOpened up a causal pathway that made our estimates worse.\n\n\n\nX3 = df[['discrimination', 'occupation', 'ability']]\nX3 = sm.add_constant(X3)\nmodel3 = sm.OLS(Y, X3)\nresults3 = model3.fit()\nresults3.params\n\nconst             0.988717\ndiscrimination   -0.986841\noccupation        1.025762\nability           1.975298\ndtype: float64\n\n\n\n\nWhat have we learned?\nA major problem is that in the real world we can’t observe ability directly and put it in a regression model. Another issue is that this causal model is still very incomplete. Nonetheless, the way the sign flips back and forth depending on the model is hopefully an illustration of why it’s so important to have a theoretical model and not just throw in as much data as possible.\nData is a powerful way to tell stories, but data by itself never tells us everything we need to know. We have to interpret it carefully and think hard about the underlying models of the world we’re bringing to the data when we interpret it.\nTwo things to remember from this post:\n\nThink about the causal model before doing statistics or machine learning\nDon’t believe companies that say the gender gap goes away if you control for other things. That’s only true if you believe the causal model underlying their analysis - and you probably shouldn’t."
  },
  {
    "objectID": "posts/market_norms_social_norms/index.html",
    "href": "posts/market_norms_social_norms/index.html",
    "title": "Market Norms are Crowding out Social Norms",
    "section": "",
    "text": "Imagine you are going over to a friend’s house for a nice dinner. You want to show appreciation for the lovely meal they’ve prepared for you, and you have a few options. You can offer to have them over for dinner in the future. You can bring a bottle of wine or a six pack of beer. You can simply say thank you and tell them what a lovely evening you have had. The one thing you would not want to do is offer to pay them the fair market value of the food and ambiance they have prepared for you. This would be a major social faux pas, the replacing of a social norm with a market interaction.\nThere are some situations that are governed by social norms and other situations that are governed by market norms. To an economist, however, if those norms ever do overlap, then they could only strengthen each other. After all, if someone is willing to do something based only on the strength of a social norm, shouldn’t they be more willing to do it for both the social norm and the additional monetary reward of the market? To be more concrete about it, if lawyers are willing to volunteer their time to help the elderly with legal analysis, it would be completely irrational for them to not be at least as willing to sell their time at a discount.\nAs it turns out, in many situations market norms don’t complement social norms, they crowd them out. The AARP really did ask some lawyers if they would offer services to retirees at a discount. The lawyers said no. Then the AARP asked if they would do it for free and they said yes. Now the economically rational thing to do would be for them to think of themselves as volunteers who get paid a small stipend, but once market norms were part of the consideration, social norms departed.\nIn Predictably Irrational, Dan Ariely writes about an experiment he set up to test market norms and social norms. Participants were asked to complete a mind-numbingly dull computer task, dragging circles into a square for 5 minutes. They split their participants into three groups, one group received 5 dollars, one group 50 cents, and one group was simply asked to complete the task as a favor to the experimenters. Those making 5 dollars outperformed those making only 50 cents by a wide margin, 159 to 101. The volunteers, however, dragged 168 circles. To be clear, that’s not significantly more than the 5 dollar group. If you pay people a reasonable wage to do a task they really will work hard at it. The point, however, is that the social norm was every bit as strong as the fair wage norm, and much, much stronger than the market norm created by offering substandard compensation.\nOne other concrete example of this crowding out of social norms comes from an Israeli daycare. The daycare decided to implement a fine on parents who were late to pick up their children. The fine backfired in epic fashion, and the number of parents who were late actually increased. Before, the parents felt bound by a social norm. Having worked in after-school care I can tell you that parents really do feel bad about inconveniencing the staff by being late. Once money enters the equation, however, being late is simply an extra service that can be paid for and falls under the realm of market norms.\nThe most disturbing part of the day care story is what happened next. The day care, realizing that the fine wasn’t working, decided to eliminate it. The parents behavior, however, did not revert to prior levels of lateness, in fact, it rose a little bit. Now that the social norm was gone it did not return, and having eliminated the market penalty as well it makes sense that tardiness increased. Once market norms have crowded out social norms it may not be easy to get them back.\nIt turns out that being exposed to market cues and signals has a negative effect on our social behavior. Psychologist use a technique called priming to see how certain unconscious cues affect our behavior. Priming works by exposing its subjects to something that will later impact their thinking. In this case, subjects were exposed to something money-related, i.e. a stack of monopoly money on a table or a computer screen saver with dollar bills floating in the water. Money primed people become more self-reliant, but also more selfish and less willing to help others. They were less social and showed an increased desire to be left alone.\nMichael Sandel writes about the moral implications of our transition to a market driven society in What Money Can’t Buy:\n\nwhen we decide that certain goods may be bought and sold, we decide, at least implicitly, that it is appropriate to treat them as commodities, as instruments of profit and use. But not all goods are properly valued in this way.\n\nThe problem is that over time, and without ever really discussing it,\n\n…we drifted from having a market economy to being a market society. The difference is this: A market economy is a tool – for organizing productive activity. A market society is a way of life in which market values seep into every aspect of human endeavor. It’s a place where social relations are made over in the image of the market.\n\nNow, a market economy is a good way to regulate the sales of books, clothing, shoes, technology, and a great many other things. But it’s not a good way to live a full and flourishing life as a human being. My problem is not with markets, but rather with a series of assumptions that associates human well-being with market well-being, as though the two were the same thing. For now let me conclude with some wise words from Robert F. Kennedy, who recognized this problem back in 1968:\n\nOur Gross National Product is now over 800 billion dollars a year. But that Gross National Product counts air pollution and cigarette advertising, and ambulances to clear our highways of carnage. It counts special locks for our doors and the jails for the people who break them. It counts the destruction of the redwood and the loss of our natural wonder in chaotic sprawl. It counts napalm and counts nuclear warheads and armored cars for the police to fight the riots in our cities. It counts…the television programs which glorify violence in order to sell toys to our children. Yet the Gross National Product does not allow for the health of our children, the quality of their education or the joy of their play. It does not include the beauty of our poetry or the strength of our marriages, the intelligence of our public debate or the integrity of our public officials. It measures neither our wit nor our courage, neither our wisdom nor our learning, neither our compassion nor our devotion to our country. It measures everything, in short, except that which makes life worthwhile."
  },
  {
    "objectID": "posts/meritocracy_unjust/index.html",
    "href": "posts/meritocracy_unjust/index.html",
    "title": "Meritocracy is Unjust",
    "section": "",
    "text": "Equality of opportunity is one of the few goals that is still shared by policymakers in both parties (who disagree on how to achieve it, but pay it considerable lip service in speeches). It’s also shared by academics who spend considerable time and effort measuring just how likely it is that a child born to poor parents will end up middle-class or wealthy. Equality of opportunity and the meritocratic society it envisions (one where everyone has an equal opportunity to get what they deserve) assumes that the important thing is that those who deserve it have the opportunity to attain high economic outcomes.\nTo think that a more meritocratic society is a just one that is worth striving for is to misunderstand the nature of what it means to deserve something. Meritocracy tends to confuse a very practical sense of merit with a more abstract and moral one. An individual may deserve a high-paying job or admission to a selective college because they are productive or qualified. However, in a moral sense, individuals do not merit the skills and abilities they are born with, nor do they merit the environments they were born into that allowed them to develop those skills.\nShould society really be structured such that smart kids who are born into poverty are able to escape poverty while less intelligent children are not?\nThis is undoubtedly better than a world in which no one escapes poverty, but is genetic predisposition for problem solving really the criteria by which we should determine who is condemned to the suffering that poverty entails? For that matter, the ability to focus and the development of a strong work ethic are also determined by factors outside of a child’s control. Some of those factors are genetic, and some are environmental but none of them are actually controlled by the child. Thus from the standpoint of the lottery of birth, a meritocratic society is not just, it’s arbitrary.\nNone of this is meant to deny the good work that is done in the name of equality of opportunity. It really is good to have better schools in poor communities. But it’s not because it means some percent of the children who are actually deserving will escape poverty (while the rest will be said to have had an opportunity and squandered it), it’s because all children deserve good schools.\nPeople don’t choose where they’re born, to whom, with what genetic predispositions, or in what sort of environment they grow up. They don’t choose their intelligence, creativity, or work ethic. Unfortunately, equality of opportunity all too often turns into a way to sort people (particularly poor people of color) into deserving and undeserving. It’s time to stop sorting, and focus on making people’s lives better (or at least a bit less harsh). None of us deserve the life we’re born into. Shouldn’t those of us who were lucky enough to be born with the ability to help others use that ability to help them without stopping to inquire whether or not they deserve it?"
  },
  {
    "objectID": "posts/moral_limits_of_markets/index.html",
    "href": "posts/moral_limits_of_markets/index.html",
    "title": "The Moral Limits of Markets",
    "section": "",
    "text": "Michael Sandel’s new book What Money Can’t Buy: The Moral Limits of Markets is a well-timed critique not of capitalist economics, but of the spread of economic thinking well beyond the boundaries of traditional economic issues like trading, inflation, prices, wages, etc. I just started a microeconomics course in preparation for graduate school in the fall and the textbook simply defined economics as “the study of choice.”\nSandel’s thesis is relatively simple: “…we drifted from having a market economy to being a market society. The difference is this: A market economy is a tool - a valuable and effective tool - for organizing productive activity. A market society is a way of life in which market values seep into every aspect of human endeavor. It’s a place where social relations are made over in the image of the market.”\nSandel makes two key arguments for not extending economic thinking to other aspects of life. First, he argues applying prices to certain goods degrades the good itself. Second, he argues that pricing certain goods is unfair. He refers to these two objections as the corruption objection and the fairness objection. The fairness objection is relatively familiar, summed up well by Sandel, who writes “the willingness to pay for a good does not show who values it most highly. This is because market prices reflect the ability as well as the willingness to pay.” To illustrate his point Sandel talks about the growing market of paying people to stand in line for things as varied as congressional hearings and free theatre performances of Shakespeare in Central Park.\nSandel argues that there are both goods that cannot be bought and sold, such as friendships and honorifics (Oscars, Nobel Prize, etc.) and goods that are degraded when they are sold. Some of the more obvious examples include websites that allow you to buy wedding toasts and services that will send people to apologize for you. The rise of giving money as a gift is another example. As Miss Manners writes that money and gift cards have “taken the heart and soul out of the holiday. You’re basically paying somebody - paying them to go away.” Gift giving is economically inefficient, and yet attempts to increase its efficiency through gift cards and direct money transfers destroy both the meaning and the joy behind giving.\nAs Sandel writes, “when we decide that certain goods may be bought and sold, we decide, at least implicitly, that it is appropriate to treat them as commodities, as instruments of profit and use. But not all goods are properly valued in this way.” Sandel continues,“Economists often assume that markets do not touch or taint the goods they regulate. But this is untrue. Markets leave their mark on social norms. Often, market incentives crowd out or erode nonmarket incentives.”\nOne of the best documented cases of crowded out comes from a two-part survey in Switzerland, in which a small community was asked about storing nuclear waste in a nearby mountain. There were initially simply asked to store the waste out of civic duty, and then in round two they were asked and offered an annual payment. As a result, the acceptance rate went down, from 51 percent who would accept the nuclear waste simply out of civic duty, to a mere 25 percent who would accept when offered an annual monetary payment. Economists then increased the monetary offer, but even that did not help. Sandel sums up, “If the community was found to be the safest storage site they were willing to bear the burden. Against the background of this civic commitment, the offer of cash to residents felt like a bribe, an effort to buy their vote. In fact, 83 percent of those who rejected the monetary proposal explained their opposition by saying they could not be bribed.”\nChild care centers that begin charging late fees have also found that the number of late parents actually increases, because instead of viewing it as an inconvenience to the child care providers, it is a paid service. (In a sense, they are even one of the providers best customers).\nSandel is at his very best in a short section entitled ‘Two Tenets of Market Faith’: The first assumption is that “commercializing an activity doesn’t change it,” an assumption already critiqued by his arguments about corruption and the subsequent crowding out of non-market incentives. The second tenet is “that ethical behavior is a commodity that needs to be economized.” He cites several economists, including Lawrence Summers, who replied to an objection that markets rely on selfishness and greed by arguing, “We all have only so much altruism in us. Economists like me think of altruism as a valuable and rare good that needs conserving.”\nSandel’s take-down is blunt and immediate, “Altruism, generosity, solidarity, and civic spirit are not like commodities that are depleted with use. They are more like muscles that develop and grow stronger with exercise. One of the defects of a market-driven society is that it lets these virtues languish. To renew our public life we need to exercise them more strenuously.”\n\nWhy Inequality Matters\nSandel’s key insight is his ability to insist that the market is simultaneously an invaluable tool for the distribution of commodities, and a terrible danger to society when goods that should never be treated as commodities are placed on the market to be bought and sold like the latest computer gadgets or fashions. With that in mind, Sandel offers a better argument for why inequality matters than any I have heard or read before:\n\nIf the only advantage of affluence were the ability to buy yachts, sports cars, and fancy vacations, inequalities of income and wealth would not matter very much. But as money comes to buy more and more - political influence, good medical care, a home in a safe neighborhood rather than a crime-ridden one, access to elite schools rather than failing ones - the distribution of income and wealth looms larger and larger. Where all good things are bought and sold, having money makes all the difference in the world.\n\nMy only quibble is that he left out clean drinking water and the non-destruction of lands that have been in families for generations. The impacts of mountaintop removal coal mining are horrifying, and simply would not be allowed to happen in rich areas of the country. For the price of a few high wage but incredibly dangerous and hard jobs, coal companies have been allowed to do almost anything they want to the residents of Appalachia."
  },
  {
    "objectID": "posts/partitioned_regression/index.html",
    "href": "posts/partitioned_regression/index.html",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "",
    "text": "Partitioned regression is a helpful way to gain more intuition for how regression works. What does it mean when we say that regression allows us to adjust for (or control for) other variables? By looking at a regression in multiple pieces we can gain a better understanding of what’s happening and also produce a pretty cool visualization of our key result. (Partitioned regression could also come in handy if you ever have to run a regression on a computer with limited RAM, but that’s not our focus here)."
  },
  {
    "objectID": "posts/partitioned_regression/index.html#getting-started",
    "href": "posts/partitioned_regression/index.html#getting-started",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "Getting Started",
    "text": "Getting Started\nLike most Python projects, we’ll start by loading some libraries.\n\nNumpy is standard for numerical arrays, and Pandas for dataframes and dataframe manipulation.\nAltair is a visualization library. It’s not as well known as matplotlib or Plotly, but I like the aesthetics of the plots it produces and I find it’s grammar of graphics a bit more intuitive.\npalmerpenguins is an easy way to load the demonstration data set I’ll be using here. You could also download it as a .csv file from here\nI’m using scikit-learn (sklearn) for the regression because it’s a useful package to learn for additional work in Python. Statsmodels is another choice that would have worked well for everything in this post.\n\n\nimport numpy as np\nimport pandas as pd\nimport altair as alt\nfrom palmerpenguins import load_penguins\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\n\nLoading the penguins data and showing the first few rows\n\ndf = load_penguins()\n\n# if you download the .csv instead of using the library\n# df = pd.read_csv(\"palmer_penguins.csv\")\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      2007\n    \n  \n\n\n\n\n\n\n\nPenguins illustrated by Allison Horst\n\n\nThe penguins dataset is a small dataset that’s useful for doing demonstrations for everyone who is tired of the iris dataset and thinks penguins are cute."
  },
  {
    "objectID": "posts/partitioned_regression/index.html#preparing-the-data",
    "href": "posts/partitioned_regression/index.html#preparing-the-data",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "Preparing the data",
    "text": "Preparing the data\nBefore we can run any regressions we need to clean up the data a bit. There’s a row that is NA that we can drop and we have some categorical variables that can’t be used directly. We’ll remove the NA data and transform the categorical variables into a series of dichotomous variables that have the same information but can be used in our analysis.\n\ndf = df.dropna().reset_index() # if you don't reset the index then merging on index later will result in data mismatches and destroy your data silently. \nenc = preprocessing.OneHotEncoder(sparse_output = False) # important to have sparse_output = False for the array to be easily put back into a dataframe afterwards\nencoded_results = enc.fit_transform(df[['sex', 'species']])\n\n# names are not automatically preserved in this process, so if you want feature names you need to bring them back out. \ndf2 = pd.DataFrame(encoded_results, columns = enc.get_feature_names_out())\n\n# putting the dichotomous variables in along with everything else\n# this still has the original categorial versions, so check that everything lines up correctly\ndf = pd.concat([df, df2], axis = 1)\n\n#instead of using scikit-learns preprocessing features you could do this manually with np.where\n#df['male'] = np.where(df['sex'] == 'male', 1, 0)"
  },
  {
    "objectID": "posts/partitioned_regression/index.html#partitioned-regression",
    "href": "posts/partitioned_regression/index.html#partitioned-regression",
    "title": "Partitioned Regression with Palmer Penguins and Scikit-Learn",
    "section": "Partitioned regression",
    "text": "Partitioned regression\nLet’s say we’re interested in the relationship between bill depth and bill length. Bill length will be our dependent (or target) variable. We think there are things other than bill depth that are related to bill length, so we want to adjust for those when considering the relationship between depth and length. I’m going to put all of those other variables into a matrix called X. (For the dichotomous variables one category has to be left out).\nThen we’re going to run three different regressions. First we’ll regress bill length on all the X variables. Then we’ll also regress bill depth on all of the X variables. Finally, we’ll regress the residuals from the first regression on the residuals from the second regression. The residuals represent what is left unexplained about the dependent variable after accounting for the control variables. And by regressing both bill length and bill depth on the same set of control variables, we get residuals that can be thought of as bill length and bill depth after adjusting for everything in X. That lets us see the relationship between bill length and bill depth after accounting for anything else we think is relevant.\n\ny = df['bill_length_mm'] # target variable\nz = df['bill_depth_mm'] # effect we're interested in\nX = df[['flipper_length_mm', 'body_mass_g', 'sex_female', 'species_Adelie', 'species_Chinstrap']] # other variables we want to adjust for\n\n\nmodel1 = LinearRegression().fit(X, y)\n#residuals aren't actually saved by scikit-learn, but we can create them from the original data and the predictions\nresiduals_y_on_X = (y - model1.predict(X))\n\nmodel2 = LinearRegression().fit(X, z)\nresiduals_z_on_X = (z - model2.predict(X))\n\n#need to reshape for scikit learn to work with a single feature input\nz_resids = residuals_z_on_X.to_numpy().reshape(-1, 1)\ny_resids = residuals_y_on_X.to_numpy().reshape(-1, 1)\n\npart_reg_outcome = LinearRegression().fit(z_resids, y_resids)\n\n#has to be np.round, not round. And has to be [0, 0] not [0] for a 1d array\nprint(\"The regression coefficient using partitioned regression is {}\".format(np.round((part_reg_outcome.coef_[0, 0]), 3)))\n\nThe regression coefficient using partitioned regression is 0.313\n\n\nWe can also verify that we’d get the same result from an ordinary linear regression\n\n#add the bill depth variable back into the X array\nX2 = df[['bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex_female', 'species_Adelie', 'species_Chinstrap']]\n\n#here we can just use [0] for some reasons\nlr_outcome = LinearRegression().fit(X2, y)\nprint(\"The regression coefficient using linear regression is {}\".format(np.round(lr_outcome.coef_[0], 3)))\n\nThe regression coefficient using linear regression is 0.313\n\n\nOne advantage of the partitioned regression is that it allows us to look at the relationship visually. Instead of just having the point estimate, standard error, and any test statistics (e.g. p-value) we can visually inspect a full scatterplot of the data. I’ve added a regression line, and the slope of the line is equal to the regression coefficients found above. You can visually see from this plot that it isn’t a very strong relationship.\n\nplt_df = pd.DataFrame(data = {'Adjusted Bill Length': residuals_y_on_X, 'Adjusted Bill Depth': residuals_z_on_X})\n\nsp = alt.Chart(plt_df, title = \"Bill Depth and Bill Length (Adjusted)\").mark_circle().encode(\n    alt.X('Adjusted Bill Depth', scale = alt.Scale(zero = False)),\n    alt.Y('Adjusted Bill Length', scale = alt.Scale(zero = False)),\n)\n\nsp + sp.transform_regression('Adjusted Bill Depth', 'Adjusted Bill Length').mark_line()\n\n\n\n\n\n\nA disadvantage of using scikit learn is that it doesn’t give us traditional regression statistics. The easiest way to get those is through statsmodels, which shows the expected 0.313 coefficent and tells us the standard error is 0.154 with a p-value of .043. This tells us it is actually a statistically significant relationship, so without the visual evidence from the scatterplot above, we might assume it’s a stronger relationship than it actually is. It is unlikely to have occured purely by chance, but that doesn’t mean it’s necessarily tightly correlated or has a large effect size.\n\nimport statsmodels.api as sm\n\n#unlike scikit learn, statsmodels does not add a constant for you unless you specify that you want one. \nX2 = sm.add_constant(X2)\nest = sm.OLS(y, X2).fit()\nest.summary2()\n\n\n\n\n        Model:               OLS         Adj. R-squared:     0.836  \n\n\n  Dependent Variable:  bill_length_mm         AIC:         1482.2547\n\n\n         Date:        2023-03-18 14:44        BIC:         1508.9117\n\n\n   No. Observations:         333         Log-Likelihood:    -734.13 \n\n\n       Df Model:              6           F-statistic:       282.3  \n\n\n     Df Residuals:           326       Prob (F-statistic): 7.50e-126\n\n\n      R-squared:            0.839            Scale:         4.9162  \n\n\n\n\n                     Coef.  Std.Err.    t     P>|t| [0.025  0.975] \n\n\n  const             23.4506  4.8836  4.8019  0.0000 13.8433 33.0579\n\n\n  bill_depth_mm     0.3130   0.1541  2.0316  0.0430 0.0099  0.6160 \n\n\n  flipper_length_mm 0.0686   0.0232  2.9608  0.0033 0.0230  0.1141 \n\n\n  body_mass_g       0.0011   0.0004  2.5617  0.0109 0.0003  0.0019 \n\n\n  sex_female        -2.0297  0.3892  -5.2153 0.0000 -2.7953 -1.2641\n\n\n  species_Adelie    -6.4044  1.0304  -6.2154 0.0000 -8.4314 -4.3773\n\n\n  species_Chinstrap 3.1612   0.9927  3.1844  0.0016 1.2082  5.1141 \n\n\n\n\n     Omnibus:    32.842  Durbin-Watson:    2.068\n\n\n  Prob(Omnibus):  0.000 Jarque-Bera (JB): 90.331\n\n\n       Skew:      0.430     Prob(JB):      0.000\n\n\n     Kurtosis:    5.402  Condition No.:   173513\n\n\n\n\nFinally, we might interested in how different this picture is from the unadjusted relationship between bill length and depth if we had not taken into account other variables\n\nsp = alt.Chart(df, title = \"Bill Depth and Bill Length (Unadjusted)\").mark_circle().encode(\n    alt.X('bill_depth_mm', title = 'Bill Depth', scale = alt.Scale(zero = False)),\n    alt.Y('bill_length_mm', title = 'Bill Length', scale = alt.Scale(zero = False)),\n)\n\nsp + sp.transform_regression('bill_depth_mm', 'bill_length_mm').mark_line()\n\n\n\n\n\n\nThis difference and sign reversal is mostly because of the relationships between species, bill length, and bill depth. But that’s a subject for a post about Simpson’s paradox."
  },
  {
    "objectID": "posts/racing_lou_season1/index.html",
    "href": "posts/racing_lou_season1/index.html",
    "title": "Racing Louisville in their first NWSL season",
    "section": "",
    "text": "The NWSL Challenge Cup kicked off earlier this week, so I took quick look at some of the stats from last season. I got the data from FBREF and made some quick graphs - I’m including the code here, but feel free to ignore it if you’re only interested in the football statistics.\nI wanted to know if the NWSL had teams that were focused on offense or defense, so I looked first at average goals scored and allowed per game. On average, teams score 1.15 goals per game, so I added those as reference lines.\nFor the most part teams weren’t really good at one end and not the other. The closest any team came to that is Houston, which is above average on offense and below average on defense. For the most part though, offensive and defensive skill go together.\nWe’d expect average goals to matter a lot, but soccer is a pretty high variance sport, so I also wanted to know how well goal differential predicted results. Here results are the points that determine standings (3 pts for a win, 1 for a draw, 0 for a loss).\nAs we’d expect they track pretty neatly. Washington and Chicago had slightly better seasons than you’d expect from goal differential alone, but nothing wild."
  },
  {
    "objectID": "posts/racing_lou_season1/index.html#racing-louisville-and-homefield-advantage",
    "href": "posts/racing_lou_season1/index.html#racing-louisville-and-homefield-advantage",
    "title": "Racing Louisville in their first NWSL season",
    "section": "Racing Louisville and Homefield Advantage",
    "text": "Racing Louisville and Homefield Advantage\nRacing Louisville is my team, so I also pulled some of their game specific data and here again started looking at goals. In this case I was curious about how much of a homefield advantage they have.\n\ndf2 = pd.read_csv(\"https://raw.githubusercontent.com/natekratzer/nwsl/main/data/team/lou_games.csv\")\n\ndf2 = df2[df2['Comp'] == 'NWSL'] #exclude challenge cup which is in this dataset\n\n# Reformat to long\ngoals_df = df2[['Venue', 'GF', 'GA']].melt(id_vars = ['Venue'], value_vars = ['GF', 'GA'])\n\n# recode GF and GA to Scored and Allowed\nold_list = ['GF', 'GA']\nnew_list = ['Scored', 'Allowed']\ngoals_df['variable'] = goals_df['variable'].replace(old_list, new_list)\n\n# Group by and summarize into new dataframe\ngrouped_df = (goals_df.groupby(['Venue', 'variable'])['value']\n                      .mean()\n                      .to_frame(name='Goals')\n                      .reset_index())\n                      \n# Visualize\nfig = px.bar(grouped_df,\n             x = 'variable',\n             y = 'Goals',\n             color = 'variable',\n             facet_col = 'Venue',\n             labels = dict(variable = 'Allowed/Scored', Goals = 'Goals Per Match'),\n             template = 'simple_white',\n             title = \"Racing Louisville Struggles with Defense on the Road\")\n\nfig.show()\n\n\n                                                \n\n\nHere we do see a clear offense/defense distinction, which is that Louisville’s defense collapses during road games. The offense is slightly worse (0.75 goals per match compared to 1.0 at home), but the defense gives up over 2 goals a game on average during away matches.\nNot surprisingly, Louisville also wound up with a much worse away record (1-3-8) than home record (4-4-4)\n\nrecord_df = (df2.groupby(['Venue', 'Result'])['Date']\n                .count()\n                .to_frame(name = 'Matches')\n                .reset_index())\n\nfig = px.bar(record_df,\n             x = 'Result',\n             y = 'Matches',\n             color = 'Result',\n             facet_col = 'Venue',\n             #labels = dict(variable = 'Allowed/Scored', Goals = 'Goals Per Match'),\n             template = 'simple_white',\n             title = \"Racing Louisville is Much Better at Home\")\n\nfig.show()"
  },
  {
    "objectID": "posts/theory_of_moral_sentiments/index.html",
    "href": "posts/theory_of_moral_sentiments/index.html",
    "title": "A Theory of Moral Sentiments",
    "section": "",
    "text": "Adam Smith is largely responsible for starting the field of economics as an academic subject. His magnum opus The Wealth of Nations is considered the first work of modern economics, and Smith is sometimes referred to as “the father of modern economics.” I tell you this because I think Smith would be at least mildly ashamed of his academic progeny. Smith is perhaps best known for his famous statement on mutually beneficial trade:\n\nIt is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner, but from their own interest. We address ourselves, not to their humanity, but to their self-love, and never talk to them of our necessities, but of their advantages.\n\nSmith continues on to write (entirely correctly) about why it makes sense for people to specialize in tasks like butchery or baking and then trade with each other. However, the economics profession as a whole (with notable exceptions) has taken the idea of self-interest much farther than Smith intended. In addition to The Wealth of Nations, Smith’s other major work was A Theory of Moral Sentiments, which Smith opens with these words:\n\nHow selfish soever man may be supposed, there are evidently some principles in his nature, which interest him in the fortune of others, and render their happiness necessary to him, though he derives nothing from it except the pleasure of seeing it.\n\nIn fact, Smith goes on to argue that the entire basis of morality is sympathy, or ability to project ourselves into the others place and to feel with them either the pain or joy that they are experiencing:\n\nAnd hence it is, that to feel much for others and little for ourselves, that to restrain our selfish, and to indulge our benevolent affections, constitutes the perfection of human nature; and can alone produce among mankind that harmony of sentiments and passions in which consists their whole grace and propriety.\n\nSmith the moral philosopher seems radically different from Smith the economist, but Smith’s positions are not contradictory. Some have suggested that The Wealth of Nations, being written after A Theory of Moral Sentiments, represents a change in the direction of Smith’s thought, but in reality the same concerns the Smith expresses in his moral philosophy can be found in his economics. Smith’s analysis of human behavior is much, much more complex and layered than modern classical economics gives him credit for.\nSmith, for instance, argues that stockholders often want policies that are not in the interest of the general public. Amartya Sen points out that while Smith believed that self-interest was sufficient to motivate market exchanges, there are problems involved in setting up markets equitably and managing externalities and public goods that self-interest is unable to deal with. To quote Sen:\n\nBut in dealing with other problems, those of distribution and equity and rule-following for generating productive efficiency – Smith emphasized broader motivations. In these broad contexts, while prudence remained “of all virtues that which is most helpful to the individual,” he explained why “humanity, generosity, and public spirit, are the qualities most useful to others.” The variety of motivations that we have reason to accommodate is, in fact, quite central to Smith’s remarkably rich analysis of human behavior.\n\nEconomists today often draw a distinction between self-interest and selfishness. The self can be interested in ends that are not selfish. To some extent, this is a semantics games, but it is one that Smith explicitly rejects.\n\nThose who are found of deducing all our sentiments from certain refinements of self-love, think themselves at no loss to account, according to their own principles, both for this pleasure and this pain. Man, say they, conscious of his own weakness, and of the need which he has for others, rejoices whenever he observes that they adopt his own passions, because he is then assured of assistance; and grieves whenever he observes the contrary, because he is then assured of opposition. But both the pleasure and the pain are felt so instantaneously, and often upon such frivolous occasions, that it seems evident that neither of them can be derived from any such self-interested consideration.\n\nFortunately for us, behavioral economics is confirming experimentally most of what Smith already knew. Today, we know that exposure to money and to economics makes people more likely to act selfishly. Smith, however, is already a step ahead of us (he’s pretty spry for a dead guy). He knew that not only was this bad for society, it was bad for the individuals as well.\n\nSociety and conversation, therefore, are the most powerful remedies for restoring the mind to its tranquility, if, at any time, it has unfortunately lost it; as well as the best preservatives of that equal and happy temper, which is so necessary to self-satisfaction and enjoyment.\n\nSmith was, no doubt, wrong about many things, but economics could be much further ahead if it had simply paid attention to its founding father. Today, you can get a PhD in economics without ever reading Smith. This is a tragedy for those of us who want to have an economics discipline that recognizes the rich variety in human behavior and is willing to grapple with the tough philosophical issues and value judgments that underlie the calculations and empirical methodologies."
  }
]